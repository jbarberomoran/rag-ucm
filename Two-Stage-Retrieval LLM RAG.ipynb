{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48f3a07",
   "metadata": {},
   "source": [
    "# **Two-Stage-Retrieval LLM RAG**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cd76b",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "\n",
    "* [1. Introduction & Key Features](#1-introduction--key-features)\n",
    "* [2. Project Structure](#2-project-structure)\n",
    "    * [2.1 Input Data](#21-input-data)\n",
    "    * [2.2 Source Code](#22-source-code)\n",
    "    * [2.3 Output Data](#23-output-data)\n",
    "* [3. Workflow Summary](#3-workflow-summary)\n",
    "* [4. Demonstration](#4-demonstration)\n",
    "    * [4.1 Data Acquisition and Preparation](#41-data-acquisition-and-preparation)\n",
    "    * [4.2 Methodology and Pipeline Design](#42-methodology-and-pipeline-design)\n",
    "    * [4.3 Execution and Results](#43-execution-and-results)\n",
    "* [5. Data Analysis and Inference](#5-data-analysis-and-inference)\n",
    "    * [5.1. Defining Metrics and Data Collection](#51-defining-metrics-and-data-collection)\n",
    "    * [5.2. The Evaluation Challenge: Defining \"Likeness\"](#52-the-evaluation-challenge-defining-likeness)\n",
    "    * [5.3. Pipeline Performance and Inference](#53-pipeline-performance-and-inference)\n",
    "    * [5.4. Conclusion: Optimal Pipeline and Conditions](#54-conclusion-optimal-pipeline-and-conditions)\n",
    "* [6. Authors](#6-authors)\n",
    "* [7. License](#7-license)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a559e-12f3-4091-b474-6f4b0304930d",
   "metadata": {},
   "source": [
    "# **1. Introduction & Key Features**\n",
    "\n",
    "\n",
    "This project implements a question-answering (Q&A) system over a technical research paper from Meta SuperIntelligence Labs. The goal is to compare how different retrieval-augmented generation (RAG) setups help a large language model (LLM) answer multiple-choice questions.\n",
    "\n",
    "The following retrieval pipelines are compared:\n",
    "\n",
    "- **LLM Baseline** ‚Äì Direct generation without retrieval.\n",
    "\n",
    "- **BM25** ‚Äì Classic sparse keyword search (TF-IDF based).\n",
    "\n",
    "- **Dense Retrieval** ‚Äì Embedding-based semantic retrieval.\n",
    "\n",
    "- **(Bonus) Hybrid Retrieval** ‚Äì Combination of BM25 and Dense Retrieval.\n",
    "\n",
    "- **(Bonus) Hybrid Retrieval + Cross-Encoder** ‚Äì A SOTA approach where a high-recall retriever fetches candidates (Hybrid, k=20) and a Cross-Encoder reranks them for high precision (k=5).\n",
    "\n",
    "\n",
    "To ensure robustness, reproducibility, and scientific rigor, this project integrates several software engineering patterns:\n",
    "\n",
    "- **Singleton Pattern for Retrieval Engine:** The `RetrievalEngine` class uses a Singleton pattern with **Lazy Loading**. This prevents memory overhead by loading heavy models only when necessary and ensures consistent database connections across the pipeline.\n",
    "\n",
    "- **Smart Ingestion Strategy:** We compare **Recursive Chunking** (optimized with high overlap to preserve scientific context) against **Semantic Chunking** (experimental), which uses embedding distances to segment text by topic.\n",
    "\n",
    "- **Automated \"Judge\" Evaluation:** Instead of relying solely on the final answer accuracy, we implement a **Ground Truth Verification** system. It compares the retrieved chunks against the source reference in the dataset to detect lucky guesses.\n",
    "\n",
    "- **Two-Stage Architecture:** A funnel approach that prioritizes *Recall* in the first stage (Hybrid Search) and *Precision* in the second stage (Cross-Encoder Re-ranking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34e29e",
   "metadata": {},
   "source": [
    "# **2. Project Structure**\n",
    "\n",
    " The project is organized into multiple files under the src/ folder, with a clear separation of responsibilities. Here‚Äôs how it works:\n",
    "\n",
    "## 2.1 **Input Data**\n",
    "\n",
    "  All of the following are on the `data/` directory:\n",
    "- `data/questions.json` - Contains 70 multiple-choice questions extracted from a technical research paper.\n",
    "Each question includes: the correct answer, three distractors, and an optional reference to the source paper.\n",
    "\n",
    "- `data/chroma_db` - Embedded vector data base.\n",
    "- `data/enunciado.pdf` - The problem statement already described.\n",
    "- `data/paper_refrag.pdf` - The technical paper from which questions and answers are extracted.\n",
    "\n",
    "\n",
    "## 2.2 **Source Code**\n",
    "- `main.py` - Entry point of the project.\n",
    "Supports *Local* mode (`results/local_results/`) and *Persistent* mode (`results/persistent_results/<test_name>/`).  \n",
    "    - Calls the pipeline, saves final results, and generates plots.\n",
    "  \n",
    "\n",
    "  All of the following are on the `src/` directory:\n",
    "- `launcher.py` - Sets up the environment for experiments.\n",
    "    - Initializes the vector database (ChromaDB).\n",
    "    - Clears previous results if needed.\n",
    "    - Creates all necessary directories (plots, final CSVs, etc.).\n",
    "\n",
    "- `src/ingestion.py` - Prepares and creates the data base. Invoked by the launcher.\n",
    "\n",
    "- `queries.py` - Main logic to execute questions.\n",
    "    - For each question and method it:\n",
    "        - Sends the query to the LLM and retrieves documents.\n",
    "        - Computes accuracy and evidence verification.\n",
    "        - Stores partial results in results/resultados_parciales.csv.\n",
    "        - Returns a DataFrame with all results for further evaluation.\n",
    "\n",
    "- `rag_pipeline.py` - Implements RAG logic for different retrieval methods\n",
    "    - Contains functions to verify ground truth against retrieved documents.\n",
    "     Computes retrieval scores and status tags for each answer.\n",
    "\n",
    "- `retrieval.py` - Implements the retrieval engine.\n",
    "    - Provides a singleton engine to handle different retrieval methods efficiently.\n",
    "\n",
    "- `evaluation.py` - Evaluates the results and generates dashboards.\n",
    "    - Accuracy per method  \n",
    "    - RAG quality distribution  \n",
    "    - Response latency  \n",
    "    - Retrieval fidelity\n",
    "\n",
    "## 2.3 **Output Data**\n",
    "\n",
    "    Found on the `results/` directory:\n",
    "\n",
    "- **Partial results** - Always stored in `results/resultados_parciales.csv`.\n",
    "    - Updated after each question is processed.\n",
    "\n",
    "- **Final results** - Stored in `results/local_results/` or `results/persistent_results/<test_name>`.\n",
    "    - File name: `resultados_finales.csv`.\n",
    "\n",
    "- **Plots/Dashboard** - Stored in `plots/` inside the corresponding results folder.\n",
    "    - Include:\n",
    "        - Bar charts for accuracy and RAG quality\n",
    "        - Boxplots for response latency\n",
    "        - Violin plots for retrieval fidelity\n",
    "\n",
    "> **Tip:** Run `main.py try_n` on the terminal in orden to save the try number n results in that directory<br> in orden not to overwrite other results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832372ef",
   "metadata": {},
   "source": [
    "# **3. Workflow Summary**\n",
    "\n",
    "1. Load questions dataset (questions.json).\n",
    "\n",
    "2. Initialize vector database (ChromaDB).\n",
    "\n",
    "3. For each question:\n",
    "    - Retrieve relevant documents (BM25 / Dense / Hybrid / Hybrid + Cross-Encoder).\n",
    "    - Query LLM for an answer.\n",
    "    - Verify against ground truth.\n",
    "    - Save partial results.\n",
    "\n",
    "4. After all questions are processed:\n",
    "    - Concatenate all results.\n",
    "    - Save final results CSV.\n",
    "    - Generate plots/dashboard for comparison and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc109c3d",
   "metadata": {},
   "source": [
    "# **4. Demonstration**\n",
    "This section initiates the transition from the theoretical design to the **operational demonstration** of our Two-Stage Retrieval system. We will trace the execution flow, relying on the **modular architecture** established within the `src/` directory.\n",
    "\n",
    "For the stability and reproducibility of the results, it is of vital importance that the project operates within a dedicated virtual environment (Anaconda/Miniconda). All complex dependencies (LangChain, ChromaDB, Sentence-Transformers) must be installed prior to execution by running: `pip install -r requirements.txt`. In addition, a valid API key for Google AI Studio must be provided in a private file called `.env`. The content of this file should be similar to: `GOOGLE_API_KEY=\"AI...\"`\n",
    "\n",
    "The initial code block below serves two primary engineering purposes:\n",
    "1.  **Dependency Loading:** Loading the necessary core libraries (`pandas`, `matplotlib`) and setting up environment variables (`.env`).\n",
    "2.  **Path Resolution:** Establishing the correct pathing (`sys.path.append`) to ensure all subsequent modules from our **clean `src/` architecture** are correctly imported and ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a3e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable de entorno cargada correctamente.\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS Y SETUP ---\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.evaluation import plot_accuracy, plot_rag_quality, plot_latency, plot_retrieval_score, setup_plot_style\n",
    "from src.rag_pipeline import verify_ground_truth_v1, query_rag\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# A√±adimos la ra√≠z del proyecto al path para poder importar 'src'\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"Error: GOOGLE_API_KEY no est√° configurada en las variables de entorno.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Variable de entorno y librer√≠as cargadas correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6216694",
   "metadata": {},
   "source": [
    "## 4.1 Data Acquisition and Preparation\n",
    "\n",
    "The initial stage of the RAG pipeline focuses on **Data Pre-processing**. This involves breaking down the source document (`paper_refrag.pdf`) into discrete units of knowledge (**chunks**) that are stored in **ChromaDB**.\n",
    "\n",
    "### Chunking Methodologies\n",
    "\n",
    "We implemented two primary chunking strategies, selectable via the `CHUNKING_METHOD` variable in `src/ingestion.py`:\n",
    "\n",
    "1.  **Recursive Chunking (Recursive Character Splitter):** This is the baseline method, optimized for technical context. It relies on rules of fixed length (e.g., **1200 characters** with **350 characters of overlap**).\n",
    "\n",
    "2.  **Semantic Chunking:** The advanced method. It utilizes an **embedding model** to compute the similarity between consecutive sentences, identifying natural **topic shifts** to define chunk boundaries.\n",
    "\n",
    "### Why Semantic Chunking?\n",
    "\n",
    "We chose to focus on **Semantic Chunking** because it addresses the primary weakness of RAG in technical domains: **avoiding false negatives** in retrieval.\n",
    "\n",
    "In dense scientific papers, naive splitting often cuts conclusions, cross-references, or complex sentences in half. This leads to:\n",
    "\n",
    "* **Loss of Context:** The embedding model creates an inaccurate vector because the sentence is fragmented.\n",
    "\n",
    "* **Lucky guesses:** The system answers correctly, but the strict evaluation judge fails to find the complete reference phrase because it was broken up.\n",
    "\n",
    "By using **Semantic Chunking**, we ensure that each unit of information passed to the retriever has the **highest possible semantic coherence**, which directly improves the **Retrieval Recall** and **Fidelity** metrics in our evaluation dashboard.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Once the text is chunked, the system proceeds to **initialize the Retriever** in a **Lazy Loading** state, ensuring the database connection is established efficiently right before the first query is made.\n",
    "\n",
    "The code below creates a new database if neccesary and initializes our retrieval engine for the next steps. To test the difference in runtime between recursive and semantic chunking set `rebuild_db=True` Note: due to our implementation, changing the chunking method requires to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34afb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîå Desconectando motor de b√∫squeda...\n",
      "üìÑ Cargando PDF...\n",
      "   -> PDF cargado: 30 p√°ginas.\n",
      "‚úÇÔ∏è Procesando fragmentos\n",
      "   -> Generados 128 fragmentos.\n",
      "üß† Guardando vectores en disco...\n",
      "üíæ Base de datos guardada exitosamente.\n",
      "‚úÖ Setup completado.\n",
      "\n",
      "‚öôÔ∏è Conectando al motor de recuperaci√≥n...\n",
      "‚úÖ SISTEMA OPERATIVO\n",
      "   - Estado: Base de Datos Conectada\n",
      "   - Ruta: ./data/chroma_db\n",
      "   - Chunks Indexados: 128\n",
      "   - Modelo Embeddings: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# --- GESTI√ìN DE DATOS Y VERIFICACI√ìN ---\n",
    "import os\n",
    "from src.ingestion import db_setup\n",
    "from src.retrieval import RetrievalEngine\n",
    "\n",
    "try:\n",
    "    # 1. GARANT√çA DE EXISTENCIA DE BD (Ingesta)\n",
    "    db_setup(rebuild_db=False, chunking_method=\"recursive\") # Opciones: \"recursive\", \"semantic\"\n",
    "    \n",
    "    # 2. CONEXI√ìN AL MOTOR (Singleton)\n",
    "    print(\"\\n‚öôÔ∏è Conectando al motor de recuperaci√≥n...\")\n",
    "    engine = RetrievalEngine.get_instance()\n",
    "    # 3. PRUEBA DE RECUPERADOR\n",
    "    # Esto asegura que ChromaDB y los Embeddings est√°n cargados en RAM\n",
    "    test_retriever = engine.get_retriever(\"hybrid\", k=1) # As√≠ aseguramos de que se inicien dense y bm25\n",
    "    \n",
    "    # Obtenemos estad√≠sticas reales de la BD\n",
    "    if engine.db:\n",
    "        doc_count = engine.db._collection.count()\n",
    "        print(f\"‚úÖ SISTEMA OPERATIVO\")\n",
    "        print(f\"   - Estado: Base de Datos Conectada\")\n",
    "        print(f\"   - Ruta: ./data/chroma_db\")\n",
    "        print(f\"   - Chunks Indexados: {doc_count}\")\n",
    "        print(f\"   - Modelo Embeddings: all-MiniLM-L6-v2\")\n",
    "    \n",
    "except Exception as e:\n",
    "    engine.unload_db()  # Liberamos memoria antes de la prueba\n",
    "    print(f\"\\n‚ùå ERROR CR√çTICO DE INICIALIZACI√ìN: {e}\")\n",
    "    print(\"   - Verifica que la base de datos est√© construida correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786fd750",
   "metadata": {},
   "source": [
    "## 4.2 Methodology and Pipeline Design\n",
    "\n",
    "Our solution employs a modular two-stage pipeline designed to overcome the inherent limitations of standard vector search. The process is defined by two primary phases: Retrieval (Sourcing the evidence) and Evaluation (Verifying the source).\n",
    "\n",
    "### The Execution Flow\n",
    "\n",
    "**1. Retrieval Phase (Engine)**: The system first receives the query and selects the retrieval strategy (BM25, Dense, Hybrid, or Cross-Encoder). \n",
    "\n",
    "**a√±adir que hace cada bm, dense e hybrid**\n",
    "\n",
    "If the advanced Cross-Encoder method is selected, the system executes a Two-Stage Retrieval funnel:\n",
    "- Stage 1 (Broad Search): The Hybrid Retriever fetches a wide pool of candidates (e.g., $k=20$).\n",
    "- Stage 2 (Fine Filter): The Cross-Encoder reads all 25 candidates and re-ranks them, selecting only the top $k=5$ highest-precision documents for the LLM.\n",
    "\n",
    "**2. Generation Phase (LLM)**: The final context and the strict prompt are sent to Gemini 2.5 Flash Lite for answer generation.\n",
    "\n",
    "**3. Verification Phase (Judge)**: The system immediately computes two critical metrics:\n",
    "\n",
    "- Accuracy Check: Compares the predicted letter (A/B/C/D) against the ground truth answer key.\n",
    "\n",
    "- Fidelity Check: Compares the retrieved documents against the specific reference text from the paper (paper_reference) using a Fuzzy Matching Judge or the LLM-as-a-Judge. This determines the Status Tag (e.g., Perfect RAG vs. Lucky Guess).\n",
    "\n",
    "**4. Logging**: All metrics (Time, Score, Status) are logged to the final CSV file.\n",
    "\n",
    "---\n",
    "The following code shows an example of steps 2 and 3 with one question. Note that this demo question can be changed [0-69] and that the run may seem slow due to an intended sleep time to avoid API rate limits problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077d1e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREGUNTA: What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?\n",
      "   [BASELINE] Procesando... ‚ö†Ô∏è ACIERTO SUERTE (Pred: C | T: 0.61s)\n",
      "   [BM25] Procesando... ‚ö†Ô∏è ACIERTO SUERTE (Pred: C | T: 0.50s)\n",
      "   [DENSE] Procesando... ‚úÖ ACIERTO PERFECTO (RAG) (Pred: C | T: 0.50s)\n",
      "   [HYBRID] Procesando... ‚úÖ ACIERTO PERFECTO (RAG) (Pred: C | T: 0.60s)\n",
      "   [CROSS_ENCODER] Procesando... ‚úÖ ACIERTO PERFECTO (RAG) (Pred: C | T: 2.20s)\n"
     ]
    }
   ],
   "source": [
    "# Aseguramos que la pregunta de demo est√© cargada (asumimos que questions[0] es la pregunta de prueba)\n",
    "path_json = \"./data/questions.json\"\n",
    "if not os.path.exists(path_json):\n",
    "    print(\"‚ùå ERROR: No encuentro 'data/questions.json'\")\n",
    "with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "if questions:\n",
    "    q = questions[0] # Change index for different questions\n",
    "    methods = [\"baseline\", \"bm25\", \"dense\", \"hybrid\", \"cross_encoder\"]\n",
    "\n",
    "    print(f\"PREGUNTA: {q['question']}\")\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"   [{method.upper()}] Procesando...\", end=\" \")\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # query_rag devuelve (raw_answer, retrieved_docs)\n",
    "        raw_answer, retrieved_docs = query_rag(\n",
    "            question=q['question'],\n",
    "            options=q['answers'],\n",
    "            method=method,\n",
    "            api_key=API_KEY\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        time.sleep(2)  # Peque√±a pausa para no saturar API\n",
    "\n",
    "        # Limpieza de respuesta\n",
    "        match = re.search(r'(?i)\\b([A-D])\\b', raw_answer)\n",
    "        predicted_letter = match.group(1).upper() if match else \"X\"\n",
    "\n",
    "        # Evaluaci√≥n b√°sica\n",
    "        correct_letter = q['correct_answer']\n",
    "        is_correct = (predicted_letter == correct_letter)\n",
    "\n",
    "        # 3. --- NUEVO: JUEZ DE GROUND TRUTH + LLM Judge ---\n",
    "        paper_ref = q.get('paper_reference', \"\")\n",
    "        found_evidence, evidence_score = False, 0.0\n",
    "\n",
    "        # --- SIMULACI√ìN DE LA EVALUACI√ìN ---\n",
    "        # Usamos la funci√≥n de verificaci√≥n para obtener el score real (necesario para el status)\n",
    "        found_evidence, retrieval_score = verify_ground_truth_v1(retrieved_docs, paper_ref)\n",
    "        \n",
    "        # L√≥gica del Status (Simplificada)\n",
    "        is_correct = (predicted_letter == correct_letter)\n",
    "        \n",
    "        if is_correct and found_evidence:\n",
    "            status_tag = \"‚úÖ ACIERTO PERFECTO (RAG)\"\n",
    "        elif is_correct and not found_evidence:\n",
    "            status_tag = \"‚ö†Ô∏è ACIERTO SUERTE\"\n",
    "        else:\n",
    "            status_tag = \"‚ùå INCORRECTO\"\n",
    "\n",
    "        print(f\"{status_tag} (Pred: {predicted_letter} | T: {latency:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab07eca",
   "metadata": {},
   "source": [
    "## 4.3 Execution and Results\n",
    "\n",
    "In this final phase, we execute the full benchmark to evaluate the performance of each retrieval strategy.\n",
    "\n",
    "Experiment Setup:\n",
    "\n",
    "**- Dataset**: 70 multiple-choice questions derived from the REFRAG paper.\n",
    "\n",
    "**- Methods Evaluated**: BM25, Dense, Hybrid, and Cross-Encoder.\n",
    "\n",
    "**- Metrics**: Accuracy, Latency, Retrieval Fidelity, and RAG Quality Diagnosis.\n",
    "\n",
    "**Note**: Running the full experiment takes approximately 15-20 minutes due to the computational cost of the Cross-Encoder and the API rate limits. For the purpose of this notebook, we will load the pre-computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fffb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"./results/demo_results.csv\"\n",
    "# 1. CARGA DE DATOS (Real vs. Demo)\n",
    "if os.path.exists(RESULTS_PATH):\n",
    "    print(f\"üìÇ Cargando resultados reales desde: {RESULTS_PATH}\")\n",
    "    df_results = pd.read_csv(RESULTS_PATH)\n",
    "else:\n",
    "    print(\"Error al cargar resultados reales.\")\n",
    "\n",
    "# 2. GENERACI√ìN DE DASHBOARD VISUAL\n",
    "print(\"\\nüìä Generando Dashboard de Rendimiento...\")\n",
    "setup_plot_style()\n",
    "\n",
    "# Creamos una figura grande para mostrar todo junto (Opcional, o llamamos a las funciones una a una)\n",
    "# Aqu√≠ llamamos a tus funciones de evaluation.py que guardan los PNGs y los mostramos.\n",
    "\n",
    "# Gr√°fica 1: Precisi√≥n\n",
    "plot_accuracy(df_results)\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fica 2: Calidad RAG (El gr√°fico de colores)\n",
    "plot_rag_quality(df_results)\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fica 3: Latencia\n",
    "plot_latency(df_results)\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fica 4: Fidelidad\n",
    "plot_retrieval_score(df_results)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ An√°lisis completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016eb13b",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "Based on the visualized data, we can draw the following conclusions:\n",
    "\n",
    "**Precision vs. Speed Trade-off**: The Cross-Encoder pipeline demonstrates superior accuracy and retrieval fidelity, effectively filtering out noise. However, this comes at the cost of higher latency (~3s vs 1s), confirming our hypothesis that Two-Stage Retrieval is ideal for offline or high-stakes QA, but Hybrid Retrieval is better for real-time applications.\n",
    "\n",
    "**The \"Luck\" Factor**: The RAG Quality plot reveals that a significant portion of \"correct\" answers in baseline methods (BM25/Dense) are classified as \"Lucky Guesses\" (Yellow). The Cross-Encoder significantly converts these into \"Perfect RAG\" (Green) by ensuring the retrieved context actually contains the semantic answer.\n",
    "\n",
    "**Overall Winner**: The Hybrid + Cross-Encoder architecture provides the most robust performance for scientific document analysis, minimizing hallucinations and reasoning failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbb2c07",
   "metadata": {},
   "source": [
    "# **5. Data Analysis and Inference**\n",
    "\n",
    "\n",
    "Our project automatically generates four plots associated with the metrics we use:\n",
    "\n",
    "1. Accuracy on answer selected\n",
    "2. RAG quality\n",
    "3. Latency\n",
    "4. Retrieval fidelity\n",
    "\n",
    "\n",
    "\n",
    "## 5.1. Defining Metrics and Data Collection\n",
    "\n",
    "The evaluation system in the src/evaluation.py module processes the raw data from queries.py to produce four key metrics for an integral performance assessment. This table details how data is collected and valued for each metric within the project:\n",
    "\n",
    "| Metric | Data Collection Method (Source Module) | Evaluation (Scale/Type) |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Accuracy on answer selected** | Comparison of the LLM's final chosen option against the `correct_answer` field in `questions.json`. | Binary (0 or 1). |\n",
    "| **2. RAG Quality** | Result of the **\"Judge\"** comparing the final answer to the retrieved context and ground truth. | Categorical (Correct, Lucky Answer, Reasoning Fail, Incorrect). |\n",
    "| **3. Latency** | Time for the retrieval and generation steps combined. | Continuous (s). |\n",
    "| **4. Retrieval fidelity** | Measures the relevance of the retrieved documents against the `source_reference` (ground truth chunk location). |  Ranking Metric (based on the position of the relevant chunk). |\n",
    "\n",
    "\n",
    "However, it is important not to ignore how the **\"Judge\"** is made for the RAG Quality, nor the function that evaluates the retrieval fidelity: \n",
    "\n",
    "\n",
    "### RAG Quality Judges\n",
    "\n",
    "The **RAG Quality** metric requires a **Judge** to determine whether the LLM's answer is *justified* by the retrieved context or if it was a **\"Lucky Answer\"**. We have explored several methodologies, implemented in the `src.evaluation.py` module through the functions `verify_ground_truth_v1`, `v2`, and `v3`:\n",
    "\n",
    "1.  **Judge V1 (Simple/Fuzzy Text Matching):** This uses substring search and the *Fuzzy Matching* algorithm (`SequenceMatcher` from `difflib`) in `verify_ground_truth_v1` and `verify_ground_truth_v3`. This approach is considered **conservative** and **literal**, requiring a match of $\\ge 50\\%$ of the ground truth text (`ground_truth_ref`) within the retrieved context. If it fails, the LLM is marked as \"hallucinating\" (Lucky  when the LLM got the right answer but without a correct finding; or a Reasoning Fail when it got the right finding but the wrong answer).\n",
    "\n",
    "2.  **Judge V2/V3 (LLM Judge):** This employs an LLM (`verify_ground_truth_v2` and `verify_ground_truth_v3`) to determine justification.\n",
    "    * **V2 (Literal-Tolerant):** Based on `JUDGE_PROMPT2`, it instructs the LLM to ignore only formatting artifacts (hyphens, line breaks) but demands a **literal match** of the word sequence.\n",
    "    * **V3 (Semantic):** Based on `JUDGE_PROMPT`, it instructs the LLM to look for **semantic matching** (same meaning) even if the words are different. We have implemented a two-stage cascading check judge. Judge V1 acts as a fast, low-cost filter, handling all deterministic verifications . Only when V1 fails to issue a positive verdict does the flow proceed use a LLM as the final contextual judge. This approach prioritizes efficiency with V1 while ensuring accuracy and nuanced evaluation with the LLM.\n",
    "\n",
    "### Retrieval fidelity\n",
    "\n",
    "The **Retrieval Fidelity** metric evaluates the quality of the search engine. It is calculated by measuring whether the text fragment containing the correct answer (the `ground_truth_ref` or reference *chunk*) was retrieved.\n",
    "\n",
    "The validation of whether the reference *chunk* was retrieved is performed using the `verify_ground_truth_v1` function or its variants. Once its presence is validated, the Retrieval Fidelity metric is defined as:\n",
    "\n",
    "$$\\text{Fidelity} = \\frac{\\text{Number of the letters in a sequence of the retrieved text that are exactly the same as the expected answer}}{\\text{Total number of letters in the expected answer}}$$\n",
    "\n",
    "This metric is **fundamental** because it acts as an upper bound for **RAG Quality**: if half of the absolute truth *chunk* is not retrieved, it is impossible for the LLM to answer justifiably (except by internal knowledge in the `baseline` case).\n",
    "\n",
    "\n",
    "## 5.2. The Evaluation Challenge: Defining \"Likeness\"\n",
    "Therefore, we will discuss each **pipeline's performance** in each metric described, but before that, we must take into account the **methodology** considered for the evaluation of the RAG quality and the retrieval fidelity judge. This is due to the fact that these metrics are attached to how we evaluate likeness and the threshold of a \"lucky answer.\" We will give an insight into this idea:\n",
    "\n",
    "- The plots in `results/persistent_results/27-11-1/plots/` are obtained through a judge that is based on text matching:\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"results/persistent_results/27-11-1/plots/2_rag_quality_pct.png\" alt=\"Text Matching Judge (Conservative)\" width=\"45%\"/>\n",
    "</center>\n",
    "\n",
    "- Whereas the ones in `results/persistent_results/8-11-1/plots/` have an **LLM judge** that searches for the semantic match:¬†\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"results/persistent_results/28-11-1/plots/2_rag_quality_pct.png\" alt=\"LLM Judge (Semantic)\" width=\"45%\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "Suddenly, all the pipelines have improved. However, when **manually supervised**, neither of the judges has a **flawless performance** at this task. The first judge **incorrectly** marks as \"lucky answers\" a group that is, in fact, a \"correct answer,\" and the second does the opposite.\n",
    "\n",
    "This **false positives** and **false negatives** concept is an extremely important idea in statistics and in other science fields, and the general conclusion for this problem is to be **conservative**.\n",
    "\n",
    "For this reason, we will mention other methodologies we have developed but only show and explore the ones with a risk-averse approach.\n",
    "\n",
    "For this reason, we have chosen the function **`verify_ground_truth_v1` (Judge V1 - Text/Fuzzy Matching)** to calculate the **RAG Quality** and **Retrieval Fidelity** metrics that will be presented next. This approach minimizes the possibility of **false positives**, ensuring the LLM is only marked as \"justified\" when there is clear, strict text matching evidence in the retrieved context.\n",
    "\n",
    "\n",
    "## 5.3. Pipeline Performance and Inference\n",
    "\n",
    "explicar como de bueno es cada pipeline en cada metrica y decir cual es mejor y en que condiciones\n",
    "\n",
    "## 5.4. Conclusion: Optimal Pipeline and Conditions\n",
    "The analysis of the four key metrics‚Äî**Accuracy**, **RAG Quality (Justification)**, **Latency**, and **Retrieval Fidelity**‚Äîallows us to establish the most suitable Retrieval-Augmented Generation (RAG) pipeline for this specific academic validation use case, balancing performance and cost.\n",
    "\n",
    "### üéØ The Necessity of RAG: The Control Group\n",
    "\n",
    "The **Baseline** method (no context retrieval) acts as our control group:\n",
    "* It achieves a low **Accuracy** of **65.7%** .\n",
    "* Its **RAG Quality (Justification)** is **0.0%**.\n",
    "\n",
    "The fact that all active RAG methods immediately jump to a maximum **Accuracy** of **98.6%** confirms that **RAG is essential** for solving the questions in the corpus. The analysis focuses, therefore, on **which pipeline offers the best justification and latency** above this high accuracy floor.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Hierarchical Progression and Performance\n",
    "\n",
    "Performance is analyzed in three stages to understand the marginal contribution of each technique:\n",
    "\n",
    "#### 1. Simple Engines (BM25 vs. Dense)\n",
    "Comparing the simple engines reveals the importance of retrieval method quality:\n",
    "\n",
    "* The **BM25** method (keyword search) achieves maximum **Accuracy** (**98.6%**) and a **RAG Quality** of **34%**. Its **Latency** is the lowest among active RAG methods (median of **0.70s**).\n",
    "* The **Dense** method (semantic vector search) has lower **Accuracy** (**90.0%**) and a slightly higher **RAG Quality** (**39%**), with similar latency.\n",
    "\n",
    "**Intermediate Conclusion:** For this type of technical corpus, the **BM25** method provides a more reliable and faster initial retrieval than the simple **Dense** vector search.\n",
    "\n",
    "#### 2. From Simple to Hybrid: The Justification Jump \n",
    "The **Hybrid** method combines the robustness of **BM25** with the semantic capability of **Dense**, achieving a drastic increase in justification:\n",
    "\n",
    "* **Improvement:** Moving from **BM25 (34%)** to **Hybrid (64%)**, the percentage of **Perfect Justification** nearly **doubles**. This occurs with only a marginal latency penalty, as the median only increases from 0.70s to 0.80s .\n",
    "\n",
    "**Intermediate Conclusion:** The **Hybrid Pipeline** provides the best performance foundation, achieving the highest justification with very low latency.\n",
    "\n",
    "#### 3. From Hybrid to Reranking (Cross-Encoder): The Cost of Refinement\n",
    "The **Cross-Encoder** method adds a costly re-ranking step to the Hybrid process:\n",
    "\n",
    "* **Quality vs. Latency:** The **Cross-Encoder RAG Quality** is **61%** , falling slightly *below* the simple **Hybrid** method (**64%**). However, the **Latency cost** is severe, increasing from **0.80s** (Hybrid) to **1.70s** (Cross-Encoder) .\n",
    "\n",
    "**Final Conclusion:** In this use case, re-ranking does not provide a quality improvement that justifies the latency penalty of over **100%**.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: The Optimal Pipeline\n",
    "\n",
    "The **Hybrid Pipeline** is the **optimal method** as it offers the best balance between Justification and Latency.\n",
    "\n",
    "#### Comprehensive Performance Table\n",
    "\n",
    "| Pipeline | Accuracy (%) | RAG Quality (Perfect Match) (%) | Latency (Median, s) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Baseline** | 65.7% | 0.0% | 0.55 |\n",
    "| **BM25** | **98.6%** | 34% | **0.70** |\n",
    "| **Dense** | 90.0% | 39% | 0.85 |\n",
    "| **Hybrid** | **98.6%** | **64%** | 0.80 |\n",
    "| **Cross-Encoder** | **98.6%** | 61% | 1.70 |\n",
    "\n",
    "### Conditions for Selection\n",
    "\n",
    "The choice of the final pipeline must be based on the application's priority:\n",
    "\n",
    "* **Maximum Efficiency (Speed + Quality):** The **Hybrid Pipeline** is the clear choice. It offers the highest justification rate with low latency. It is the most suitable for RAG systems in production.\n",
    "* **Absolute Priority to Justification (Ignoring Cost):** Although **Hybrid** is marginally superior, **Cross-Encoder** could be considered if the goal is to demonstrate the technical viability of the most advanced justification technique.\n",
    "* **Simple/Constrained Environment:** The **BM25** method remains a very strong option, offering maximum accuracy and low latency if infrastructure constraints prevent the use of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c8d7f",
   "metadata": {},
   "source": [
    "# **6. Authors**\n",
    "- Jorge Barbero Mor√°n ‚Äì UCM, Faculty of Mathematics\n",
    "- David Marcos Jimeno ‚Äì UCM, Faculty of Mathematics\n",
    "\n",
    "\n",
    "# **7. License**\n",
    "This project is licensed under the **MIT License**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_contest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
