{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48f3a07",
   "metadata": {},
   "source": [
    "# **Two-Stage-Retrieval LLM RAG**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cd76b",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "\n",
    "* [1. Introduction & Key Features](#1-introduction--key-features)\n",
    "* [2. Project Structure](#2-project-structure)\n",
    "    * [2.1 Input Data](#21-input-data)\n",
    "    * [2.2 Source Code](#22-source-code)\n",
    "    * [2.3 Output Data](#23-output-data)\n",
    "* [3. Workflow Summary](#3-workflow-summary)\n",
    "* [4. Demonstration](#4-demonstration)\n",
    "    * [4.1 Data Acquisition and Preparation](#41-data-acquisition-and-preparation)\n",
    "    * [4.2 Methodology and Pipeline Design](#32-methodology-and-pipeline-design)\n",
    "    * [4.3 Execution and Results](#43-execution-and-results)\n",
    "* [5. Data Analysis and Inference](#5-data-analysis-and-inference)\n",
    "    * [5.1. Defining Metrics and Data Collection](#51-defining-metrics-and-data-collection)\n",
    "    * [5.2. The Evaluation Challenge: Defining \"Likeness\"](#52-the-evaluation-challenge-defining-likeness)\n",
    "    * [5.3. Pipeline Performance and Inference](#53-pipeline-performance-and-inference)\n",
    "    * [5.4. Conclusion: Optimal Pipeline and Conditions](#54-conclusion-optimal-pipeline-and-conditions)\n",
    "* [6. Authors](#6-authors)\n",
    "* [7. License](#7-license)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a559e-12f3-4091-b474-6f4b0304930d",
   "metadata": {},
   "source": [
    "# **1. Introduction & Key Features**\n",
    "\n",
    "\n",
    "This project implements a question-answering (Q&A) system over a technical research paper from Meta SuperIntelligence Labs. The goal is to compare how different retrieval-augmented generation (RAG) setups help a large language model (LLM) answer multiple-choice questions.\n",
    "\n",
    "The following retrieval pipelines are compared:\n",
    "\n",
    "- **LLM Baseline** ‚Äì Direct generation without retrieval.\n",
    "\n",
    "- **BM25** ‚Äì Classic sparse keyword search (TF-IDF based).\n",
    "\n",
    "- **Dense Retrieval** ‚Äì Embedding-based semantic retrieval.\n",
    "\n",
    "- **(Bonus) Hybrid Retrieval** ‚Äì Combination of BM25 and Dense Retrieval.\n",
    "\n",
    "- **(Bonus) Hybrid Retrieval + Cross-Encoder** ‚Äì A SOTA approach where a high-recall retriever fetches candidates (Hybrid, k=20) and a Cross-Encoder reranks them for high precision (k=5).\n",
    "\n",
    "\n",
    "To ensure robustness, reproducibility, and scientific rigor, this project integrates several software engineering patterns:\n",
    "\n",
    "- **Singleton Pattern for Retrieval Engine:** The `RetrievalEngine` class uses a Singleton pattern with **Lazy Loading**. This prevents memory overhead by loading heavy models only when necessary and ensures consistent database connections across the pipeline.\n",
    "\n",
    "- **Smart Ingestion Strategy:** We compare **Recursive Chunking** (optimized with high overlap to preserve scientific context) against **Semantic Chunking** (experimental), which uses embedding distances to segment text by topic.\n",
    "\n",
    "- **Automated \"Judge\" Evaluation:** Instead of relying solely on the final answer accuracy, we implement a **Ground Truth Verification** system. It compares the retrieved chunks against the source reference in the dataset to detect lucky guesses.\n",
    "\n",
    "- **Two-Stage Architecture:** A funnel approach that prioritizes *Recall* in the first stage (Hybrid Search) and *Precision* in the second stage (Cross-Encoder Re-ranking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34e29e",
   "metadata": {},
   "source": [
    "# **2. Project Structure**\n",
    "\n",
    " The project is organized into multiple files under the src/ folder, with a clear separation of responsibilities. Here‚Äôs how it works:\n",
    "\n",
    "## 2.1 **Input Data**\n",
    "\n",
    "  All of the following are on the `data/` directory:\n",
    "- `**data/questions.json**` - Contains 50 multiple-choice questions extracted from a technical research paper.\n",
    "Each question includes: the correct answer, three distractors, and an optional reference to the source paper.\n",
    "\n",
    "- `**data/chroma_db**` - Embedded vector data base.\n",
    "- `**data/enunciado.pdf**` - The problem statement already described.\n",
    "- `**data/paper_refrag.pdf**` - The technical paper from which questions and answers are extracted.\n",
    "\n",
    "\n",
    "## 2.2 **Source Code**\n",
    "- `**main.py**` - Entry point of the project.\n",
    "Supports *Local* mode (`results/local_results/`) and *Persistent* mode (`results/persistent_results/<test_name>/`).  \n",
    "    - Calls the pipeline, saves final results, and generates plots.\n",
    "  \n",
    "\n",
    "  All of the following are on the `src/` directory:\n",
    "- `**launcher.py**` - Sets up the environment for experiments.\n",
    "    - Initializes the vector database (ChromaDB).\n",
    "    - Clears previous results if needed.\n",
    "    - Creates all necessary directories (plots, final CSVs, etc.).\n",
    "\n",
    "- `**src/ingestion.py**` - Prepares and creates the data base. Invoked by the launcher.\n",
    "\n",
    "- `**queries.py**` - Main logic to execute questions.\n",
    "    - For each question and method it:\n",
    "        - Sends the query to the LLM and retrieves documents.\n",
    "        - Computes accuracy and evidence verification.\n",
    "        - Stores partial results in results/resultados_parciales.csv.\n",
    "        - Returns a DataFrame with all results for further evaluation.\n",
    "\n",
    "- `**rag_pipeline.py**` - Implements RAG logic for different retrieval methods\n",
    "    - Contains functions to verify ground truth against retrieved documents.\n",
    "     Computes retrieval scores and status tags for each answer.\n",
    "\n",
    "- `**retrieval.py**` - Implements the retrieval engine.\n",
    "    - Provides a singleton engine to handle different retrieval methods efficiently.\n",
    "\n",
    "- `**evaluation.py**` - Evaluates the results and generates dashboards.\n",
    "    - Accuracy per method  \n",
    "    - RAG quality distribution  \n",
    "    - Response latency  \n",
    "    - Retrieval fidelity\n",
    "\n",
    "## 2.3 **Output Data**\n",
    "\n",
    "    Found on the `results/` directory:\n",
    "\n",
    "- **Partial results** - Always stored in `results/resultados_parciales.csv`.\n",
    "    - Updated after each question is processed.\n",
    "\n",
    "- **Final results** - Stored in `results/local_results/` or `results/persistent_results/<test_name>`.\n",
    "    - File name: `resultados_finales.csv`.\n",
    "\n",
    "- **Plots/Dashboard** - Stored in `plots/` inside the corresponding results folder.\n",
    "    - Include:\n",
    "        - Bar charts for accuracy and RAG quality\n",
    "        - Boxplots for response latency\n",
    "        - Violin plots for retrieval fidelity\n",
    "\n",
    "> **Tip:** Run `main.py try_n` on the terminal in orden to save the try number n results in that directory<br> in orden not to overwrite other results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832372ef",
   "metadata": {},
   "source": [
    "# **3. Workflow Summary**\n",
    "\n",
    "1. Load questions dataset (questions.json).\n",
    "\n",
    "2. Initialize vector database (ChromaDB).\n",
    "\n",
    "3. For each question:\n",
    "    - Retrieve relevant documents (BM25 / Dense / Hybrid / Hybrid + Cross-Encoder).\n",
    "    - Query LLM for an answer.\n",
    "    - Verify against ground truth.\n",
    "    - Save partial results.\n",
    "\n",
    "4. After all questions are processed:\n",
    "    - Concatenate all results.\n",
    "    - Save final results CSV.\n",
    "    - Generate plots/dashboard for comparison and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc109c3d",
   "metadata": {},
   "source": [
    "# **4. Demonstration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89a3e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\miniconda3\\envs\\rag_contest\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entorno configurado y librer√≠as cargadas.\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS Y SETUP ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# A√±adimos la ra√≠z del proyecto al path para poder importar 'src'\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "# Importamos nuestros m√≥dulos de ingenier√≠a\n",
    "from src.retrieval import RetrievalEngine\n",
    "from src.rag_pipeline import query_rag\n",
    "from src.evaluation import generate_dashboard\n",
    "from src.queries import run_questions\n",
    "\n",
    "# Cargar API Key\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Configuraci√≥n visual para Pandas\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"‚úÖ Entorno configurado y librer√≠as cargadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6216694",
   "metadata": {},
   "source": [
    "## 4.1 Data Ingestions and Chunking Strategy\n",
    "\n",
    "The initial stage of the RAG pipeline focuses on **Data Pre-processing**. This involves breaking down the source document (`paper_refrag.pdf`) into discrete units of knowledge (**chunks**) that are stored in **ChromaDB**.\n",
    "\n",
    "### Chunking Methodologies\n",
    "\n",
    "We implemented two primary chunking strategies, selectable via the `CHUNKING_METHOD` variable in `src/ingestion.py`:\n",
    "\n",
    "1.  **Recursive Chunking (Recursive Character Splitter):** This is the baseline method, optimized for technical context. It relies on rules of fixed length (e.g., **1200 characters** with **350 characters of overlap**).\n",
    "\n",
    "2.  **Semantic Chunking:** The advanced method. It utilizes an **embedding model** to compute the similarity between consecutive sentences, identifying natural **topic shifts** to define chunk boundaries.\n",
    "\n",
    "### Why Semantic Chunking?\n",
    "\n",
    "We chose to focus on **Semantic Chunking** because it addresses the primary weakness of RAG in technical domains: **avoiding false negatives** in retrieval.\n",
    "\n",
    "In dense scientific papers, naive splitting often cuts conclusions, cross-references, or complex sentences in half. This leads to:\n",
    "\n",
    "* **Loss of Context:** The embedding model creates an inaccurate vector because the sentence is fragmented.\n",
    "\n",
    "* **Lucky guesses:** The system answers correctly, but the strict evaluation judge fails to find the complete reference phrase because it was broken up.\n",
    "\n",
    "By using **Semantic Chunking**, we ensure that each unit of information passed to the retriever has the **highest possible semantic coherence**, which directly improves the **Retrieval Recall** and **Fidelity** metrics in our evaluation dashboard.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Once the text is chunked, the system proceeds to **initialize the Retriever** in a **Lazy Loading** state, ensuring the database connection is established efficiently right before the first query is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34afb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîå Desconectando motor de b√∫squeda...\n",
      "üìÑ Cargando PDF...\n",
      "   -> PDF cargado: 30 p√°ginas.\n",
      "‚úÇÔ∏è Procesando fragmentos\n",
      "   -> Generados 81 fragmentos.\n",
      "üß† Guardando vectores en disco...\n",
      "üíæ Base de datos guardada exitosamente.\n",
      "‚úÖ Setup completado.\n",
      "\n",
      "‚öôÔ∏è Conectando al motor de recuperaci√≥n...\n",
      "‚úÖ SISTEMA OPERATIVO\n",
      "   - Estado: Base de Datos Conectada\n",
      "   - Ruta: ./data/chroma_db\n",
      "   - Chunks Indexados: 81\n",
      "   - Modelo Embeddings: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# --- GESTI√ìN DE DATOS Y VERIFICACI√ìN ---\n",
    "import os\n",
    "from src.ingestion import db_setup\n",
    "from src.retrieval import RetrievalEngine\n",
    "\n",
    "try:\n",
    "    # 1. GARANT√çA DE EXISTENCIA DE BD (Ingesta Manual)\n",
    "    db_setup(rebuild_db=True, chunking_method=\"semantic\") # Opciones: \"recursive\", \"semantic\"\n",
    "    \n",
    "    # 2. CONEXI√ìN AL MOTOR (Singleton)\n",
    "    print(\"\\n‚öôÔ∏è Conectando al motor de recuperaci√≥n...\")\n",
    "    engine = RetrievalEngine.get_instance()\n",
    "    \n",
    "    # 3. PRUEBA DE RECUPERADOR\n",
    "    # Esto asegura que ChromaDB y los Embeddings est√°n cargados en RAM\n",
    "    test_retriever = engine.get_retriever(\"hybrid\", k=1) # As√≠ aseguramos de que se inicien dense y bm25\n",
    "    \n",
    "    # Obtenemos estad√≠sticas reales de la BD\n",
    "    if engine.db:\n",
    "        doc_count = engine.db._collection.count()\n",
    "        print(f\"‚úÖ SISTEMA OPERATIVO\")\n",
    "        print(f\"   - Estado: Base de Datos Conectada\")\n",
    "        print(f\"   - Ruta: ./data/chroma_db\")\n",
    "        print(f\"   - Chunks Indexados: {doc_count}\")\n",
    "        print(f\"   - Modelo Embeddings: all-MiniLM-L6-v2\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR CR√çTICO DE INICIALIZACI√ìN: {e}\")\n",
    "    print(\"   - Verifica que la base de datos est√© construida correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbb2c07",
   "metadata": {},
   "source": [
    "# **5. Data Analysis and Inference**\n",
    "\n",
    "\n",
    "Our project automatically generates four plots associated with the metrics we use:\n",
    "\n",
    "1. Accuracy on answer selected\n",
    "2. RAG quality\n",
    "3. Latency\n",
    "4. Retrieval fidelity\n",
    "\n",
    "\n",
    "\n",
    "## 5.1. Defining Metrics and Data Collection\n",
    "\n",
    "The evaluation system in the src/evaluation.py module processes the raw data from queries.py to produce four key metrics for an integral performance assessment. This table details how data is collected and valued for each metric within the project:\n",
    "\n",
    "| Metric | Data Collection Method (Source Module) | Evaluation (Scale/Type) |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Accuracy on answer selected** | Comparison of the LLM's final chosen option against the `correct_answer` field in `questions.json`. | Binary (0 or 1). |\n",
    "| **2. RAG Quality** | Result of the **\"Judge\"** comparing the final answer to the retrieved context and ground truth. | Categorical (Correct, Lucky Answer, Reasoning Fail, Incorrect). |\n",
    "| **3. Latency** | Time for the retrieval and generation steps combined. | Continuous (s). |\n",
    "| **4. Retrieval fidelity** | Measures the relevance of the retrieved documents against the `source_reference` (ground truth chunk location). |  Ranking Metric (based on the position of the relevant chunk). |\n",
    "\n",
    "\n",
    "However, it is important not to ignore how the **\"Judge\"** is made for the RAG Quality, nor the function that evaluates the retrieval fidelity: \n",
    "\n",
    "\n",
    "### RAG Quality Judges\n",
    "\n",
    "The **RAG Quality** metric requires a **Judge** to determine whether the LLM's answer is *justified* by the retrieved context or if it was a **\"Lucky Answer\"**. We have explored several methodologies, implemented in the `src.evaluation.py` module through the functions `verify_ground_truth_v1`, `v2`, and `v3`:\n",
    "\n",
    "1.  **Judge V1 (Simple/Fuzzy Text Matching):** This uses substring search and the *Fuzzy Matching* algorithm (`SequenceMatcher` from `difflib`) in `verify_ground_truth_v1` and `verify_ground_truth_v3`. This approach is considered **conservative** and **literal**, requiring a match of $\\ge 50\\%$ of the ground truth text (`ground_truth_ref`) within the retrieved context. If it fails, the LLM is marked as \"hallucinating\" (Lucky  when the LLM got the right answer but without a correct finding; or a Reasoning Fail when it got the right finding but the wrong answer).\n",
    "\n",
    "2.  **Judge V2/V3 (LLM Judge):** This employs an LLM (`verify_ground_truth_v2` and `verify_ground_truth_v3`) to determine justification.\n",
    "    * **V2 (Literal-Tolerant):** Based on `JUDGE_PROMPT2`, it instructs the LLM to ignore only formatting artifacts (hyphens, line breaks) but demands a **literal match** of the word sequence.\n",
    "    * **V3 (Semantic):** Based on `JUDGE_PROMPT`, it instructs the LLM to look for **semantic matching** (same meaning) even if the words are different.\n",
    "\n",
    "### Retrieval fidelity\n",
    "\n",
    "The **Retrieval Fidelity** metric evaluates the quality of the search engine. It is calculated by measuring whether the text fragment containing the correct answer (the `ground_truth_ref` or reference *chunk*) was retrieved.\n",
    "\n",
    "The validation of whether the reference *chunk* was retrieved is performed using the `verify_ground_truth_v1` function or its variants. Once its presence is validated, the Retrieval Fidelity metric is defined as:\n",
    "\n",
    "$$\\text{Fidelity} = \\frac{\\text{Number of the letters in a sequence of the retrieved text that are exactly the same as the expected answer}}{\\text{Total number of letter in the expected answer}}$$\n",
    "\n",
    "This metric is **fundamental** because it acts as an upper bound for **RAG Quality**: if half of the absolute truth *chunk* is not retrieved, it is impossible for the LLM to answer justifiably (except by internal knowledge in the `baseline` case).\n",
    "\n",
    "\n",
    "## 5.2. The Evaluation Challenge: Defining \"Likeness\"\n",
    "Therefore, we will discuss each **pipeline's performance** in each metric described, but before that, we must take into account the **methodology** considered for the evaluation of the RAG quality and the retrieval fidelity judge. This is due to the fact that these metrics are attached to how we evaluate likeness and the threshold of a \"lucky answer.\" We will give an insight into this idea:\n",
    "\n",
    "- The plots in `results/persistent_results/27-11-1/plots/` are obtained through a judge that is based on text matching:\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"results/persistent_results/27-11-1/plots/2_rag_quality_pct.png\" alt=\"Text Matching Judge (Conservative)\" width=\"45%\"/>\n",
    "</center>\n",
    "\n",
    "- Whereas the ones in `results/persistent_results/8-11-1/plots/` have an **LLM judge** that searches for the semantic match:¬†\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"results/persistent_results/28-11-1/plots/2_rag_quality_pct.png\" alt=\"LLM Judge (Semantic)\" width=\"45%\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "Suddenly, all the pipelines have improved. However, when **manually supervised**, neither of the judges has a **flawless performance** at this task. The first judge **incorrectly** marks as \"lucky answers\" a group that is, in fact, a \"correct answer,\" and the second does the opposite.\n",
    "\n",
    "This **false positives** and **false negatives** concept is an extremely important idea in statistics and in other science fields, and the general conclusion for this problem is to be **conservative**.\n",
    "\n",
    "For this reason, we will mention other methodologies we have developed but only show and explore the ones with a risk-averse approach.\n",
    "\n",
    "For this reason, we have chosen the function **`verify_ground_truth_v1` (Judge V1 - Text/Fuzzy Matching)** to calculate the **RAG Quality** and **Retrieval Fidelity** metrics that will be presented next. This approach minimizes the possibility of **false positives**, ensuring the LLM is only marked as \"justified\" when there is clear, strict text matching evidence in the retrieved context.\n",
    "\n",
    "\n",
    "## 5.3. Pipeline Performance and Inference\n",
    "\n",
    "explicar como de bueno es cada pipeline en cada metrica y decir cual es mejor y en que condiciones\n",
    "\n",
    "## 5.4. Conclusion: Optimal Pipeline and Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c8d7f",
   "metadata": {},
   "source": [
    "# **6. Authors**\n",
    "- Jorge Barbero Mor√°n ‚Äì UCM, Faculty of Mathematics\n",
    "- David Marcos Jimeno ‚Äì UCM, Faculty of Mathematics\n",
    "\n",
    "\n",
    "# **7. License**\n",
    "This project is licensed under the **MIT License**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
