question_id,method,correct,predicted,ground_truth,response_time,raw_output,status,retrieval_score,retrieved_docs
1,baseline,True,C,C,0.4,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
1,bm25,True,C,C,0.86,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0225,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='the reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over')]"
1,dense,False,A,C,0.63,A,❌ FALLO TOTAL,0.0125,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='the reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='model’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17')]"
1,hybrid,True,C,C,0.54,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0125,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='the reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='model’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17')]"
2,baseline,True,B,B,0.75,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
2,bm25,True,B,B,0.68,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='input (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='as input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMAK:Similar to theLLaMA-Full Context, we pass lastK tokens xsK:s in addition toxs+1:s+o to\ncompute perplexity inxs+1:s+o. The performance ofLLaMAK falls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.')]"
2,dense,False,A,B,0.49,A,❌ FALLO TOTAL,0.0332409972299169,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at')]"
2,hybrid,True,B,B,0.66,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='input (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='as input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMAK:Similar to theLLaMA-Full Context, we pass lastK tokens xsK:s in addition toxs+1:s+o to\ncompute perplexity inxs+1:s+o. The performance ofLLaMAK falls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at')]"
3,baseline,True,D,D,0.45,D,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
3,bm25,True,D,D,0.65,D,✅ ACIERTO PERFECTO (RAG),0.8235294117647058,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='This observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.')]"
3,dense,True,D,D,0.68,D,⚠️ ACIERTO SUERTE (Sin Evidencia),0.00980392156862745,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='model’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote')]"
3,hybrid,True,D,D,0.69,D,⚠️ ACIERTO SUERTE (Sin Evidencia),0.00980392156862745,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='This observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='model’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24')]"
4,baseline,True,C,C,0.68,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
4,bm25,True,C,C,0.77,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.016241299303944315,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only')]"
4,dense,True,C,C,0.9,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.002320185614849188,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×\nREFRAG8 + 8 passages 50.2992.27 99.66 94.70 45.23 68.94 71.38 57.701×\nREFRAG16+ 8 passages 49.8489.18 99.66 98.0139.3368.42 70.29 56.672×\nREFRAG32+ 8 passages 49.5191.75 99.50 97.35 42.86 68.17 68.34 56.754×\nLong context\nLLaMAFT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -\nLLaMA-32K+80 passages 22.21 16.49 19.80 16.56 23.76 24.16 34.17 48.86\nREFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging')]"
4,hybrid,True,C,C,0.58,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.016241299303944315,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×\nREFRAG8 + 8 passages 50.2992.27 99.66 94.70 45.23 68.94 71.38 57.701×\nREFRAG16+ 8 passages 49.8489.18 99.66 98.0139.3368.42 70.29 56.672×\nREFRAG32+ 8 passages 49.5191.75 99.50 97.35 42.86 68.17 68.34 56.754×\nLong context\nLLaMAFT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -\nLLaMA-32K+80 passages 22.21 16.49 19.80 16.56 23.76 24.16 34.17 48.86\nREFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging')]"
5,baseline,True,C,C,0.65,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
5,bm25,True,C,C,0.76,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0025380710659898475,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='superior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='chunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while'), Document(metadata={'page': 21, 'source': './data/paper_refrag.pdf'}, page_content='Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='bility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we\nshow that machine learning is an efficient and accurate method\nto compute the self - energy of the model and to predict the spec-\ntral function of the model . we also show that machine learning\nalgorithms can be used to efficiently compute the self - consistent\ngreen s function starting from any hybridization function .\nparticle swarm optimization is used in several combinatorial op-\ntimization problems . in this work , particle swarms are used to\nsolve quadratic programming problems with quadratic constraints'), Document(metadata={'page': 21, 'source': './data/paper_refrag.pdf'}, page_content='P0P1P2P3P4\nlayer 23\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 24\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 25\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 26\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 27\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 28\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 29\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 30\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 31\nall layers avg\nFigure 7Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model.\nThe diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are\nthe averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10.\nAdditional results in latency measurement.Figure 9 and figure 8 shows the latency comparison of different\nmodels when usingk= 8andk= 32compression rate forREFRAGrespectively.\nAblation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.')]"
5,dense,True,C,C,0.94,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005076142131979695,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='model’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 21}, page_content='Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='the reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate')]"
5,hybrid,True,C,C,0.86,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005076142131979695,"[Document(metadata={'page': 21, 'source': './data/paper_refrag.pdf'}, page_content='Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='model’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='superior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='chunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='bility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we\nshow that machine learning is an efficient and accurate method\nto compute the self - energy of the model and to predict the spec-\ntral function of the model . we also show that machine learning\nalgorithms can be used to efficiently compute the self - consistent\ngreen s function starting from any hybridization function .\nparticle swarm optimization is used in several combinatorial op-\ntimization problems . in this work , particle swarms are used to\nsolve quadratic programming problems with quadratic constraints'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.'), Document(metadata={'page': 21, 'source': './data/paper_refrag.pdf'}, page_content='P0P1P2P3P4\nlayer 23\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 24\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 25\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 26\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 27\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 28\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 29\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 30\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 31\nall layers avg\nFigure 7Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model.\nThe diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are\nthe averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10.\nAdditional results in latency measurement.Figure 9 and figure 8 shows the latency comparison of different\nmodels when usingk= 8andk= 32compression rate forREFRAGrespectively.\nAblation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='the reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate')]"
