question_id,method,correct,predicted,ground_truth,response_time,raw_output,status,retrieval_score,retrieved_docs
1,baseline,True,C,C,0.51,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
1,bm25,False,B,C,1.14,B,❌ FALLO TOTAL,0.0018248175182481751,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.')]"
1,dense,True,C,C,1.14,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24')]"
1,hybrid,True,C,C,0.94,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.')]"
1,cross_encoder,True,C,C,2.18,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24')]"
2,baseline,True,C,C,0.8,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
2,bm25,True,C,C,0.94,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0225,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture')]"
2,dense,False,A,C,0.73,A,❌ FALLO TOTAL,0.0125,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%')]"
2,hybrid,True,C,C,1.14,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0225,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%')]"
2,cross_encoder,True,C,C,2.21,C,✅ ACIERTO PERFECTO (RAG),0.9925,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.')]"
3,baseline,True,B,B,0.58,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
3,bm25,True,B,B,0.62,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.')]"
3,dense,True,B,B,0.83,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0332409972299169,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
3,hybrid,True,B,B,1.03,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
3,cross_encoder,True,B,B,1.96,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across')]"
4,baseline,True,D,D,0.42,D,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
4,bm25,True,D,D,0.92,D,⚠️ ACIERTO SUERTE (Sin Evidencia),0.00980392156862745,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant')]"
4,dense,True,D,D,0.93,D,⚠️ ACIERTO SUERTE (Sin Evidencia),0.00980392156862745,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24')]"
4,hybrid,True,D,D,1.11,D,⚠️ ACIERTO SUERTE (Sin Evidencia),0.00980392156862745,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24')]"
4,cross_encoder,True,D,D,1.66,D,⚠️ ACIERTO SUERTE (Sin Evidencia),0.004901960784313725,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant')]"
5,baseline,True,C,C,0.62,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
5,bm25,True,C,C,0.93,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.004640371229698376,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 24}, page_content='the brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema,\naccording to a group of researchers, was brought on by hyponatraemia. In their study, which was\npublished in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because\nhis kidneys were unable to eliminate extra water. The findings are very different from old theories\nabout how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops\nwhen the body’s sodium levels get diluted as a result of consuming too much water. The cells in\nthe body, particularly those in the brain,\nP2 circumstances, you’re bound to get some truly insane conspiracy theories, and there are plenty\nabout Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big\nmistake after Bruce Lee’s death. Hoping to protect Lee’s image, Chow’s production company\nclaimed the actor died at home with his wife, Linda. But once the press found out the truth, the\ntabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved')]"
5,dense,True,C,C,0.93,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.002320185614849188,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×\nREFRAG8 + 8 passages 50.2992.27 99.66 94.70 45.23 68.94 71.38 57.701×\nREFRAG16+ 8 passages 49.8489.18 99.66 98.0139.3368.42 70.29 56.672×\nREFRAG32+ 8 passages 49.5191.75 99.50 97.35 42.86 68.17 68.34 56.754×\nLong context\nLLaMAFT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -\nLLaMA-32K+80 passages 22.21 16.49 19.80 16.56 23.76 24.16 34.17 48.86\nREFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='LLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95\nREFRAG16 4 21.84 15.2126.12\nREFRAG32 4 21.75 15.33 25.77\nLLaMAFT 6 14.44 10.72 19.52\nREFRAG8 6 20.5911.0025.16\nREFRAG16 6 21.05 10.50 24.96\nREFRAG32 621.6710.7925.23\nTable 5Performance on multi-turn RAG tasks with different number of passages.\nREFRAG LLaMA FT\n# Passages ORConvQA QReCC TopiOCQA↑ORConvQA QReCC TopiOCQA↑\n0 19.27 15.32 28.19 19.16 15.49 28.22\n5 20.18 17.3728.2419.6518.7127.08\n820.5217.60 28.17 16.87 18.05 25.36\n10 19.67 17.41 27.62 15.72 17.42 23.60\n6 Related Works\nRetrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi-\ntectures to improve retrieval-augmented generation. Guu et al. (2020) introduced pre-training for retrieval-\naugmented masked language models. Building on this, Borgeaud et al. (2022) proposed a new architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG')]"
5,hybrid,True,C,C,1.13,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.004640371229698376,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×\nREFRAG8 + 8 passages 50.2992.27 99.66 94.70 45.23 68.94 71.38 57.701×\nREFRAG16+ 8 passages 49.8489.18 99.66 98.0139.3368.42 70.29 56.672×\nREFRAG32+ 8 passages 49.5191.75 99.50 97.35 42.86 68.17 68.34 56.754×\nLong context\nLLaMAFT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -\nLLaMA-32K+80 passages 22.21 16.49 19.80 16.56 23.76 24.16 34.17 48.86\nREFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 24}, page_content='the brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema,\naccording to a group of researchers, was brought on by hyponatraemia. In their study, which was\npublished in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because\nhis kidneys were unable to eliminate extra water. The findings are very different from old theories\nabout how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops\nwhen the body’s sodium levels get diluted as a result of consuming too much water. The cells in\nthe body, particularly those in the brain,\nP2 circumstances, you’re bound to get some truly insane conspiracy theories, and there are plenty\nabout Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big\nmistake after Bruce Lee’s death. Hoping to protect Lee’s image, Chow’s production company\nclaimed the actor died at home with his wife, Linda. But once the press found out the truth, the\ntabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='LLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95\nREFRAG16 4 21.84 15.2126.12\nREFRAG32 4 21.75 15.33 25.77\nLLaMAFT 6 14.44 10.72 19.52\nREFRAG8 6 20.5911.0025.16\nREFRAG16 6 21.05 10.50 24.96\nREFRAG32 621.6710.7925.23\nTable 5Performance on multi-turn RAG tasks with different number of passages.\nREFRAG LLaMA FT\n# Passages ORConvQA QReCC TopiOCQA↑ORConvQA QReCC TopiOCQA↑\n0 19.27 15.32 28.19 19.16 15.49 28.22\n5 20.18 17.3728.2419.6518.7127.08\n820.5217.60 28.17 16.87 18.05 25.36\n10 19.67 17.41 27.62 15.72 17.42 23.60\n6 Related Works\nRetrieval-Augmented Language Modeling.Recent research has extensively investigated novel model archi-\ntectures to improve retrieval-augmented generation. Guu et al. (2020) introduced pre-training for retrieval-\naugmented masked language models. Building on this, Borgeaud et al. (2022) proposed a new architecture'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG')]"
5,cross_encoder,True,C,C,2.16,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.002320185614849188,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
6,baseline,True,C,C,0.49,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
6,bm25,True,C,C,0.76,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.007614213197969543,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='indicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we\nshow that machine learning is an efficient and accurate method\nto compute the self - energy of the model and to predict the spec-\ntral function of the model . we also show that machine learning\nalgorithms can be used to efficiently compute the self - consistent\ngreen s function starting from any hybridization function .\nparticle swarm optimization is used in several combinatorial op-\ntimization problems . in this work , particle swarms are used to\nsolve quadratic programming problems with quadratic constraints\n. the approach of particle swarms is an example for interior point\nmethods in optimization as an iterative technique . this approach\nis novel and deals with classification problems without the use\nof a traditional classifier . our method determines the optimal\nhyperplane or classification boundary for a data set . in a bi-'), Document(metadata={'page': 19, 'source': './data/paper_refrag.pdf'}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 21}, page_content='P0P1P2P3P4\nlayer 22\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 23\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 24\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 25\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 26\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 27\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 28\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 29\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 30\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 31\nall layers avg\nFigure 7Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model.\nThe diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are\nthe averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10.\nAdditional results in latency measurement.Figure 9 and figure 8 shows the latency comparison of different\nmodels when usingk= 8andk= 32compression rate forREFRAGrespectively.\nAblation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
6,dense,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),0.6269035532994924,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 19}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
6,hybrid,True,C,C,1.14,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.4517766497461929,"[Document(metadata={'page': 19, 'source': './data/paper_refrag.pdf'}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='indicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we\nshow that machine learning is an efficient and accurate method\nto compute the self - energy of the model and to predict the spec-\ntral function of the model . we also show that machine learning\nalgorithms can be used to efficiently compute the self - consistent\ngreen s function starting from any hybridization function .\nparticle swarm optimization is used in several combinatorial op-\ntimization problems . in this work , particle swarms are used to\nsolve quadratic programming problems with quadratic constraints\n. the approach of particle swarms is an example for interior point\nmethods in optimization as an iterative technique . this approach\nis novel and deals with classification problems without the use\nof a traditional classifier . our method determines the optimal\nhyperplane or classification boundary for a data set . in a bi-'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 21}, page_content='P0P1P2P3P4\nlayer 22\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 23\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 24\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 25\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 26\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 27\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 28\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 29\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 30\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 31\nall layers avg\nFigure 7Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model.\nThe diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are\nthe averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10.\nAdditional results in latency measurement.Figure 9 and figure 8 shows the latency comparison of different\nmodels when usingk= 8andk= 32compression rate forREFRAGrespectively.\nAblation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
6,cross_encoder,True,C,C,2.23,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.4517766497461929,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5')]"
7,baseline,True,C,C,0.74,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
7,bm25,True,C,C,1.03,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the')]"
7,dense,True,C,C,1.13,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='Motivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices\nl1, . . . , lT′, wherel t ∈[L]. At staget, the policy samples from:\nπθ(lt =i|x,{l j}t−1\nj=1) :=π θ(lt =i|{c j}L\nj=1,{l j}t−1\nj=1) = exp(si −ni)PL\nj=1 exp(sj −nj)\n.\nwhere nj = ∞ iff j∈ {li}t−1\ni=1 and0otherwise 4;s= gθ({ci}i∈[L],i/∈{lj}t−1\nj=1\n)is the output of a two-layer\ntransformer network over chunk embeddings, producing logitsi for each chunk. In practice, we reuse chunk\nembeddings {ci}L\ni=1 as transformer input and do not recompute logitssi after each selection, as state changes\nhave minimal impact and this improves training speed.\nWe use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to\nminimize contamination across different segment prediction task. Specifically, for eachx we randomly select\nGnumber of lengthT ′ action sequences{l (i)}G\ni=1 . We have the following objective:\nJθ = 1\nG\nPG\ni=1 E x∼P(X),\n{l(i)}G\ni=1∼πθ([L]|x)\n1\nT′\nPT′\nt=1 min\n\x14\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
7,hybrid,True,C,C,1.14,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='Motivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices\nl1, . . . , lT′, wherel t ∈[L]. At staget, the policy samples from:\nπθ(lt =i|x,{l j}t−1\nj=1) :=π θ(lt =i|{c j}L\nj=1,{l j}t−1\nj=1) = exp(si −ni)PL\nj=1 exp(sj −nj)\n.\nwhere nj = ∞ iff j∈ {li}t−1\ni=1 and0otherwise 4;s= gθ({ci}i∈[L],i/∈{lj}t−1\nj=1\n)is the output of a two-layer\ntransformer network over chunk embeddings, producing logitsi for each chunk. In practice, we reuse chunk\nembeddings {ci}L\ni=1 as transformer input and do not recompute logitssi after each selection, as state changes\nhave minimal impact and this improves training speed.\nWe use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to\nminimize contamination across different segment prediction task. Specifically, for eachx we randomly select\nGnumber of lengthT ′ action sequences{l (i)}G\ni=1 . We have the following objective:\nJθ = 1\nG\nPG\ni=1 E x∼P(X),\n{l(i)}G\ni=1∼πθ([L]|x)\n1\nT′\nPT′\nt=1 min\n\x14\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
7,cross_encoder,True,C,C,1.85,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16')]"
8,baseline,False,A,B,0.56,A,❌ FALLO TOTAL,0.0,[]
8,bm25,True,B,B,0.8,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005714285714285714,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='is the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to\nmachine learning . however , its use in nlp tasks has been limited\nby its over - generalisation properties . this paper investigates two\noptimisations that can be applied to memory - based learning in\norder to improve its generalisation performance : ( 1 ) replacing\ninstance types ( memory tokens ) by instance types with frequency\ninformation , and ( 2 ) removing redundant information ( i.e. ,\ninstance types with low predictive power ) . we perform exper-\niments on a large data set of english word pronunciations . we\nshow that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low')]"
8,dense,True,B,B,1.33,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.011428571428571429,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%')]"
8,hybrid,True,B,B,1.15,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005714285714285714,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='is the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to\nmachine learning . however , its use in nlp tasks has been limited\nby its over - generalisation properties . this paper investigates two\noptimisations that can be applied to memory - based learning in\norder to improve its generalisation performance : ( 1 ) replacing\ninstance types ( memory tokens ) by instance types with frequency\ninformation , and ( 2 ) removing redundant information ( i.e. ,\ninstance types with low predictive power ) . we perform exper-\niments on a large data set of english word pronunciations . we\nshow that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%')]"
8,cross_encoder,True,B,B,2.13,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.011428571428571429,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The')]"
9,baseline,True,C,C,1.01,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
9,bm25,True,C,C,0.64,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented')]"
9,dense,True,C,C,1.03,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
9,hybrid,True,C,C,1.24,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
9,cross_encoder,True,C,C,2.98,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95')]"
10,baseline,True,C,C,1.14,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
10,bm25,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can')]"
10,dense,True,C,C,1.04,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
10,hybrid,True,C,C,1.24,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
10,cross_encoder,True,C,C,1.96,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95')]"
11,baseline,False,A,C,0.83,A,❌ FALLO TOTAL,0.0,[]
11,bm25,True,C,C,1.0,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 24}, page_content='the brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema,\naccording to a group of researchers, was brought on by hyponatraemia. In their study, which was\npublished in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because\nhis kidneys were unable to eliminate extra water. The findings are very different from old theories\nabout how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops\nwhen the body’s sodium levels get diluted as a result of consuming too much water. The cells in\nthe body, particularly those in the brain,\nP2 circumstances, you’re bound to get some truly insane conspiracy theories, and there are plenty\nabout Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big\nmistake after Bruce Lee’s death. Hoping to protect Lee’s image, Chow’s production company\nclaimed the actor died at home with his wife, Linda. But once the press found out the truth, the\ntabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight')]"
11,dense,True,C,C,0.67,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0031446540880503146,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'page': 10, 'source': './data/paper_refrag.pdf'}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model')]"
11,hybrid,True,C,C,1.15,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 24}, page_content='the brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema,\naccording to a group of researchers, was brought on by hyponatraemia. In their study, which was\npublished in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because\nhis kidneys were unable to eliminate extra water. The findings are very different from old theories\nabout how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops\nwhen the body’s sodium levels get diluted as a result of consuming too much water. The cells in\nthe body, particularly those in the brain,\nP2 circumstances, you’re bound to get some truly insane conspiracy theories, and there are plenty\nabout Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big\nmistake after Bruce Lee’s death. Hoping to protect Lee’s image, Chow’s production company\nclaimed the actor died at home with his wife, Linda. But once the press found out the truth, the\ntabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model')]"
11,cross_encoder,True,C,C,1.55,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture')]"
12,baseline,False,A,C,0.48,A,❌ FALLO TOTAL,0.0,[]
12,bm25,True,C,C,0.78,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,')]"
12,dense,True,C,C,1.24,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight')]"
12,hybrid,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,')]"
12,cross_encoder,True,C,C,2.26,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.')]"
13,baseline,False,A,C,0.83,A,❌ FALLO TOTAL,0.0,[]
13,bm25,True,C,C,1.24,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.011560693641618497,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='as input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMAK:Similar to theLLaMA-Full Context, we pass lastK tokens xsK:s in addition toxs+1:s+o to\ncompute perplexity inxs+1:s+o. The performance ofLLaMAK falls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant')]"
13,dense,True,C,C,1.42,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.011560693641618497,"[Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 20, 'source': './data/paper_refrag.pdf'}, page_content='m .\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput= bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\n\x00s\nk +o\n\x01\nTTFT (24d2+4ds)lbs\nf\n(24d2+4d s\nk )lb s\nk\nf\nTTIT 2n+4dlb(s+o)\nm\n2n+4dlb( s\nk +o)\nm\nThroughput bo\nTTFTbefore+TTITbefore\nbo\nTTFTafter+TTITafter\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3')]"
13,hybrid,True,C,C,0.95,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.03468208092485549,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 20, 'source': './data/paper_refrag.pdf'}, page_content='m .\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput= bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\n\x00s\nk +o\n\x01\nTTFT (24d2+4ds)lbs\nf\n(24d2+4d s\nk )lb s\nk\nf\nTTIT 2n+4dlb(s+o)\nm\n2n+4dlb( s\nk +o)\nm\nThroughput bo\nTTFTbefore+TTITbefore\nbo\nTTFTafter+TTITafter\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='as input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMAK:Similar to theLLaMA-Full Context, we pass lastK tokens xsK:s in addition toxs+1:s+o to\ncompute perplexity inxs+1:s+o. The performance ofLLaMAK falls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3')]"
13,cross_encoder,True,C,C,1.96,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005780346820809248,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
14,baseline,True,C,C,0.72,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
14,bm25,True,C,C,1.14,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0026455026455026454,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight')]"
14,dense,True,C,C,0.72,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0026455026455026454,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then')]"
14,hybrid,True,C,C,0.73,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.010582010582010581,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then')]"
14,cross_encoder,True,C,C,2.36,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005291005291005291,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices')]"
15,baseline,True,C,C,0.72,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
15,bm25,True,C,C,0.73,C,✅ ACIERTO PERFECTO (RAG),0.5864406779661017,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 27}, page_content='Ground True Abstract Generated Abstract\nbackground : timely access to cardiovascular health services is\nnecessary to prevent heart damages . the present study examined\ninequality in geographical distribution of cardiovascular health\nservices in iran . methods : present study is a cross - sectional study\nconducted using demographic data from all iranian provinces ( 31\nprovinces ) from 2012 census by the statistics center of iran ( sci ) .\nthe gini coefficients of ccu beds and cardiologists were used to assess\nequality in access to cardiovascular health services in iran . ms\nexcel software was used to calculate gini coefficients . results : the\nproportions of ccu bed and cardiologist per 100,000 population were\n4.88 and 1.27 , respectively ; also the gini coefficients were 0.129\nand 0.045 , respectively . conclusion : descriptive statistics showed\na skewness in distribution of pubic cardiovascular health services\nin iran , though gini coefficient revealed no significant inequality\n. however , equal distribution of ccu beds and cardiovascular\nspecialists does not mean they are sufficiently available in iran .\nbackground : this study aimed to investigate the inequality of'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 27, 'source': './data/paper_refrag.pdf'}, page_content='a skewness in distribution of pubic cardiovascular health services\nin iran , though gini coefficient revealed no significant inequality\n. however , equal distribution of ccu beds and cardiovascular\nspecialists does not mean they are sufficiently available in iran .\nbackground : this study aimed to investigate the inequality of\ndistribution of cardiac care units ( ccu ) and cardiologists in iran\n. methods : this study used demographic data from national\nstatistics collected by the central statistics of iran ( sci ) in 2012 .\nthe number of ccu beds and cardiologists per 100,000 individuals\nand the number of cardiologists per 10 ccu beds were explored .\nthe gini coefficient was applied to measure inequality . results :\nthe mean number of ccu beds per 100,000 individuals in iran was\n4.88 , which is out of a total of 31 provinces below the national\nmean . the mean number of cardiologists per 100,000 individuals\nwas 1.27 and the mean number of cardiologists per 10 ccu beds was\n2.6 . the gini coefficient of cardiologists in 2012 was 0.045 and the\ngini coefficient of ccu beds was 0.129 . conclusion : our descriptive\nstatistics showed that there is a skewness in the distribution of')]"
15,dense,True,C,C,1.02,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.06440677966101695,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.46\n1.48\n1.50\nLoss\nLlama-2-7B\nLlama-2-13B\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.46\n1.47\n1.48\n1.49\n1.50\n1.51\nLoss\nRoberta-Base\nRoberta-Large\nFigure 11Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder\nthe Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model.\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.45\n1.46\n1.47\n1.48\n1.49\n1.50\nLoss\nRoberta-Base\nRoberta-Large\nFigure 12Training trajectory for different encoder paired with LLaMA-2-13B decoder.\noutperforms others. Table 15 shows the performance ofREFRAGunder different number of context for\nstrong retriever setting.\nDemonstration of generated summary for Arxiv and Pubmed articles.Table 20 and table 19 shows the ground\ntrue abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23')]"
15,hybrid,True,C,C,0.85,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.06440677966101695,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 27}, page_content='Ground True Abstract Generated Abstract\nbackground : timely access to cardiovascular health services is\nnecessary to prevent heart damages . the present study examined\ninequality in geographical distribution of cardiovascular health\nservices in iran . methods : present study is a cross - sectional study\nconducted using demographic data from all iranian provinces ( 31\nprovinces ) from 2012 census by the statistics center of iran ( sci ) .\nthe gini coefficients of ccu beds and cardiologists were used to assess\nequality in access to cardiovascular health services in iran . ms\nexcel software was used to calculate gini coefficients . results : the\nproportions of ccu bed and cardiologist per 100,000 population were\n4.88 and 1.27 , respectively ; also the gini coefficients were 0.129\nand 0.045 , respectively . conclusion : descriptive statistics showed\na skewness in distribution of pubic cardiovascular health services\nin iran , though gini coefficient revealed no significant inequality\n. however , equal distribution of ccu beds and cardiovascular\nspecialists does not mean they are sufficiently available in iran .\nbackground : this study aimed to investigate the inequality of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.46\n1.48\n1.50\nLoss\nLlama-2-7B\nLlama-2-13B\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.46\n1.47\n1.48\n1.49\n1.50\n1.51\nLoss\nRoberta-Base\nRoberta-Large\nFigure 11Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder\nthe Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model.\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.45\n1.46\n1.47\n1.48\n1.49\n1.50\nLoss\nRoberta-Base\nRoberta-Large\nFigure 12Training trajectory for different encoder paired with LLaMA-2-13B decoder.\noutperforms others. Table 15 shows the performance ofREFRAGunder different number of context for\nstrong retriever setting.\nDemonstration of generated summary for Arxiv and Pubmed articles.Table 20 and table 19 shows the ground\ntrue abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task'), Document(metadata={'page': 27, 'source': './data/paper_refrag.pdf'}, page_content='a skewness in distribution of pubic cardiovascular health services\nin iran , though gini coefficient revealed no significant inequality\n. however , equal distribution of ccu beds and cardiovascular\nspecialists does not mean they are sufficiently available in iran .\nbackground : this study aimed to investigate the inequality of\ndistribution of cardiac care units ( ccu ) and cardiologists in iran\n. methods : this study used demographic data from national\nstatistics collected by the central statistics of iran ( sci ) in 2012 .\nthe number of ccu beds and cardiologists per 100,000 individuals\nand the number of cardiologists per 10 ccu beds were explored .\nthe gini coefficient was applied to measure inequality . results :\nthe mean number of ccu beds per 100,000 individuals in iran was\n4.88 , which is out of a total of 31 provinces below the national\nmean . the mean number of cardiologists per 100,000 individuals\nwas 1.27 and the mean number of cardiologists per 10 ccu beds was\n2.6 . the gini coefficient of cardiologists in 2012 was 0.045 and the\ngini coefficient of ccu beds was 0.129 . conclusion : our descriptive\nstatistics showed that there is a skewness in the distribution of'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23')]"
15,cross_encoder,True,C,C,2.43,C,✅ ACIERTO PERFECTO (RAG),0.5864406779661017,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the')]"
16,baseline,True,C,C,0.63,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
16,bm25,True,C,C,0.72,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.002506265664160401,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 28, 'source': './data/paper_refrag.pdf'}, page_content='show that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .\nmachine learning methods are applied to finding the green s func-\ntion of the anderson impurity model , a basic model system of\nquantum many - body condensed - matter physics . different\nmethods of parametrizing the green s function are investigated ;\na representation in terms of legendre polynomials is found to be\nsuperior due to its limited number of coefficients and its applica-\nbility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture')]"
16,dense,True,C,C,1.02,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005012531328320802,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='BoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric\nsequence for each type of data point, based on the intuition that training should begin with a greater\nproportion of easier examples and gradually introduce more challenging ones as training progresses. The\nright-most column indicates the total number of data points for each type. We allocate more data points to\nlonger-context examples to encourage the model to focus on learning more difficult tasks.\nB.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model\nIn this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model.\nWe denote the following parameters:s as the context length,o as the output length,b as the batch size,d as\n19'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'page': 19, 'source': './data/paper_refrag.pdf'}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.')]"
16,hybrid,True,C,C,0.93,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.002506265664160401,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 28, 'source': './data/paper_refrag.pdf'}, page_content='show that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .\nmachine learning methods are applied to finding the green s func-\ntion of the anderson impurity model , a basic model system of\nquantum many - body condensed - matter physics . different\nmethods of parametrizing the green s function are investigated ;\na representation in terms of legendre polynomials is found to be\nsuperior due to its limited number of coefficients and its applica-\nbility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='BoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric\nsequence for each type of data point, based on the intuition that training should begin with a greater\nproportion of easier examples and gradually introduce more challenging ones as training progresses. The\nright-most column indicates the total number of data points for each type. We allocate more data points to\nlonger-context examples to encourage the model to focus on learning more difficult tasks.\nB.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model\nIn this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model.\nWe denote the following parameters:s as the context length,o as the output length,b as the batch size,d as\n19'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 19}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.')]"
16,cross_encoder,True,C,C,2.31,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.002506265664160401,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture')]"
17,baseline,False,D,C,0.67,D,❌ FALLO TOTAL,0.0,[]
17,bm25,True,C,C,0.63,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0036101083032490976,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18')]"
17,dense,True,C,C,0.82,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.47653429602888087,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated')]"
17,hybrid,True,C,C,0.77,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.010830324909747292,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated')]"
17,cross_encoder,True,C,C,2.26,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.47653429602888087,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial')]"
18,baseline,False,B,C,0.47,B,❌ FALLO TOTAL,0.0,[]
18,bm25,True,C,C,0.73,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can')]"
18,dense,True,C,C,0.63,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='ensure fair comparison with prior work (Yen et al., 2024; Shi et al., 2024). Each data point containsT = 4096\ntokens, split intos = 2048context ando = 2048output tokens. We evaluate perplexity onxs+1:s+o. Below, we\nbriefly describe the main baselines; further details are provided in section B.LLaMA-No Context: LLaMA-2-7B\nevaluated onxs+1:s+o with only output tokens as input.LLaMA-Full Context: LLaMA-2-7B evaluated on\nxs+1:s+o with the full sequencex1:T as input.CEPE: Memory-efficient long-context model (Yen et al., 2024)\na previous SOTA model which share some similarity toREFRAG CEPEDdenotes its instruction-tuned\nvariant.LLaMA-32K: LLaMA-2-7B fine-tuned for 32K context length.REPLUG: Retrieval-augmented LLaMA-\n2-7B (Shi et al., 2024).REFRAG: Our approach (see Figure 1);REFRAGk denotes compression ratek,\nREFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
18,hybrid,True,C,C,1.43,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='ensure fair comparison with prior work (Yen et al., 2024; Shi et al., 2024). Each data point containsT = 4096\ntokens, split intos = 2048context ando = 2048output tokens. We evaluate perplexity onxs+1:s+o. Below, we\nbriefly describe the main baselines; further details are provided in section B.LLaMA-No Context: LLaMA-2-7B\nevaluated onxs+1:s+o with only output tokens as input.LLaMA-Full Context: LLaMA-2-7B evaluated on\nxs+1:s+o with the full sequencex1:T as input.CEPE: Memory-efficient long-context model (Yen et al., 2024)\na previous SOTA model which share some similarity toREFRAG CEPEDdenotes its instruction-tuned\nvariant.LLaMA-32K: LLaMA-2-7B fine-tuned for 32K context length.REPLUG: Retrieval-augmented LLaMA-\n2-7B (Shi et al., 2024).REFRAG: Our approach (see Figure 1);REFRAGk denotes compression ratek,\nREFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the\ntruncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
18,cross_encoder,True,C,C,2.16,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can')]"
19,baseline,False,B,C,0.83,B,❌ FALLO TOTAL,0.0,[]
19,bm25,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8')]"
19,dense,True,C,C,0.93,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 26}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 26, 'source': './data/paper_refrag.pdf'}, page_content='REPLUG+80 passages - - - 76.16 - 65.46 - 55.33\nLLaMA-32K+80 passages 22.01 18.04 19.97 16.56 23.69 23.80 33.19 48.62\nREFRAG8 +80 passages50.03 90.72 99.66 97.35 44.44 67.66 69.48 56.911×\nREFRAG16+80 passages49.77 90.21 99.66 95.3638.2968.12 70.57 56.912×\nREFRAG32+80 passages50.03 91.24 99.50 98.01 43.02 68.58 68.55 57.224×\n- means the corresponding model has out-of-memory error.\nTable 18Performance of our model under compression rate of 16 with different number of retrieved passages in RAG\nunder the weak retriever scenario.\n# Passages MMLU NQ FEVER WebQA FreebaseQA CommonsenseQA ECQA StrategyQA HellaSwag SIQA PIQA↑\n0 48.14 19.09 61.40 30.67 59.71 85.05 86.75 55.56 36.57 64.59 68.82\n1 49.97 20.08 64.15 38.67 64.62 87.63 92.72 71.11 39.08 68.58 70.57\n3 49.64 20.63 60.80 40.00 68.55 89.69 95.36 75.56 39.41 69.40 71.11\n5 49.84 20.60 60.45 40.00 66.09 90.21 96.69 73.33 39.52 68.63 70.95\n8 49.77 20.73 60.86 40.00 66.83 90.21 96.69 77.78 39.32 68.73 70.46\n20 50.03 21.29 62.32 36.00 68.06 89.69 95.36 75.56 38.58 69.29 70.62\n50 49.84 22.12 63.54 37.33 71.99 89.69 96.69 75.56 38.11 68.53 70.84\n80 49.77 22.63 65.07 38.67 71.74 90.21 95.36 68.89 38.29 68.12 70.57')]"
19,hybrid,True,C,C,1.14,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 26, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 26}, page_content='REPLUG+80 passages - - - 76.16 - 65.46 - 55.33\nLLaMA-32K+80 passages 22.01 18.04 19.97 16.56 23.69 23.80 33.19 48.62\nREFRAG8 +80 passages50.03 90.72 99.66 97.35 44.44 67.66 69.48 56.911×\nREFRAG16+80 passages49.77 90.21 99.66 95.3638.2968.12 70.57 56.912×\nREFRAG32+80 passages50.03 91.24 99.50 98.01 43.02 68.58 68.55 57.224×\n- means the corresponding model has out-of-memory error.\nTable 18Performance of our model under compression rate of 16 with different number of retrieved passages in RAG\nunder the weak retriever scenario.\n# Passages MMLU NQ FEVER WebQA FreebaseQA CommonsenseQA ECQA StrategyQA HellaSwag SIQA PIQA↑\n0 48.14 19.09 61.40 30.67 59.71 85.05 86.75 55.56 36.57 64.59 68.82\n1 49.97 20.08 64.15 38.67 64.62 87.63 92.72 71.11 39.08 68.58 70.57\n3 49.64 20.63 60.80 40.00 68.55 89.69 95.36 75.56 39.41 69.40 71.11\n5 49.84 20.60 60.45 40.00 66.09 90.21 96.69 73.33 39.52 68.63 70.95\n8 49.77 20.73 60.86 40.00 66.83 90.21 96.69 77.78 39.32 68.73 70.46\n20 50.03 21.29 62.32 36.00 68.06 89.69 95.36 75.56 38.58 69.29 70.62\n50 49.84 22.12 63.54 37.33 71.99 89.69 96.69 75.56 38.11 68.53 70.84\n80 49.77 22.63 65.07 38.67 71.74 90.21 95.36 68.89 38.29 68.12 70.57')]"
19,cross_encoder,True,C,C,2.13,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×')]"
20,baseline,True,C,C,0.73,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
20,bm25,True,C,C,1.03,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.01606425702811245,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5')]"
20,dense,True,C,C,0.73,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.020080321285140562,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='as input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMAK:Similar to theLLaMA-Full Context, we pass lastK tokens xsK:s in addition toxs+1:s+o to\ncompute perplexity inxs+1:s+o. The performance ofLLaMAK falls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low')]"
20,hybrid,True,C,C,0.62,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.01606425702811245,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='as input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMAK:Similar to theLLaMA-Full Context, we pass lastK tokens xsK:s in addition toxs+1:s+o to\ncompute perplexity inxs+1:s+o. The performance ofLLaMAK falls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 10, 'source': './data/paper_refrag.pdf'}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low')]"
20,cross_encoder,True,C,C,1.75,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.01606425702811245,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial')]"
21,baseline,True,C,C,0.84,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
21,bm25,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='is the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to\nmachine learning . however , its use in nlp tasks has been limited\nby its over - generalisation properties . this paper investigates two\noptimisations that can be applied to memory - based learning in\norder to improve its generalisation performance : ( 1 ) replacing\ninstance types ( memory tokens ) by instance types with frequency\ninformation , and ( 2 ) removing redundant information ( i.e. ,\ninstance types with low predictive power ) . we perform exper-\niments on a large data set of english word pronunciations . we\nshow that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The')]"
21,dense,True,C,C,0.71,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
21,hybrid,True,C,C,0.73,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='is the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to\nmachine learning . however , its use in nlp tasks has been limited\nby its over - generalisation properties . this paper investigates two\noptimisations that can be applied to memory - based learning in\norder to improve its generalisation performance : ( 1 ) replacing\ninstance types ( memory tokens ) by instance types with frequency\ninformation , and ( 2 ) removing redundant information ( i.e. ,\ninstance types with low predictive power ) . we perform exper-\niments on a large data set of english word pronunciations . we\nshow that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
21,cross_encoder,True,C,C,1.71,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting')]"
22,baseline,False,A,C,0.66,A,❌ FALLO TOTAL,0.0,[]
22,bm25,True,C,C,0.94,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.03468208092485549,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3')]"
22,dense,True,C,C,1.03,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005780346820809248,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 20, 'source': './data/paper_refrag.pdf'}, page_content='m .\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput= bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\n\x00s\nk +o\n\x01\nTTFT (24d2+4ds)lbs\nf\n(24d2+4d s\nk )lb s\nk\nf\nTTIT 2n+4dlb(s+o)\nm\n2n+4dlb( s\nk +o)\nm\nThroughput bo\nTTFTbefore+TTITbefore\nbo\nTTFTafter+TTITafter\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results')]"
22,hybrid,True,C,C,0.85,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005780346820809248,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 20}, page_content='m .\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput= bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\n\x00s\nk +o\n\x01\nTTFT (24d2+4ds)lbs\nf\n(24d2+4d s\nk )lb s\nk\nf\nTTIT 2n+4dlb(s+o)\nm\n2n+4dlb( s\nk +o)\nm\nThroughput bo\nTTFTbefore+TTITbefore\nbo\nTTFTafter+TTITafter\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results')]"
22,cross_encoder,True,C,C,1.83,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.011560693641618497,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss')]"
23,baseline,True,C,C,0.4,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
23,bm25,True,C,C,1.04,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='. the approach of particle swarms is an example for interior point\nmethods in optimization as an iterative technique . this approach\nis novel and deals with classification problems without the use\nof a traditional classifier . our method determines the optimal\nhyperplane or classification boundary for a data set . in a bi-\nnary classification problem , we constrain each class as a cluster ,\nwhich is enclosed by an ellipsoid . the estimation of the optimal\nhyperplane between the two clusters is posed as a quadratically\nconstrained quadratic problem . the optimization problem is solved\nin distributed format using modified particle swarms . our method\nhas the advantage of using the direction towards optimal solution\nrather than searching the entire feasible region . our results on the\niris , pima , wine , and thyroid datasets show that the proposed\nmethod works better than a neural network and the performance\nis close to that of svm . * keywords * quadratic programming\n; particle swarms ; hyperplane ; quadratic constraints ; binary\nclassification .\nsupport vector machines are used for classification of data in\nmachine learning . support vector machines use quadratic pro-')]"
23,dense,True,C,C,0.62,C,✅ ACIERTO PERFECTO (RAG),0.659846547314578,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'page': 16, 'source': './data/paper_refrag.pdf'}, page_content='Motivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices\nl1, . . . , lT′, wherel t ∈[L]. At staget, the policy samples from:\nπθ(lt =i|x,{l j}t−1\nj=1) :=π θ(lt =i|{c j}L\nj=1,{l j}t−1\nj=1) = exp(si −ni)PL\nj=1 exp(sj −nj)\n.\nwhere nj = ∞ iff j∈ {li}t−1\ni=1 and0otherwise 4;s= gθ({ci}i∈[L],i/∈{lj}t−1\nj=1\n)is the output of a two-layer\ntransformer network over chunk embeddings, producing logitsi for each chunk. In practice, we reuse chunk\nembeddings {ci}L\ni=1 as transformer input and do not recompute logitssi after each selection, as state changes\nhave minimal impact and this improves training speed.\nWe use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to\nminimize contamination across different segment prediction task. Specifically, for eachx we randomly select\nGnumber of lengthT ′ action sequences{l (i)}G\ni=1 . We have the following objective:\nJθ = 1\nG\nPG\ni=1 E x∼P(X),\n{l(i)}G\ni=1∼πθ([L]|x)\n1\nT′\nPT′\nt=1 min\n\x14\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.')]"
23,hybrid,True,C,C,1.04,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='Motivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices\nl1, . . . , lT′, wherel t ∈[L]. At staget, the policy samples from:\nπθ(lt =i|x,{l j}t−1\nj=1) :=π θ(lt =i|{c j}L\nj=1,{l j}t−1\nj=1) = exp(si −ni)PL\nj=1 exp(sj −nj)\n.\nwhere nj = ∞ iff j∈ {li}t−1\ni=1 and0otherwise 4;s= gθ({ci}i∈[L],i/∈{lj}t−1\nj=1\n)is the output of a two-layer\ntransformer network over chunk embeddings, producing logitsi for each chunk. In practice, we reuse chunk\nembeddings {ci}L\ni=1 as transformer input and do not recompute logitssi after each selection, as state changes\nhave minimal impact and this improves training speed.\nWe use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to\nminimize contamination across different segment prediction task. Specifically, for eachx we randomly select\nGnumber of lengthT ′ action sequences{l (i)}G\ni=1 . We have the following objective:\nJθ = 1\nG\nPG\ni=1 E x∼P(X),\n{l(i)}G\ni=1∼πθ([L]|x)\n1\nT′\nPT′\nt=1 min\n\x14\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='. the approach of particle swarms is an example for interior point\nmethods in optimization as an iterative technique . this approach\nis novel and deals with classification problems without the use\nof a traditional classifier . our method determines the optimal\nhyperplane or classification boundary for a data set . in a bi-\nnary classification problem , we constrain each class as a cluster ,\nwhich is enclosed by an ellipsoid . the estimation of the optimal\nhyperplane between the two clusters is posed as a quadratically\nconstrained quadratic problem . the optimization problem is solved\nin distributed format using modified particle swarms . our method\nhas the advantage of using the direction towards optimal solution\nrather than searching the entire feasible region . our results on the\niris , pima , wine , and thyroid datasets show that the proposed\nmethod works better than a neural network and the performance\nis close to that of svm . * keywords * quadratic programming\n; particle swarms ; hyperplane ; quadratic constraints ; binary\nclassification .\nsupport vector machines are used for classification of data in\nmachine learning . support vector machines use quadratic pro-'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.')]"
23,cross_encoder,True,C,C,2.16,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight')]"
24,baseline,True,C,C,0.84,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
24,bm25,True,C,C,0.72,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.46\n1.48\n1.50\nLoss\nLlama-2-7B\nLlama-2-13B\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.46\n1.47\n1.48\n1.49\n1.50\n1.51\nLoss\nRoberta-Base\nRoberta-Large\nFigure 11Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder\nthe Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model.\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.45\n1.46\n1.47\n1.48\n1.49\n1.50\nLoss\nRoberta-Base\nRoberta-Large\nFigure 12Training trajectory for different encoder paired with LLaMA-2-13B decoder.\noutperforms others. Table 15 shows the performance ofREFRAGunder different number of context for\nstrong retriever setting.\nDemonstration of generated summary for Arxiv and Pubmed articles.Table 20 and table 19 shows the ground\ntrue abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial')]"
24,dense,True,C,C,0.78,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.46\n1.48\n1.50\nLoss\nLlama-2-7B\nLlama-2-13B\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.46\n1.47\n1.48\n1.49\n1.50\n1.51\nLoss\nRoberta-Base\nRoberta-Large\nFigure 11Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder\nthe Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model.\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.45\n1.46\n1.47\n1.48\n1.49\n1.50\nLoss\nRoberta-Base\nRoberta-Large\nFigure 12Training trajectory for different encoder paired with LLaMA-2-13B decoder.\noutperforms others. Table 15 shows the performance ofREFRAGunder different number of context for\nstrong retriever setting.\nDemonstration of generated summary for Arxiv and Pubmed articles.Table 20 and table 19 shows the ground\ntrue abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
24,hybrid,True,C,C,0.87,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.46\n1.48\n1.50\nLoss\nLlama-2-7B\nLlama-2-13B\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.46\n1.47\n1.48\n1.49\n1.50\n1.51\nLoss\nRoberta-Base\nRoberta-Large\nFigure 11Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder\nthe Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model.\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.45\n1.46\n1.47\n1.48\n1.49\n1.50\nLoss\nRoberta-Base\nRoberta-Large\nFigure 12Training trajectory for different encoder paired with LLaMA-2-13B decoder.\noutperforms others. Table 15 shows the performance ofREFRAGunder different number of context for\nstrong retriever setting.\nDemonstration of generated summary for Arxiv and Pubmed articles.Table 20 and table 19 shows the ground\ntrue abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
24,cross_encoder,True,C,C,1.85,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the')]"
25,baseline,False,B,C,0.93,B,❌ FALLO TOTAL,0.0,[]
25,bm25,True,C,C,1.25,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='BoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric\nsequence for each type of data point, based on the intuition that training should begin with a greater\nproportion of easier examples and gradually introduce more challenging ones as training progresses. The\nright-most column indicates the total number of data points for each type. We allocate more data points to\nlonger-context examples to encourage the model to focus on learning more difficult tasks.\nB.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model\nIn this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model.\nWe denote the following parameters:s as the context length,o as the output length,b as the batch size,d as\n19'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the')]"
25,dense,True,C,C,0.84,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.006097560975609756,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16')]"
25,hybrid,True,C,C,0.82,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='BoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric\nsequence for each type of data point, based on the intuition that training should begin with a greater\nproportion of easier examples and gradually introduce more challenging ones as training progresses. The\nright-most column indicates the total number of data points for each type. We allocate more data points to\nlonger-context examples to encourage the model to focus on learning more difficult tasks.\nB.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model\nIn this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model.\nWe denote the following parameters:s as the context length,o as the output length,b as the batch size,d as\n19'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
25,cross_encoder,True,C,C,1.9,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture')]"
26,baseline,False,B,C,0.74,B,❌ FALLO TOTAL,0.0,[]
26,bm25,True,C,C,0.93,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0823045267489712,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated')]"
26,dense,True,C,C,0.93,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0823045267489712,"[Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 20, 'source': './data/paper_refrag.pdf'}, page_content='m .\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput= bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\n\x00s\nk +o\n\x01\nTTFT (24d2+4ds)lbs\nf\n(24d2+4d s\nk )lb s\nk\nf\nTTIT 2n+4dlb(s+o)\nm\n2n+4dlb( s\nk +o)\nm\nThroughput bo\nTTFTbefore+TTITbefore\nbo\nTTFTafter+TTITafter\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The')]"
26,hybrid,True,C,C,1.03,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0823045267489712,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 20}, page_content='m .\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput= bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\n\x00s\nk +o\n\x01\nTTFT (24d2+4ds)lbs\nf\n(24d2+4d s\nk )lb s\nk\nf\nTTIT 2n+4dlb(s+o)\nm\n2n+4dlb( s\nk +o)\nm\nThroughput bo\nTTFTbefore+TTITbefore\nbo\nTTFTafter+TTITafter\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and')]"
26,cross_encoder,True,C,C,1.61,C,✅ ACIERTO PERFECTO (RAG),0.8765432098765432,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The')]"
27,baseline,True,C,C,0.55,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
27,bm25,True,C,C,0.93,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 27, 'source': './data/paper_refrag.pdf'}, page_content='mean . the mean number of cardiologists per 100,000 individuals\nwas 1.27 and the mean number of cardiologists per 10 ccu beds was\n2.6 . the gini coefficient of cardiologists in 2012 was 0.045 and the\ngini coefficient of ccu beds was 0.129 . conclusion : our descriptive\nstatistics showed that there is a skewness in the distribution of\npubic cardiovascular health services in iran . moreover , the equal\ndistribution of cardiovascular health facilities such as ccu beds is\nnot necessarily provided in iran .\nlumbar spinal stenosis is a commonly treated with epidural in-\njections of local anesthetics and corticosteroids , however , these\ntherapies may relieve leg pain for weeks to months but do not\ninfluence functional status . furthermore , the majority of pa-\ntients report no substantial symptom change over the repeated\ntreatment . utilizing balloon catheters , we successfully treated\nwith three patients who complained persistent symptoms despite\nrepeated conventional steroid injections . our results suggest that\ntransforaminal decompression using a balloon catheter may have\npotential in the nonsurgical treatment of spinal stenosis by modi-\nfying the underlying pathophysiology .'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
27,dense,True,C,C,0.74,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 26}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 20, 'source': './data/paper_refrag.pdf'}, page_content='hyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results\nSparse attention across different retrieved passages.We retrieve 200 passages using the query “how bruce\nlee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to\nsimulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it\nto LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the\nattention values for tokens within each passages are significantly larger than attention values for tokens in\ndifferent passages which suggests redundancy in the current attention computation for RAG applications.\n21')]"
27,hybrid,True,C,C,0.71,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'page': 26, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'page': 27, 'source': './data/paper_refrag.pdf'}, page_content='mean . the mean number of cardiologists per 100,000 individuals\nwas 1.27 and the mean number of cardiologists per 10 ccu beds was\n2.6 . the gini coefficient of cardiologists in 2012 was 0.045 and the\ngini coefficient of ccu beds was 0.129 . conclusion : our descriptive\nstatistics showed that there is a skewness in the distribution of\npubic cardiovascular health services in iran . moreover , the equal\ndistribution of cardiovascular health facilities such as ccu beds is\nnot necessarily provided in iran .\nlumbar spinal stenosis is a commonly treated with epidural in-\njections of local anesthetics and corticosteroids , however , these\ntherapies may relieve leg pain for weeks to months but do not\ninfluence functional status . furthermore , the majority of pa-\ntients report no substantial symptom change over the repeated\ntreatment . utilizing balloon catheters , we successfully treated\nwith three patients who complained persistent symptoms despite\nrepeated conventional steroid injections . our results suggest that\ntransforaminal decompression using a balloon catheter may have\npotential in the nonsurgical treatment of spinal stenosis by modi-\nfying the underlying pathophysiology .'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 20, 'source': './data/paper_refrag.pdf'}, page_content='hyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results\nSparse attention across different retrieved passages.We retrieve 200 passages using the query “how bruce\nlee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to\nsimulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it\nto LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the\nattention values for tokens within each passages are significantly larger than attention values for tokens in\ndifferent passages which suggests redundancy in the current attention computation for RAG applications.\n21')]"
27,cross_encoder,True,C,C,2.16,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG')]"
28,baseline,True,C,C,0.62,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
28,bm25,True,C,C,1.14,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 19, 'source': './data/paper_refrag.pdf'}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.')]"
28,dense,True,C,C,1.04,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved')]"
28,hybrid,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 19, 'source': './data/paper_refrag.pdf'}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved')]"
28,cross_encoder,True,C,C,1.66,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential')]"
29,baseline,True,C,C,0.73,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
29,bm25,True,C,C,0.65,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18')]"
29,dense,True,C,C,1.01,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
29,hybrid,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
29,cross_encoder,True,C,C,1.53,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23')]"
30,baseline,True,C,C,0.73,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
30,bm25,True,C,C,0.93,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.')]"
30,dense,True,C,C,0.83,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.12582781456953643,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2')]"
30,hybrid,True,C,C,1.54,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2')]"
30,cross_encoder,True,C,C,2.17,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across')]"
31,baseline,False,B,C,0.41,B,❌ FALLO TOTAL,0.0,[]
31,bm25,True,C,C,1.04,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 20}, page_content='hyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results\nSparse attention across different retrieved passages.We retrieve 200 passages using the query “how bruce\nlee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to\nsimulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it\nto LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the\nattention values for tokens within each passages are significantly larger than attention values for tokens in\ndifferent passages which suggests redundancy in the current attention computation for RAG applications.\n21')]"
31,dense,True,C,C,1.14,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
31,hybrid,True,C,C,0.97,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 20}, page_content='hyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results\nSparse attention across different retrieved passages.We retrieve 200 passages using the query “how bruce\nlee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to\nsimulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it\nto LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the\nattention values for tokens within each passages are significantly larger than attention values for tokens in\ndifferent passages which suggests redundancy in the current attention computation for RAG applications.\n21'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
31,cross_encoder,True,C,C,1.72,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of')]"
32,baseline,False,B,C,0.62,B,❌ FALLO TOTAL,0.0,[]
32,bm25,True,C,C,0.63,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.03468208092485549,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='indicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we\nshow that machine learning is an efficient and accurate method\nto compute the self - energy of the model and to predict the spec-\ntral function of the model . we also show that machine learning\nalgorithms can be used to efficiently compute the self - consistent\ngreen s function starting from any hybridization function .\nparticle swarm optimization is used in several combinatorial op-\ntimization problems . in this work , particle swarms are used to\nsolve quadratic programming problems with quadratic constraints\n. the approach of particle swarms is an example for interior point\nmethods in optimization as an iterative technique . this approach\nis novel and deals with classification problems without the use\nof a traditional classifier . our method determines the optimal\nhyperplane or classification boundary for a data set . in a bi-')]"
32,dense,True,C,C,1.04,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005780346820809248,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
32,hybrid,True,C,C,1.04,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005780346820809248,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='indicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we\nshow that machine learning is an efficient and accurate method\nto compute the self - energy of the model and to predict the spec-\ntral function of the model . we also show that machine learning\nalgorithms can be used to efficiently compute the self - consistent\ngreen s function starting from any hybridization function .\nparticle swarm optimization is used in several combinatorial op-\ntimization problems . in this work , particle swarms are used to\nsolve quadratic programming problems with quadratic constraints\n. the approach of particle swarms is an example for interior point\nmethods in optimization as an iterative technique . this approach\nis novel and deals with classification problems without the use\nof a traditional classifier . our method determines the optimal\nhyperplane or classification boundary for a data set . in a bi-'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
32,cross_encoder,True,C,C,1.64,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005780346820809248,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
33,baseline,False,B,C,0.61,B,❌ FALLO TOTAL,0.0,[]
33,bm25,True,C,C,1.03,C,✅ ACIERTO PERFECTO (RAG),0.5069767441860465,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
33,dense,True,C,C,0.73,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.09302325581395349,"[Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
33,hybrid,True,C,C,1.02,C,✅ ACIERTO PERFECTO (RAG),0.5069767441860465,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
33,cross_encoder,True,C,C,1.86,C,✅ ACIERTO PERFECTO (RAG),0.5069767441860465,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can')]"
34,baseline,True,C,C,0.52,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
34,bm25,True,C,C,0.72,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.019027484143763214,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
34,dense,True,C,C,0.92,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.010570824524312896,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting')]"
34,hybrid,True,C,C,0.92,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.010570824524312896,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG')]"
34,cross_encoder,True,C,C,2.67,C,✅ ACIERTO PERFECTO (RAG),0.8393234672304439,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting')]"
35,baseline,True,C,C,0.61,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
35,bm25,True,C,C,0.93,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.002044989775051125,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 19, 'source': './data/paper_refrag.pdf'}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.')]"
35,dense,True,C,C,0.93,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.002044989775051125,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 19}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 16, 'source': './data/paper_refrag.pdf'}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096')]"
35,hybrid,True,C,C,0.76,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.00408997955010225,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 19, 'source': './data/paper_refrag.pdf'}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096')]"
35,cross_encoder,True,C,C,2.32,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.081799591002045,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
36,baseline,True,C,C,0.65,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
36,bm25,True,C,C,0.94,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4')]"
36,dense,True,C,C,0.99,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.008021390374331552,"[Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
36,hybrid,True,C,C,1.05,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated')]"
36,cross_encoder,True,C,C,2.36,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.00267379679144385,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5')]"
37,baseline,False,B,C,0.62,B,❌ FALLO TOTAL,0.0,[]
37,bm25,True,C,C,0.88,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.41935483870967744,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18')]"
37,dense,True,C,C,1.27,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.41935483870967744,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion')]"
37,hybrid,True,C,C,0.94,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.41935483870967744,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18')]"
37,cross_encoder,True,C,C,2.36,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.41935483870967744,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5')]"
38,baseline,True,C,C,0.59,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
38,bm25,False,D,C,0.88,D,❌ FALLO TOTAL,0.009523809523809525,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.')]"
38,dense,False,D,C,0.91,D,❌ FALLO TOTAL,0.009523809523809525,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model')]"
38,hybrid,False,D,C,0.8,D,❌ FALLO TOTAL,0.009523809523809525,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model')]"
38,cross_encoder,False,D,C,2.37,D,❌ FALLO TOTAL,0.009523809523809525,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial')]"
39,baseline,False,D,C,0.41,D,❌ FALLO TOTAL,0.0,[]
39,bm25,True,C,C,0.68,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.043824701195219126,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2')]"
39,dense,True,C,C,0.84,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.00398406374501992,"[Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Roberta-Large 2048 1.107 1.065 1.025 1.170 1.130 1.091\nRoberta-Base 4096 1.067 1.032 0.999 1.142 1.105 1.070\nRoberta-Large 4096 1.065 1.031 0.998 1.130 1.096 1.064\nTable 15Performance of our model under compression rate of 16 with different number of retrieved passages in RAG\nunder the strong retriever scenario.\n# Passages MMLU NQ FEVER WebQA FreebaseQA CommonsenseQA ECQA StrategyQA HellaSwag SIQA PIQA↑\n0 48.07 18.73 65.80 34.67 60.20 89.18 87.42 68.89 43.72 67.25 70.18\n1 50.49 21.39 69.46 37.33 68.06 86.60 89.40 80.00 43.26 68.17 70.08\n3 50.49 22.01 66.02 38.67 71.01 89.18 95.36 71.11 45.50 68.73 71.44\n5 50.62 23.00 66.07 41.33 72.48 91.75 96.03 75.56 45.48 68.17 71.38\n8 50.29 22.96 66.59 38.67 73.46 92.27 94.70 75.56 45.23 68.94 71.38\n20 51.01 24.30 67.77 40.00 75.18 91.75 98.01 75.56 45.09 68.53 71.00\n50 51.08 24.76 69.39 40.00 75.92 91.75 97.35 75.56 44.78 67.81 69.97\n80 50.42 24.15 68.83 37.33 74.20 92.27 97.35 71.11 44.61 68.22 69.37\n100 50.23 23.99 69.80 36.00 74.45 92.27 97.35 71.11 44.57 68.07 69.75\n26'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat')]"
39,hybrid,True,C,C,0.95,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.043824701195219126,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Roberta-Large 2048 1.107 1.065 1.025 1.170 1.130 1.091\nRoberta-Base 4096 1.067 1.032 0.999 1.142 1.105 1.070\nRoberta-Large 4096 1.065 1.031 0.998 1.130 1.096 1.064\nTable 15Performance of our model under compression rate of 16 with different number of retrieved passages in RAG\nunder the strong retriever scenario.\n# Passages MMLU NQ FEVER WebQA FreebaseQA CommonsenseQA ECQA StrategyQA HellaSwag SIQA PIQA↑\n0 48.07 18.73 65.80 34.67 60.20 89.18 87.42 68.89 43.72 67.25 70.18\n1 50.49 21.39 69.46 37.33 68.06 86.60 89.40 80.00 43.26 68.17 70.08\n3 50.49 22.01 66.02 38.67 71.01 89.18 95.36 71.11 45.50 68.73 71.44\n5 50.62 23.00 66.07 41.33 72.48 91.75 96.03 75.56 45.48 68.17 71.38\n8 50.29 22.96 66.59 38.67 73.46 92.27 94.70 75.56 45.23 68.94 71.38\n20 51.01 24.30 67.77 40.00 75.18 91.75 98.01 75.56 45.09 68.53 71.00\n50 51.08 24.76 69.39 40.00 75.92 91.75 97.35 75.56 44.78 67.81 69.97\n80 50.42 24.15 68.83 37.33 74.20 92.27 97.35 71.11 44.61 68.22 69.37\n100 50.23 23.99 69.80 36.00 74.45 92.27 97.35 71.11 44.57 68.07 69.75\n26'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat')]"
39,cross_encoder,True,C,C,1.86,C,✅ ACIERTO PERFECTO (RAG),0.6613545816733067,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 20}, page_content='hyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results\nSparse attention across different retrieved passages.We retrieve 200 passages using the query “how bruce\nlee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to\nsimulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it\nto LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the\nattention values for tokens within each passages are significantly larger than attention values for tokens in\ndifferent passages which suggests redundancy in the current attention computation for RAG applications.\n21'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
40,baseline,True,C,C,0.43,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
40,bm25,True,C,C,0.94,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based')]"
40,dense,True,C,C,0.79,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 26, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT')]"
40,hybrid,True,C,C,0.74,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 26}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT')]"
40,cross_encoder,True,C,C,2.19,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 26}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×')]"
41,baseline,True,C,C,0.52,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
41,bm25,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT')]"
41,dense,True,C,C,0.65,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three')]"
41,hybrid,True,C,C,1.0,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three')]"
41,cross_encoder,True,C,C,1.74,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated')]"
42,baseline,True,B,B,0.32,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
42,bm25,True,B,B,1.03,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial')]"
42,dense,True,B,B,1.13,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.011235955056179775,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three')]"
42,hybrid,True,B,B,1.04,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three')]"
42,cross_encoder,True,B,B,2.27,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.')]"
43,baseline,True,C,C,0.67,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
43,bm25,True,C,C,0.68,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.003048780487804878,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved')]"
43,dense,True,C,C,1.0,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.006097560975609756,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
43,hybrid,True,C,C,0.76,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.003048780487804878,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
43,cross_encoder,True,C,C,2.43,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.003048780487804878,"[Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight')]"
44,baseline,False,B,C,0.62,B,❌ FALLO TOTAL,0.0,[]
44,bm25,False,B,C,0.58,B,❌ FALLO TOTAL,0.0032679738562091504,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='minimize contamination across different segment prediction task. Specifically, for eachx we randomly select\nGnumber of lengthT ′ action sequences{l (i)}G\ni=1 . We have the following objective:\nJθ = 1\nG\nPG\ni=1 E x∼P(X),\n{l(i)}G\ni=1∼πθ([L]|x)\n1\nT′\nPT′\nt=1 min\n\x14\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)\nπθold(l(i)\nt |x,{l(i)\nj }t−1\nj=1)A(i)\nt ,clip\n\x12\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)\nπθold(l(i)\nt |x,{l(i)\nj }t−1\nj=1),1−ϵ,1 +ϵ\n\x13\nA(i)\nt\n\x15\n(1)\nwhere ϵ is the clipping hyperparameter in PPO (Schulman et al., 2017) for stable training,θ is the current\npolicy andθold is the policy fro the previous iteration,At is the advantage function. We define our advantage\nfunction using the negative log-perplexity on theotokens xs+1:s+o:\nri =r\n\x10\nx,{l (i)\nj }T′\nj=1\n\x11\n=−M dec\n\x10\nxs+1:s+o|E(x,{l (i)\nj }T′\nj=1)\n\x11\n.\nWe compute the advantage function following GRPO as:\nA(i)\nt = ri −mean\n\x00\n{ri}G\ni=1\n\x01\nstd\n\x00\n{ri}G\ni=1\n\x01 .\nB Additional Details on Experimental Settings\nB.1 Additional Details on Baselines\nAll baseline models are based on the LLaMA-2-7B model (Touvron et al., 2023), unless otherwise specified, to\nensure a fair comparison since the previous methods are trained based on this model.5 We do provide results'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based')]"
44,dense,False,D,C,0.67,D,❌ FALLO TOTAL,0.0032679738562091504,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture')]"
44,hybrid,False,B,C,0.73,B,❌ FALLO TOTAL,0.0032679738562091504,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='minimize contamination across different segment prediction task. Specifically, for eachx we randomly select\nGnumber of lengthT ′ action sequences{l (i)}G\ni=1 . We have the following objective:\nJθ = 1\nG\nPG\ni=1 E x∼P(X),\n{l(i)}G\ni=1∼πθ([L]|x)\n1\nT′\nPT′\nt=1 min\n\x14\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)\nπθold(l(i)\nt |x,{l(i)\nj }t−1\nj=1)A(i)\nt ,clip\n\x12\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)\nπθold(l(i)\nt |x,{l(i)\nj }t−1\nj=1),1−ϵ,1 +ϵ\n\x13\nA(i)\nt\n\x15\n(1)\nwhere ϵ is the clipping hyperparameter in PPO (Schulman et al., 2017) for stable training,θ is the current\npolicy andθold is the policy fro the previous iteration,At is the advantage function. We define our advantage\nfunction using the negative log-perplexity on theotokens xs+1:s+o:\nri =r\n\x10\nx,{l (i)\nj }T′\nj=1\n\x11\n=−M dec\n\x10\nxs+1:s+o|E(x,{l (i)\nj }T′\nj=1)\n\x11\n.\nWe compute the advantage function following GRPO as:\nA(i)\nt = ri −mean\n\x00\n{ri}G\ni=1\n\x01\nstd\n\x00\n{ri}G\ni=1\n\x01 .\nB Additional Details on Experimental Settings\nB.1 Additional Details on Baselines\nAll baseline models are based on the LLaMA-2-7B model (Touvron et al., 2023), unless otherwise specified, to\nensure a fair comparison since the previous methods are trained based on this model.5 We do provide results'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture')]"
44,cross_encoder,False,B,C,2.26,B,❌ FALLO TOTAL,0.0032679738562091504,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight')]"
45,baseline,True,C,C,0.41,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
45,bm25,True,C,C,0.75,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant')]"
45,dense,True,C,C,0.93,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
45,hybrid,True,C,C,0.73,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks')]"
45,cross_encoder,True,C,C,2.15,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
46,baseline,True,C,C,0.53,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
46,bm25,True,C,C,0.63,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.018867924528301886,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 24}, page_content='the brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema,\naccording to a group of researchers, was brought on by hyponatraemia. In their study, which was\npublished in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because\nhis kidneys were unable to eliminate extra water. The findings are very different from old theories\nabout how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops\nwhen the body’s sodium levels get diluted as a result of consuming too much water. The cells in\nthe body, particularly those in the brain,\nP2 circumstances, you’re bound to get some truly insane conspiracy theories, and there are plenty\nabout Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big\nmistake after Bruce Lee’s death. Hoping to protect Lee’s image, Chow’s production company\nclaimed the actor died at home with his wife, Linda. But once the press found out the truth, the\ntabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='Ground True Abstract Generated Abstract\nmemory - based learning , keeping full memory of learning material\n, appears a viable approach to learning nlp tasks , and is often\nsuperior in generalisation accuracy to eager learning approaches\nthat abstract from learning material . here we investigate three\npartial memory - based learning approaches which remove from\nmemory specific task instance types estimated to be exceptional\n. the three approaches each implement one heuristic function for\nestimating exceptionality of instance types : ( i ) typicality , ( ii )\nclass prediction strength , and ( iii ) friendly - neighbourhood size\n. experiments are performed with the memory - based learning\nalgorithm ib1-ig trained on english word pronunciation . we find\nthat removing instance types with low prediction strength ( ii )\nis the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of')]"
46,dense,True,C,C,0.73,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.006289308176100629,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three')]"
46,hybrid,True,C,C,0.73,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.006289308176100629,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 24}, page_content='the brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema,\naccording to a group of researchers, was brought on by hyponatraemia. In their study, which was\npublished in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because\nhis kidneys were unable to eliminate extra water. The findings are very different from old theories\nabout how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops\nwhen the body’s sodium levels get diluted as a result of consuming too much water. The cells in\nthe body, particularly those in the brain,\nP2 circumstances, you’re bound to get some truly insane conspiracy theories, and there are plenty\nabout Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big\nmistake after Bruce Lee’s death. Hoping to protect Lee’s image, Chow’s production company\nclaimed the actor died at home with his wife, Linda. But once the press found out the truth, the\ntabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='Ground True Abstract Generated Abstract\nmemory - based learning , keeping full memory of learning material\n, appears a viable approach to learning nlp tasks , and is often\nsuperior in generalisation accuracy to eager learning approaches\nthat abstract from learning material . here we investigate three\npartial memory - based learning approaches which remove from\nmemory specific task instance types estimated to be exceptional\n. the three approaches each implement one heuristic function for\nestimating exceptionality of instance types : ( i ) typicality , ( ii )\nclass prediction strength , and ( iii ) friendly - neighbourhood size\n. experiments are performed with the memory - based learning\nalgorithm ib1-ig trained on english word pronunciation . we find\nthat removing instance types with low prediction strength ( ii )\nis the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three')]"
46,cross_encoder,True,C,C,2.16,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.012578616352201259,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated')]"
47,baseline,False,A,B,0.63,A,❌ FALLO TOTAL,0.0,[]
47,bm25,True,B,B,0.73,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 24}, page_content='the brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema,\naccording to a group of researchers, was brought on by hyponatraemia. In their study, which was\npublished in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because\nhis kidneys were unable to eliminate extra water. The findings are very different from old theories\nabout how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops\nwhen the body’s sodium levels get diluted as a result of consuming too much water. The cells in\nthe body, particularly those in the brain,\nP2 circumstances, you’re bound to get some truly insane conspiracy theories, and there are plenty\nabout Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big\nmistake after Bruce Lee’s death. Hoping to protect Lee’s image, Chow’s production company\nclaimed the actor died at home with his wife, Linda. But once the press found out the truth, the\ntabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant')]"
47,dense,True,B,B,0.59,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'page': 20, 'source': './data/paper_refrag.pdf'}, page_content='m .\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput= bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\n\x00s\nk +o\n\x01\nTTFT (24d2+4ds)lbs\nf\n(24d2+4d s\nk )lb s\nk\nf\nTTIT 2n+4dlb(s+o)\nm\n2n+4dlb( s\nk +o)\nm\nThroughput bo\nTTFTbefore+TTITbefore\nbo\nTTFTafter+TTITafter\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 14}, page_content='Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. In\nAssociation for Computational Linguistics (ACL), 2024.\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel. Adding recurrence to pretrained transformers, 2021.https:\n//openreview.net/forum?id=taQNxF9Sj6.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill\nDolan. DIALOGPT : Large-scale generative pre-training for conversational response generation. In Asli Celikyilmaz\nand Tsung-Hsien Wen, editors,Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278, Online, July 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.aclâĂŚdemos.30.https://aclanthology.org/2020.aclâĂŚdemos.30/.\n15'), Document(metadata={'page': 12, 'source': './data/paper_refrag.pdf'}, page_content='Journal of Machine Learning Research, 24(251):1–43, 2023b.\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for\naccelerated inference of large language models. InProceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 13358–13376, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.825.https://aclanthology.org/2023.emnlp-main.825/.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LongLLMLingua:\nAccelerating and enhancing LLMs in long context scenarios via prompt compression. InProceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Bangkok, Thailand,\nAugust 2024. Association for Computational Linguistics.\nYuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into a single vector\nand back again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar, editors,Proceedings of the 63rd Annual Meeting of the Association for'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then')]"
47,hybrid,True,B,B,1.61,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 20}, page_content='m .\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput= bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\n\x00s\nk +o\n\x01\nTTFT (24d2+4ds)lbs\nf\n(24d2+4d s\nk )lb s\nk\nf\nTTIT 2n+4dlb(s+o)\nm\n2n+4dlb( s\nk +o)\nm\nThroughput bo\nTTFTbefore+TTITbefore\nbo\nTTFTafter+TTITafter\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 14, 'source': './data/paper_refrag.pdf'}, page_content='Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. In\nAssociation for Computational Linguistics (ACL), 2024.\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel. Adding recurrence to pretrained transformers, 2021.https:\n//openreview.net/forum?id=taQNxF9Sj6.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill\nDolan. DIALOGPT : Large-scale generative pre-training for conversational response generation. In Asli Celikyilmaz\nand Tsung-Hsien Wen, editors,Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278, Online, July 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.aclâĂŚdemos.30.https://aclanthology.org/2020.aclâĂŚdemos.30/.\n15'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 12, 'source': './data/paper_refrag.pdf'}, page_content='Journal of Machine Learning Research, 24(251):1–43, 2023b.\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for\naccelerated inference of large language models. InProceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pages 13358–13376, Singapore, December 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.emnlp-main.825.https://aclanthology.org/2023.emnlp-main.825/.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LongLLMLingua:\nAccelerating and enhancing LLMs in long context scenarios via prompt compression. InProceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Bangkok, Thailand,\nAugust 2024. Association for Computational Linguistics.\nYuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into a single vector\nand back again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar, editors,Proceedings of the 63rd Annual Meeting of the Association for'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 24}, page_content='the brain. At the time, doctors believed the brain swelling was due to a painkiller. The oedema,\naccording to a group of researchers, was brought on by hyponatraemia. In their study, which was\npublished in the Clinical Kidney Journal, the researchers proposed that Bruce Lee died because\nhis kidneys were unable to eliminate extra water. The findings are very different from old theories\nabout how died, such as those regarding gangster assassination, jealous lover poisoning, curses, and\nheatstroke. According to scientists, the actor may have died from hyponatraemia, which develops\nwhen the body’s sodium levels get diluted as a result of consuming too much water. The cells in\nthe body, particularly those in the brain,\nP2 circumstances, you’re bound to get some truly insane conspiracy theories, and there are plenty\nabout Bruce Lee. The crazy Bruce Lee murder theories Producer Raymond Chow made a big\nmistake after Bruce Lee’s death. Hoping to protect Lee’s image, Chow’s production company\nclaimed the actor died at home with his wife, Linda. But once the press found out the truth, the\ntabloids got going. In fact, a lot of people pointed the finger at Betty Ting Pei, claiming she was'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then')]"
47,cross_encoder,True,B,B,1.82,B,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 20, 'source': './data/paper_refrag.pdf'}, page_content='m .\nThroughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given\nby:\nThroughput= bo\nTTFT+DL\nwhere DL is the data latency.\nBefore After\nKV cache memory4dlb(s+o) 4dlb\n\x00s\nk +o\n\x01\nTTFT (24d2+4ds)lbs\nf\n(24d2+4d s\nk )lb s\nk\nf\nTTIT 2n+4dlb(s+o)\nm\n2n+4dlb( s\nk +o)\nm\nThroughput bo\nTTFTbefore+TTITbefore\nbo\nTTFTafter+TTITafter\nTable 9Comparison of KV cache memory usage, TTFT, generation latency and throughput between the original\nLLaMA model and our model.\nB.5 Additional details on empirical measurement of latency and memory improvement in fig-\nure 2, figure 9 and figure 8\nWe measure the latency and memory usage in a controlled environment which aims to reduce other environ-\nmental factors that could make certain method advantageous.\nTo this end, our implementation uses the same modelling file which means different baselines share the same\nhyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results')]"
48,baseline,True,B,B,0.42,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
48,bm25,True,B,B,0.73,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.018421052631578946,"[Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'page': 19, 'source': './data/paper_refrag.pdf'}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.'), Document(metadata={'page': 28, 'source': './data/paper_refrag.pdf'}, page_content='show that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .\nmachine learning methods are applied to finding the green s func-\ntion of the anderson impurity model , a basic model system of\nquantum many - body condensed - matter physics . different\nmethods of parametrizing the green s function are investigated ;\na representation in terms of legendre polynomials is found to be\nsuperior due to its limited number of coefficients and its applica-\nbility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='BoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric\nsequence for each type of data point, based on the intuition that training should begin with a greater\nproportion of easier examples and gradually introduce more challenging ones as training progresses. The\nright-most column indicates the total number of data points for each type. We allocate more data points to\nlonger-context examples to encourage the model to focus on learning more difficult tasks.\nB.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model\nIn this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model.\nWe denote the following parameters:s as the context length,o as the output length,b as the batch size,d as\n19'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
48,dense,True,B,B,0.92,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005263157894736842,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 19}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
48,hybrid,True,B,B,0.62,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005263157894736842,"[Document(metadata={'page': 19, 'source': './data/paper_refrag.pdf'}, page_content='Stage 1 Stage 3 Stage 5 Stage 7 Stage 9\nTraining Stage\n0\n50\n100\nPercentage\nContext\n1 × k\n2 × k\n4 × k\n8 × k\n16 × k\n32 × k\n64 × k\n128 × k\n256 × k\nFigure 6The data mixture in curriculum learning during the training.\nFactor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation\n1×81333 445 148 49 16 6 2 1 0 2000\n2×8333 298 267 238 213 191 171 153 137 2000\n4×883 102 126 156 193 238 293 362 447 2000\n8×820 35 61 106 185 324 565 985 1719 4000\n16×85 11 23 48 103 220 468 997 2125 4000\n32×81 3 7 19 50 133 353 939 2496 4000\n64×81 3 9 25 73 212 618 1802 5259 8000\n128×81 3 9 25 73 212 618 1802 5259 8000\n256×81 3 9 25 73 212 618 1802 5259 8000\nTable 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we\nhave a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).\nFor each type of data, the number of samples in each stage is determined by a geometric sequence which sums up to\nthe total number of samples in the last column. As training proceeds, the data mixture has more and more longer\nsequences.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'page': 28, 'source': './data/paper_refrag.pdf'}, page_content='show that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .\nmachine learning methods are applied to finding the green s func-\ntion of the anderson impurity model , a basic model system of\nquantum many - body condensed - matter physics . different\nmethods of parametrizing the green s function are investigated ;\na representation in terms of legendre polynomials is found to be\nsuperior due to its limited number of coefficients and its applica-\nbility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='BoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric\nsequence for each type of data point, based on the intuition that training should begin with a greater\nproportion of easier examples and gradually introduce more challenging ones as training progresses. The\nright-most column indicates the total number of data points for each type. We allocate more data points to\nlonger-context examples to encourage the model to focus on learning more difficult tasks.\nB.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model\nIn this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model.\nWe denote the following parameters:s as the context length,o as the output length,b as the batch size,d as\n19'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88')]"
48,cross_encoder,True,B,B,2.67,B,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005263157894736842,"[Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096')]"
49,baseline,True,C,C,0.42,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
49,bm25,True,C,C,0.74,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24')]"
49,dense,True,C,C,0.71,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 26, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'page': 26, 'source': './data/paper_refrag.pdf'}, page_content='REPLUG+80 passages - - - 76.16 - 65.46 - 55.33\nLLaMA-32K+80 passages 22.01 18.04 19.97 16.56 23.69 23.80 33.19 48.62\nREFRAG8 +80 passages50.03 90.72 99.66 97.35 44.44 67.66 69.48 56.911×\nREFRAG16+80 passages49.77 90.21 99.66 95.3638.2968.12 70.57 56.912×\nREFRAG32+80 passages50.03 91.24 99.50 98.01 43.02 68.58 68.55 57.224×\n- means the corresponding model has out-of-memory error.\nTable 18Performance of our model under compression rate of 16 with different number of retrieved passages in RAG\nunder the weak retriever scenario.\n# Passages MMLU NQ FEVER WebQA FreebaseQA CommonsenseQA ECQA StrategyQA HellaSwag SIQA PIQA↑\n0 48.14 19.09 61.40 30.67 59.71 85.05 86.75 55.56 36.57 64.59 68.82\n1 49.97 20.08 64.15 38.67 64.62 87.63 92.72 71.11 39.08 68.58 70.57\n3 49.64 20.63 60.80 40.00 68.55 89.69 95.36 75.56 39.41 69.40 71.11\n5 49.84 20.60 60.45 40.00 66.09 90.21 96.69 73.33 39.52 68.63 70.95\n8 49.77 20.73 60.86 40.00 66.83 90.21 96.69 77.78 39.32 68.73 70.46\n20 50.03 21.29 62.32 36.00 68.06 89.69 95.36 75.56 38.58 69.29 70.62\n50 49.84 22.12 63.54 37.33 71.99 89.69 96.69 75.56 38.11 68.53 70.84\n80 49.77 22.63 65.07 38.67 71.74 90.21 95.36 68.89 38.29 68.12 70.57'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×')]"
49,hybrid,True,C,C,0.74,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 26, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 26, 'source': './data/paper_refrag.pdf'}, page_content='REPLUG+80 passages - - - 76.16 - 65.46 - 55.33\nLLaMA-32K+80 passages 22.01 18.04 19.97 16.56 23.69 23.80 33.19 48.62\nREFRAG8 +80 passages50.03 90.72 99.66 97.35 44.44 67.66 69.48 56.911×\nREFRAG16+80 passages49.77 90.21 99.66 95.3638.2968.12 70.57 56.912×\nREFRAG32+80 passages50.03 91.24 99.50 98.01 43.02 68.58 68.55 57.224×\n- means the corresponding model has out-of-memory error.\nTable 18Performance of our model under compression rate of 16 with different number of retrieved passages in RAG\nunder the weak retriever scenario.\n# Passages MMLU NQ FEVER WebQA FreebaseQA CommonsenseQA ECQA StrategyQA HellaSwag SIQA PIQA↑\n0 48.14 19.09 61.40 30.67 59.71 85.05 86.75 55.56 36.57 64.59 68.82\n1 49.97 20.08 64.15 38.67 64.62 87.63 92.72 71.11 39.08 68.58 70.57\n3 49.64 20.63 60.80 40.00 68.55 89.69 95.36 75.56 39.41 69.40 71.11\n5 49.84 20.60 60.45 40.00 66.09 90.21 96.69 73.33 39.52 68.63 70.95\n8 49.77 20.73 60.86 40.00 66.83 90.21 96.69 77.78 39.32 68.73 70.46\n20 50.03 21.29 62.32 36.00 68.06 89.69 95.36 75.56 38.58 69.29 70.62\n50 49.84 22.12 63.54 37.33 71.99 89.69 96.69 75.56 38.11 68.53 70.84\n80 49.77 22.63 65.07 38.67 71.74 90.21 95.36 68.89 38.29 68.12 70.57'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×')]"
49,cross_encoder,True,C,C,2.16,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×')]"
50,baseline,False,A,C,0.52,A,❌ FALLO TOTAL,0.0,[]
50,bm25,True,C,C,0.6,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.07936507936507936,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 28, 'source': './data/paper_refrag.pdf'}, page_content='show that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .\nmachine learning methods are applied to finding the green s func-\ntion of the anderson impurity model , a basic model system of\nquantum many - body condensed - matter physics . different\nmethods of parametrizing the green s function are investigated ;\na representation in terms of legendre polynomials is found to be\nsuperior due to its limited number of coefficients and its applica-\nbility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based')]"
50,dense,True,C,C,0.64,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×\nREFRAG8 + 8 passages 50.2992.27 99.66 94.70 45.23 68.94 71.38 57.701×\nREFRAG16+ 8 passages 49.8489.18 99.66 98.0139.3368.42 70.29 56.672×\nREFRAG32+ 8 passages 49.5191.75 99.50 97.35 42.86 68.17 68.34 56.754×\nLong context\nLLaMAFT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -\nLLaMA-32K+80 passages 22.21 16.49 19.80 16.56 23.76 24.16 34.17 48.86\nREFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%')]"
50,hybrid,True,C,C,1.05,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 28, 'source': './data/paper_refrag.pdf'}, page_content='show that both optimisations yield improvements in generalisation\nperformance . our results also indicate that atypicality , non -\ntypicality , and friendly - neighbourhood size are estimates of the\nimportance of instance types for their classification , rather than\ntheir removability .\nmachine learning methods are applied to finding the green s func-\ntion of the anderson impurity model , a basic model system of\nquantum many - body condensed - matter physics . different\nmethods of parametrizing the green s function are investigated ;\na representation in terms of legendre polynomials is found to be\nsuperior due to its limited number of coefficients and its applica-\nbility to state of the art methods of solution . the dependence of\nthe errors on the size of the training set is determined . the results\nindicate that a machine learning approach to dynamical mean -\nfield theory may be feasible .\newe demonstrate how machine learning techniques can be used to\nsolve the quantum many - body problem . we apply the method\nto the anderson impurity model , where we use the exact diago-\nnalization method to train the machine learning algorithms . we'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×\nREFRAG8 + 8 passages 50.2992.27 99.66 94.70 45.23 68.94 71.38 57.701×\nREFRAG16+ 8 passages 49.8489.18 99.66 98.0139.3368.42 70.29 56.672×\nREFRAG32+ 8 passages 49.5191.75 99.50 97.35 42.86 68.17 68.34 56.754×\nLong context\nLLaMAFT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -\nLLaMA-32K+80 passages 22.21 16.49 19.80 16.56 23.76 24.16 34.17 48.86\nREFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%')]"
50,cross_encoder,True,C,C,2.24,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved')]"
51,baseline,True,C,C,0.42,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
51,bm25,True,C,C,0.52,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 20}, page_content='hyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results\nSparse attention across different retrieved passages.We retrieve 200 passages using the query “how bruce\nlee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to\nsimulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it\nto LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the\nattention values for tokens within each passages are significantly larger than attention values for tokens in\ndifferent passages which suggests redundancy in the current attention computation for RAG applications.\n21')]"
51,dense,True,C,C,0.88,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 20, 'source': './data/paper_refrag.pdf'}, page_content='hyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results\nSparse attention across different retrieved passages.We retrieve 200 passages using the query “how bruce\nlee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to\nsimulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it\nto LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the\nattention values for tokens within each passages are significantly larger than attention values for tokens in\ndifferent passages which suggests redundancy in the current attention computation for RAG applications.\n21'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We')]"
51,hybrid,True,C,C,1.03,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 20}, page_content='hyper-parameter and acceleration (e.g., flash-attention). Therefore, we restrict the factors that affect the\nresource usage only among the model designs. We use the batch size of1and use a single A100 card to\nmeasure the system performance.\nC Additional Experimental Results\nSparse attention across different retrieved passages.We retrieve 200 passages using the query “how bruce\nlee died” from our retrieval corpus. We choose 5 passages that are different from each other (table 10) to\nsimulate the de-duplication process in real RAG applications. We concatenate these 5 passages and feed it\nto LLaMA-2-7B-Chat model to see the attention values between different tokens. Figure 7 shows that the\nattention values for tokens within each passages are significantly larger than attention values for tokens in\ndifferent passages which suggests redundancy in the current attention computation for RAG applications.\n21'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 10, 'source': './data/paper_refrag.pdf'}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We')]"
51,cross_encoder,True,C,C,1.4,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight')]"
52,baseline,True,C,C,0.52,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
52,bm25,True,C,C,0.73,C,✅ ACIERTO PERFECTO (RAG),0.5273224043715847,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
52,dense,True,C,C,0.71,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.00546448087431694,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'page': 12, 'source': './data/paper_refrag.pdf'}, page_content='Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into a single vector\nand back again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar, editors,Proceedings of the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 19323–19339, Vienna, Austria, July 2025. Association\nfor Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.948.https://aclanthology.\norg/2025.acl-long.948/.\nBozhou Li, Hao Liang, Zimo Meng, and Wentao Zhang. Are bigger encoders always better in vision large models?\narXiv preprint arXiv:2408.00620, August 2024. Preprint.\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large\nlanguage models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\npages 6342–6353, Singapore, December 2023. Association for Computational Linguistics.https://aclanthology.org/\n2023.emnlp-main.391.pdf.')]"
52,hybrid,True,C,C,0.7,C,✅ ACIERTO PERFECTO (RAG),0.5273224043715847,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 12, 'source': './data/paper_refrag.pdf'}, page_content='Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into a single vector\nand back again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar, editors,Proceedings of the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 19323–19339, Vienna, Austria, July 2025. Association\nfor Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.948.https://aclanthology.\norg/2025.acl-long.948/.\nBozhou Li, Hao Liang, Zimo Meng, and Wentao Zhang. Are bigger encoders always better in vision large models?\narXiv preprint arXiv:2408.00620, August 2024. Preprint.\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large\nlanguage models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\npages 6342–6353, Singapore, December 2023. Association for Computational Linguistics.https://aclanthology.org/\n2023.emnlp-main.391.pdf.')]"
52,cross_encoder,True,C,C,1.77,C,✅ ACIERTO PERFECTO (RAG),0.5273224043715847,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.')]"
53,baseline,True,C,C,0.62,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
53,bm25,True,C,C,0.52,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our')]"
53,dense,True,C,C,0.93,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated')]"
53,hybrid,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated')]"
53,cross_encoder,True,C,C,1.96,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.020512820512820513,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the')]"
54,baseline,False,B,C,0.62,B,❌ FALLO TOTAL,0.0,[]
54,bm25,True,C,C,0.51,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.046242774566473986,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial')]"
54,dense,True,C,C,1.14,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.011560693641618497,"[Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across')]"
54,hybrid,True,C,C,0.72,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.03468208092485549,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across')]"
54,cross_encoder,True,C,C,2.14,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005780346820809248,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can')]"
55,baseline,False,D,C,0.61,D,❌ FALLO TOTAL,0.0,[]
55,bm25,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The')]"
55,dense,True,C,C,1.03,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.46\n1.48\n1.50\nLoss\nLlama-2-7B\nLlama-2-13B\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.46\n1.47\n1.48\n1.49\n1.50\n1.51\nLoss\nRoberta-Base\nRoberta-Large\nFigure 11Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder\nthe Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model.\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.45\n1.46\n1.47\n1.48\n1.49\n1.50\nLoss\nRoberta-Base\nRoberta-Large\nFigure 12Training trajectory for different encoder paired with LLaMA-2-13B decoder.\noutperforms others. Table 15 shows the performance ofREFRAGunder different number of context for\nstrong retriever setting.\nDemonstration of generated summary for Arxiv and Pubmed articles.Table 20 and table 19 shows the ground\ntrue abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the')]"
55,hybrid,True,C,C,1.06,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.46\n1.48\n1.50\nLoss\nLlama-2-7B\nLlama-2-13B\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.46\n1.47\n1.48\n1.49\n1.50\n1.51\nLoss\nRoberta-Base\nRoberta-Large\nFigure 11Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder\nthe Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model.\n10000 20000 30000 40000 50000 60000\nTraining Steps\n1.44\n1.45\n1.46\n1.47\n1.48\n1.49\n1.50\nLoss\nRoberta-Base\nRoberta-Large\nFigure 12Training trajectory for different encoder paired with LLaMA-2-13B decoder.\noutperforms others. Table 15 shows the performance ofREFRAGunder different number of context for\nstrong retriever setting.\nDemonstration of generated summary for Arxiv and Pubmed articles.Table 20 and table 19 shows the ground\ntrue abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT\nHyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the\nencoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the\n18'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='retrieval from a trillion-token data store, achieving strong performance. Subsequent work by Shi et al. (2024)\nand Lin et al. (2024) focused on fine-tuning existing LLMs by prepending retrieved passages to prompts and\nemploying ensemble methods for response generation. Additionally, Izacard and Grave (2021) introduced\nfusion-in-decoder, which uses an encoder to process each passage in parallel and concatenates the hidden states\nfor generation via a decoder. This approach accelerates attention computation by removing cross-document\nattention, but does not apply compression in the decoder, which could further reduce latency.\nEfficient Long-Context LLMs.Recent research has investigated various strategies to reduce memory usage\nand accelerate latency in long-context generation for LLMs. Choromanski et al. (2021) introduced compressed\nattention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the')]"
55,cross_encoder,True,C,C,2.75,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the')]"
56,baseline,True,C,C,0.41,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
56,bm25,True,C,C,0.93,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 21}, page_content='P0P1P2P3P4\nlayer 22\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 23\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 24\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 25\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 26\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 27\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 28\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 29\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 30\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 31\nall layers avg\nFigure 7Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model.\nThe diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are\nthe averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10.\nAdditional results in latency measurement.Figure 9 and figure 8 shows the latency comparison of different\nmodels when usingk= 8andk= 32compression rate forREFRAGrespectively.\nAblation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22')]"
56,dense,True,C,C,1.13,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,')]"
56,hybrid,True,C,C,0.82,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 21}, page_content='P0P1P2P3P4\nlayer 22\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 23\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 24\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 25\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 26\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 27\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 28\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 29\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 30\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 31\nall layers avg\nFigure 7Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model.\nThe diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are\nthe averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10.\nAdditional results in latency measurement.Figure 9 and figure 8 shows the latency comparison of different\nmodels when usingk= 8andk= 32compression rate forREFRAGrespectively.\nAblation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,')]"
56,cross_encoder,True,C,C,2.68,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
57,baseline,False,B,C,0.41,B,❌ FALLO TOTAL,0.0,[]
57,bm25,True,C,C,0.63,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0033003300330033004,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices')]"
57,dense,False,B,C,0.61,B,❌ FALLO TOTAL,0.006600660066006601,"[Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 16, 'source': './data/paper_refrag.pdf'}, page_content='Motivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices\nl1, . . . , lT′, wherel t ∈[L]. At staget, the policy samples from:\nπθ(lt =i|x,{l j}t−1\nj=1) :=π θ(lt =i|{c j}L\nj=1,{l j}t−1\nj=1) = exp(si −ni)PL\nj=1 exp(sj −nj)\n.\nwhere nj = ∞ iff j∈ {li}t−1\ni=1 and0otherwise 4;s= gθ({ci}i∈[L],i/∈{lj}t−1\nj=1\n)is the output of a two-layer\ntransformer network over chunk embeddings, producing logitsi for each chunk. In practice, we reuse chunk\nembeddings {ci}L\ni=1 as transformer input and do not recompute logitssi after each selection, as state changes\nhave minimal impact and this improves training speed.\nWe use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to\nminimize contamination across different segment prediction task. Specifically, for eachx we randomly select\nGnumber of lengthT ′ action sequences{l (i)}G\ni=1 . We have the following objective:\nJθ = 1\nG\nPG\ni=1 E x∼P(X),\n{l(i)}G\ni=1∼πθ([L]|x)\n1\nT′\nPT′\nt=1 min\n\x14\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture')]"
57,hybrid,True,C,C,1.03,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.013201320132013201,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='Motivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices\nl1, . . . , lT′, wherel t ∈[L]. At staget, the policy samples from:\nπθ(lt =i|x,{l j}t−1\nj=1) :=π θ(lt =i|{c j}L\nj=1,{l j}t−1\nj=1) = exp(si −ni)PL\nj=1 exp(sj −nj)\n.\nwhere nj = ∞ iff j∈ {li}t−1\ni=1 and0otherwise 4;s= gθ({ci}i∈[L],i/∈{lj}t−1\nj=1\n)is the output of a two-layer\ntransformer network over chunk embeddings, producing logitsi for each chunk. In practice, we reuse chunk\nembeddings {ci}L\ni=1 as transformer input and do not recompute logitssi after each selection, as state changes\nhave minimal impact and this improves training speed.\nWe use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to\nminimize contamination across different segment prediction task. Specifically, for eachx we randomly select\nGnumber of lengthT ′ action sequences{l (i)}G\ni=1 . We have the following objective:\nJθ = 1\nG\nPG\ni=1 E x∼P(X),\n{l(i)}G\ni=1∼πθ([L]|x)\n1\nT′\nPT′\nt=1 min\n\x14\nπθ(l(i)\nt |x,{l(i)\nj }t−1\nj=1)'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial')]"
57,cross_encoder,True,C,C,2.37,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0033003300330033004,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices')]"
58,baseline,True,C,C,0.61,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
58,bm25,True,C,C,0.84,C,✅ ACIERTO PERFECTO (RAG),0.7557251908396947,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='. the approach of particle swarms is an example for interior point\nmethods in optimization as an iterative technique . this approach\nis novel and deals with classification problems without the use\nof a traditional classifier . our method determines the optimal\nhyperplane or classification boundary for a data set . in a bi-\nnary classification problem , we constrain each class as a cluster ,\nwhich is enclosed by an ellipsoid . the estimation of the optimal\nhyperplane between the two clusters is posed as a quadratically\nconstrained quadratic problem . the optimization problem is solved\nin distributed format using modified particle swarms . our method\nhas the advantage of using the direction towards optimal solution\nrather than searching the entire feasible region . our results on the\niris , pima , wine , and thyroid datasets show that the proposed\nmethod works better than a neural network and the performance\nis close to that of svm . * keywords * quadratic programming\n; particle swarms ; hyperplane ; quadratic constraints ; binary\nclassification .\nsupport vector machines are used for classification of data in\nmachine learning . support vector machines use quadratic pro-'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='Ground True Abstract Generated Abstract\nmemory - based learning , keeping full memory of learning material\n, appears a viable approach to learning nlp tasks , and is often\nsuperior in generalisation accuracy to eager learning approaches\nthat abstract from learning material . here we investigate three\npartial memory - based learning approaches which remove from\nmemory specific task instance types estimated to be exceptional\n. the three approaches each implement one heuristic function for\nestimating exceptionality of instance types : ( i ) typicality , ( ii )\nclass prediction strength , and ( iii ) friendly - neighbourhood size\n. experiments are performed with the memory - based learning\nalgorithm ib1-ig trained on english word pronunciation . we find\nthat removing instance types with low prediction strength ( ii )\nis the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to')]"
58,dense,True,C,C,0.72,C,✅ ACIERTO PERFECTO (RAG),0.7557251908396947,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across')]"
58,hybrid,True,C,C,0.73,C,✅ ACIERTO PERFECTO (RAG),0.7557251908396947,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='. the approach of particle swarms is an example for interior point\nmethods in optimization as an iterative technique . this approach\nis novel and deals with classification problems without the use\nof a traditional classifier . our method determines the optimal\nhyperplane or classification boundary for a data set . in a bi-\nnary classification problem , we constrain each class as a cluster ,\nwhich is enclosed by an ellipsoid . the estimation of the optimal\nhyperplane between the two clusters is posed as a quadratically\nconstrained quadratic problem . the optimization problem is solved\nin distributed format using modified particle swarms . our method\nhas the advantage of using the direction towards optimal solution\nrather than searching the entire feasible region . our results on the\niris , pima , wine , and thyroid datasets show that the proposed\nmethod works better than a neural network and the performance\nis close to that of svm . * keywords * quadratic programming\n; particle swarms ; hyperplane ; quadratic constraints ; binary\nclassification .\nsupport vector machines are used for classification of data in\nmachine learning . support vector machines use quadratic pro-'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 22}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='Ground True Abstract Generated Abstract\nmemory - based learning , keeping full memory of learning material\n, appears a viable approach to learning nlp tasks , and is often\nsuperior in generalisation accuracy to eager learning approaches\nthat abstract from learning material . here we investigate three\npartial memory - based learning approaches which remove from\nmemory specific task instance types estimated to be exceptional\n. the three approaches each implement one heuristic function for\nestimating exceptionality of instance types : ( i ) typicality , ( ii )\nclass prediction strength , and ( iii ) friendly - neighbourhood size\n. experiments are performed with the memory - based learning\nalgorithm ib1-ig trained on english word pronunciation . we find\nthat removing instance types with low prediction strength ( ii )\nis the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to')]"
58,cross_encoder,True,C,C,2.77,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.011450381679389313,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
59,baseline,False,A,C,0.82,A,❌ FALLO TOTAL,0.0,[]
59,bm25,True,C,C,0.62,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss')]"
59,dense,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 26}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will')]"
59,hybrid,True,C,C,1.33,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 26}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will')]"
59,cross_encoder,True,C,C,2.98,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG')]"
60,baseline,True,C,C,0.41,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
60,bm25,True,C,C,0.63,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat')]"
60,dense,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved')]"
60,hybrid,True,C,C,1.13,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further\nreports the performance of different models under varying numbers of passages, with our model consistently\nachieving superior results on two out of three datasets for the reasons outlined above.\nTable 4Performance on multi-turn RAG tasks for # Passages = 5 and # Passages = 10.\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 5\nLLaMAFT 2 20.7318.7226.98\nREFRAG8 221.1717.73 28.04\nREFRAG16 2 20.19 17.30 27.89\nREFRAG32 2 19.70 17.3528.67\nLLaMAFT 4 20.3316.4223.50\nREFRAG8 4 22.78 15.61 26.93\nREFRAG16 421.9415.2727.03\nREFRAG32 4 21.68 15.45 26.45\nLLaMAFT 6 20.7611.9223.10\nREFRAG8 623.1110.88 25.37\nREFRAG16 6 21.69 10.7526.17\nREFRAG32 6 21.19 10.69 25.51\n# Turns (≥) ORConvQA QReCC TopiOCQA↑\n# Passages = 10\nLLaMAFT 2 16.52 17.31 23.02\nREFRAG8 221.15 17.9227.97\nREFRAG16 2 20.79 17.3728.45\nREFRAG32 2 19.67 17.16 28.31\nLLaMAFT 4 16.90 14.69 20.23\nREFRAG8 422.63 15.6825.95'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved')]"
60,cross_encoder,True,C,C,1.86,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements\nWe thank for Jason Chen, Yao Liu, Norman Huang, Xueyuan Su, Pranesh Srinivasan, Avinash Atreya, Riham\nMansour, Jeremy Teboul for insightful discussions and support.\n11')]"
61,baseline,True,C,C,0.53,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
61,bm25,True,C,C,0.52,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.')]"
61,dense,True,C,C,1.04,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
61,hybrid,True,C,C,0.62,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Donald Trump is \nthe President of \nthe United States. \nHe assumed office \non January 20, \n2025, making\nhim the 47th \nPresident of the \nUnited States.\n Encoder  Encoder  Encoder\nContext Text\nDecoder-only Foundation ModelSequence\nPrecomputable\nLight-weight\nEncoderWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nLight-weight RL-trained chunk expansion policy\nVector DBQuery \nEncoder\nFigure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to\nproduce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to\nexpand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.\ntoken embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='being uninformative and reused across multiple inferences. Allocating memory/computation for all the\ntokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The\nretrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and\nother correlations with the query are already available due to the use of vectorizations and re-rankings. This\ninformation is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity\nand other operations such as deduplication, most context chunks during decoding are unrelated, resulting in\npredominantly zero cross-attention between chunks (see figure 7).\n1.1 Our Contributions\nWe proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts\nin RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
61,cross_encoder,True,C,C,1.76,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='reconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting\ntowards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data\nmixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.\nSelective compressionREFRAGintroduces selective token compression, expanding important context chunks\nuncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as\na negative reward, determines which chunks to retain in their original form. The encoder and decoder are\nfine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting')]"
62,baseline,False,B,C,0.63,B,❌ FALLO TOTAL,0.0,[]
62,bm25,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),0.8629807692307693,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low')]"
62,dense,True,C,C,0.62,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.04807692307692308,"[Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can')]"
62,hybrid,True,C,C,1.02,C,✅ ACIERTO PERFECTO (RAG),0.8629807692307693,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='Context Text\nSequence\nPrecomputable\nWho is the President of USA?\nDecoder Tokenizer & \nEmbedding\nDecoder Input Text\nToken Embedding\nChunk \nEmbedding\nRL-trained chunk expansion policy\nReward = - Log(Perplexity)\nDonald Trump\nAnswer\nFigure 5A demonstration of selective token compression. For all chunks, by default, we compress them to a single\ntoken, while for crucial chunks, we expand them.\non other encoder-decoder combinations in our ablation experiments (see section 4.1). Each data point contains\nT = 4096tokens, where the firsts = 2048tokens are referred to as the context tokens, and the remaining\no = 2048tokens are the output tokens, such thats + o = T. We evaluate the perplexity onxs+1:s+o in this\nsection.\nLLaMA-No Context:The original pre-trained LLaMA model evaluated directly onxs+1:s+o with onlyxs+1:s+o\nas input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can')]"
62,cross_encoder,True,C,C,1.55,C,✅ ACIERTO PERFECTO (RAG),0.8629807692307693,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='number of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The\ndetails on the latency and throughput calculation are in section B.4.\nEmpirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of\nthe acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration\nin inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG\nachieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,\n2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression')]"
63,baseline,True,C,C,0.42,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
63,bm25,True,C,C,0.51,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5')]"
63,dense,True,C,C,1.09,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6')]"
63,hybrid,True,C,C,0.98,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then\nprocesses all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding\nis then projected with a projection layerϕ to match the size of the token embedding of the decoder model,\necnk\ni = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token\nembeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk\n1 , . . . ,ecnk\nL })wheree i is the\n2'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='selective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\nLog-perplexity\nProofPile\nRL Perplexity-desc Perplexity-asc Random\nFigure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages\nof chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),\nPerplexity-asc(heuristic: higher perplexity), andRandom(random selection).\n6')]"
63,cross_encoder,True,C,C,2.0,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.\nWe sequentially pickT′ chunk indicesl = {lj}T′\nj=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′\nj=1) =\n{E1, . . . , EL}, with Ei =e cnk\ni if i /∈ {lj}T′\nj=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′\nj=1\n16'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 16}, page_content='(uncompressed). This arrangement is input to the decoderMdec to predictxs+1:s+o. The decoder’s auto-\nregressive property is maintained, and compression can be applied at any position within the input, not just\nat the beginning. Within our selective compression framework, the objective is to chooseT′ chunks fromL\ntotal chunks to maximize a specified reward. Formally, this can be expressed as the following combinatorial\noptimization problem:\nGiven[L] :={1,2, . . . , L},\nmax\nl⊆[L]\nr(x, l)\ns.t.|l|=T ′\nThis problem is non-differentiable due to its discrete nature, and exact solutions are NP-hard. Consequently,\nprior work has proposed greedy approaches that incrementally construct solutions by modeling the task as a\nsequential decision-making problem (Dai et al., 2017; Bello et al., 2017). These studies show that such greedy\nformulations enable the use of RL to achieve near-optimal solutions and generalize well across diverse settings.\nMotivated by these findings, we adopt a sequential formulation for selective compression and employ RL to\ntrain an effective policy (see section 2).\nWe learn a policy networkπθ that takes chunk embeddings{ci}L\ni=1 and sequentially selectsT′ chunk indices'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages\nchunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s\nautoregressive property and enabling flexible placement of compression. Further discussion on sequential\nselection is provided in section A.1.\n4'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='KV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression\nIn this section, we introduce selective token compression, based on the hypothesis that different context\nsegments contribute unequally to answer prediction. Less critical segments are compressed, while essential\nones remain intact, as illustrated in figure 5. We employ RL to train a policy that optimally determines which\nsegments to compress.\nTo enable selective compression, we continue pretraining the encoder and decoder to process a combination\nof token and chunk embeddings. Given a context ofs tokens x1, . . . , xs, chunked intoL fixed-length chunks\nC1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain\nuncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at\narbitrary positions, which is essential for the subsequent RL policy learning.')]"
64,baseline,False,B,C,0.58,B,❌ FALLO TOTAL,0.0,[]
64,bm25,True,C,C,0.52,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.011235955056179775,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='Ground True Abstract Generated Abstract\nmemory - based learning , keeping full memory of learning material\n, appears a viable approach to learning nlp tasks , and is often\nsuperior in generalisation accuracy to eager learning approaches\nthat abstract from learning material . here we investigate three\npartial memory - based learning approaches which remove from\nmemory specific task instance types estimated to be exceptional\n. the three approaches each implement one heuristic function for\nestimating exceptionality of instance types : ( i ) typicality , ( ii )\nclass prediction strength , and ( iii ) friendly - neighbourhood size\n. experiments are performed with the memory - based learning\nalgorithm ib1-ig trained on english word pronunciation . we find\nthat removing instance types with low prediction strength ( ii )\nis the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8')]"
64,dense,True,C,C,0.79,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 5}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 25}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
64,hybrid,True,C,C,0.94,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='policy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can\ntherefore be compressed with minimal information loss. Ideally, this approach should outperform random\nselection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves\nsuperior performance across varying compression ratesp.\n4.1 Ablation Study\nCurriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while\nintuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the\n2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect\nLLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)\n5'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 28}, page_content='Ground True Abstract Generated Abstract\nmemory - based learning , keeping full memory of learning material\n, appears a viable approach to learning nlp tasks , and is often\nsuperior in generalisation accuracy to eager learning approaches\nthat abstract from learning material . here we investigate three\npartial memory - based learning approaches which remove from\nmemory specific task instance types estimated to be exceptional\n. the three approaches each implement one heuristic function for\nestimating exceptionality of instance types : ( i ) typicality , ( ii )\nclass prediction strength , and ( iii ) friendly - neighbourhood size\n. experiments are performed with the memory - based learning\nalgorithm ib1-ig trained on english word pronunciation . we find\nthat removing instance types with low prediction strength ( ii )\nis the only tested method which does not seriously harm generali-\nsation accuracy . we conclude that keeping full memory of types\nrather than tokens , and excluding minority ambiguities appear to\nbe the only performance - preserving optimisations of memory -\nbased learning .\nmemory - based learning is a prominent and successful approach to'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
64,cross_encoder,True,C,C,2.59,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting')]"
65,baseline,True,C,C,0.62,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
65,bm25,True,C,C,0.73,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.008955223880597015,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 21}, page_content='P0P1P2P3P4\nlayer 22\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 23\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 24\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 25\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 26\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 27\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 28\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 29\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 30\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 31\nall layers avg\nFigure 7Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model.\nThe diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are\nthe averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10.\nAdditional results in latency measurement.Figure 9 and figure 8 shows the latency comparison of different\nmodels when usingk= 8andk= 32compression rate forREFRAGrespectively.\nAblation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture')]"
65,dense,True,C,C,0.63,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005970149253731343,"[Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,')]"
65,hybrid,True,C,C,0.71,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.18208955223880596,"[Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 18, 'source': './data/paper_refrag.pdf'}, page_content='parameters in the model, including the decoder parameters. For all the instruction-tuning tasks, we use the\npeak learning rate of2e− 5. We use a4%linear warm-up stage for learning rate, AdamW optimizer (Loshchilov\nand Hutter, 2019), cosine learning rate scheduler and a batch size of256for all the experiments. For the\nprojection layer, we use a 2-layer multi-layer perception (MLP) with an hidden size that is equivalent to the\noutput size (i.e.,4096for LLaMA-2-7B). For both tasks we train our model for 4 epochs on the dataset using\nthe curriculum learning schedule (see figure 6).\nComputational Resources.We train all our models in Bfloat16 precision. We adopt Fully Sharded Data\nParallel (FSDP) for all the experiments and train our model on 8 nodes with 8 H100 cards on each node.\nEvaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.\nThe reconstruction task was specifically chosen to encourage the model to rely on context memory rather\nthan its parametric memory during training. Once the encoder is aligned with the decoder through this\nreconstruction task, we initiate CPT byunfreezing the decoder.\nCurriculum learning.The training tasks described in the previous section may seem straightforward, but they\nare inherently complex. As the chunk lengthk increases, the number of possible token combinations expands\nexponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 21}, page_content='P0P1P2P3P4\nlayer 22\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 23\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 24\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 25\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 26\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 27\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 28\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 29\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 30\nP0 P1 P2 P3 P4\nP0P1P2P3P4\nlayer 31\nall layers avg\nFigure 7Attention value visualization for different retrieved passages for different layers for LLaMA-2-7B-Chat model.\nThe diagonal values are the averaged attention value for tokens within each passage while the off-diagonal values are\nthe averaged attention value between tokens from different passages. The detail of retrieved passages is in table 10.\nAdditional results in latency measurement.Figure 9 and figure 8 shows the latency comparison of different\nmodels when usingk= 8andk= 32compression rate forREFRAGrespectively.\nAblation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the\nsuccess of reconstruction task.\nAblation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and\nwithout continuing from reconstruction task.\n22'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,')]"
65,cross_encoder,True,C,C,2.07,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0029850746268656717,"[Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity\nwithin a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens\nfromLchunk embeddings further compounds the difficulty of the task.\nCounterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce\nperplexity, even for the reconstruction task.To address the optimization challenge, we propose employing\ncurriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the\nmodel to gradually and effectively acquire complex skills. For the reconstruction task, training begins with\nreconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder\nreconstructs thek tokens using the projected chunk embeddingecnk\n1 . Subsequently, the model reconstructs\nx1:2k frome cnk\n1 , ecnk\n2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over\ntime, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 25, 'source': './data/paper_refrag.pdf'}, page_content='Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported\nas average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.397 0.734 0.203 0.021\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o curriculum 3.719 3.098 2.272 1.599\nREFRAGwith curriculum 0.669 0.451 0.230 0.135\nTable 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.\nLog-Perplexity is reported as average of Arxiv and Book domain.\nP16 P32 P128 P2048↓\nLLaMA-Full Context1.448 1.458 1.464 1.449\nLLaMA-No Context3.483 2.981 2.249 1.590\nREFRAGw/o reconstruction 3.272 2.789 2.119 1.544\nREFRAGwith reconstruction 2.017 1.837 1.632 1.453\nTable 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and\nselective compression (i.e.,REFRAG16+RL).\nArxiv Book PG19 ProofPile\nCompression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓\nContext Length=2048\nREFRAG8 8 1.124 1.091 1.062 1.905 1.868 1.844 1.996 1.9561.9270.997 0.952 0.916\nREFRAG16+RL 8.2581.118 1.0901.0621.878 1.856 1.840 1.978 1.9521.9300.992 0.9510.916\nContext Length=4096'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.\n3.1 Continual Pre-training Recipe\nTo ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task\nand a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor\nachieving strong CPT performance.\nReconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in\nthe decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The\nmain objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with\nminimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the\ndecoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.')]"
66,baseline,True,C,C,0.52,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
66,bm25,True,C,C,0.94,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based')]"
66,dense,True,C,C,0.63,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then')]"
66,hybrid,True,C,C,0.92,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='true abstract for different articles and the generated summary fromREFRAG. These results complement the\nperplexity results we have shown in CPT and accuracy/F1 performance we have shown in RAG and other\napplications.\nC.1 Additional Contextual Application - Summarization Task\nWe fine-tune our model on the long document summarization dataset (Cohan et al., 2018). This dataset\ncontains long scientific articles from Arxiv and Pubmed, and the task is to generate the abstract given the\nentire article. This application is challenging due to the long-context nature of the task. We fine-tune the\nREFRAGandLLaMAmodels on these two datasets and report the performance on the validation set. The\nsummarization task provides an ideal condition to inspect whether it is beneficial to bring more information\nwith compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion\n1REFRAGwithout cache means that we recompute the chunk embedding for the context and take this latency into account.\n3'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='the KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,\ngenerating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive\nnature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding\nlatency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings\nfor prediction, similar to our method. However, their sequential compression process results in high latency\nwhen the summary vector is not cached, and their approach only supports applications where the summary\ntoken is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture\nWe denote the decoder model asMdec and the encoder model asMenc. Given an input withT tokens\nx1, x2, . . . , xT , we assume that the firstq tokens are main input tokens (e.g., questions) and the lasts tokens\nare context tokens (e.g., retrieved passages in RAG). We haveq + s = T. For clarity, we focus on a single\nturn of question and retrieval in this section.\nModel overview.Figure 1 shows the main architecture ofREFRAG. This model consists of a decoder-only\nfoundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu\net al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into\nL := s\nk number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then')]"
66,cross_encoder,True,C,C,1.75,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our\nwork is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within\nthe prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our\nmethod learns where to apply compression, allowing for adaptive compression rates at inference time without\nrecomputing chunk embeddings.\nPrompt compression.Prompt compression seeks to reduce input token length to lower latency and cost\nwhile maintaining task performance. A prominent approach isLLMLingua(Jiang et al., 2023),which employs\ncoarse-to-fine, budget-controlled compression with token-level iterative refinement, achieving high compression\nratios with minimal performance loss.LongLLMLingua(Jiang et al., 2024) extends this method to long-context\nscenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='REFRAG: Rethinking RAG based Decoding\nXiaoqiang Lin1,2,∗,Aritra Ghosh1,Bryan Kian Hsiang Low2,Anshumali Shrivastava1,3,Vijai Mohan1\n1Meta Superintelligence Labs,2National University of Singapore,3Rice University\n∗Work done at Meta\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive\nexternal knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-\naugmented generation (RAG). However, processing long-context inputs introduces significant system\nlatency and demands substantial memory for the key-value cache, resulting in reduced throughput\nand a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing\nlatency for long-context inputs is a primary objective for LLMs, we contend that RAG systems\nrequire specialized consideration. In RAG, much of the LLM context consists of concatenated\npassages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based')]"
67,baseline,True,C,C,0.83,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
67,bm25,True,C,C,0.83,C,✅ ACIERTO PERFECTO (RAG),0.9042253521126761,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025')]"
67,dense,True,C,C,1.02,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0028169014084507044,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 26, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The')]"
67,hybrid,True,C,C,0.65,C,✅ ACIERTO PERFECTO (RAG),0.9042253521126761,"[Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 26}, page_content='REFRAG8 49.9091.24 99.66 97.3545.03 68.27 70.9557.22\nREFRAG16 49.84 90.21 99.66 96.69 39.5268.6370.95 56.35\nREFRAG32 49.84 91.24 99.50 97.35 42.71 68.32 68.72 56.12\nTable 17Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the weak retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage 20.20 57.70 8.32 32.00 67.08 6.71 62.2231.251×\nREFRAG8+ 8 passages21.22 63.21 11.77 42.67 67.57 8.72 68.893.241×\nREFRAG16+ 8 passages20.73 60.86 11.60 40.0066.8311.41 77.786.362×\nREFRAG32+ 8 passages21.08 62.65 11.69 42.6766.5811.41 68.892.354×\nLong context\nLLaMAFT+ 10 passages 22.27 60.40 8.32 38.67 71.50 9.40 71.11 29.941×\nCEPED+80 passages 0.02 65.18 0.02 0.00 0.00 0.00 0.0059.33\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.03 0.12 0.37 5.33 9.34 0.00 0.00 0.03\nREFRAG8 +80 passages22.92 67.87 12.22 46.67 71.99 10.0768.89 7.191×\nREFRAG16+80 passages22.6365.0712.1238.6771.748.72 68.89 12.052×\nREFRAG32+80 passages21.86 67.24 11.54 41.3370.76 8.72 66.67 6.304×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'page': 15, 'source': './data/paper_refrag.pdf'}, page_content='Appendix\nA Additional Discussion\nAnalysis on latency and throughput improvement.We denote the following parameters:s as the context\nlength, o as the output length,b as the batch size,d as the dimensionality of the hidden states,l as the\nnumber of layers in the decoder, andn as the number of model parameters. The flop rate of the GPU is\nf, and the high bandwidth memory of the GPU ism and we use the compression rate ofk in our encoder.\nWe assume that all our chunk embeddings are precomputed and cached. The model is loaded with bfloat16\nprecision. We focus our analysis on LLaMA-2-7B model. The results should be generalizable to other models.\nWe use the following metrics: TTFT which is the latency for the system to generate the first token; TTIT\nwhich is the time that it takes to generate iterative token after the first token; Throughput which is the\nnumber of tokens that are generated from the system in a unit time. Table 6 shows that with short context\nlength s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With\nlonger context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The')]"
67,cross_encoder,True,C,C,2.31,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.005633802816901409,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%\naverage improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages,REFRAGimproves\nperformance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages for\nREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.\n5.2 Multi-Turn Conversation\nWe use three different knowledge-intensive multi-turn conversation datasets: TopiOCQA (Adlakha et al.,\n2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we\nretrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='Table 3Comparison of model performance of different models with different number of retrieved passages for RAG\nunder the strong retriever scenario.\nGenerationNQ FEVER TQA WebQA FreebaseQA GSM8K StrategyQA BoolQ↑(1/ # tokens)\nShort context with the same latency\nLLaMAFT+ 1 passage23.9662.04 9.64 37.3375.187.38 64.4429.241×\nREFRAG8+ 8 passages 22.9666.59 13.05 38.6773.46 7.3875.563.301×\nREFRAG16+ 8 passages 22.9462.88 12.97 42.6771.509.40 71.115.872×\nREFRAG32+ 8 passages 22.1164.24 12.57 41.3371.7412.75 73.331.994×\nLong context\nLLaMAFT+ 10 passages26.0865.44 9.6840.00 76.176.71 68.89 30.001×\nCEPED+80 passages 0.03 65.68 0.01 0.00 0.00 0.00 0.0057.80\nREPLUG+80 passages - - - - - - 64.44 -\nLLaMA-32K+80 passages 1.24 0.14 0.52 10.67 9.83 0.00 0.00 0.03\nREFRAG8 +80 passages 24.1568.83 13.0637.33 74.207.38 71.117.031×\nREFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong')]"
68,baseline,True,C,C,0.41,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
68,bm25,True,C,C,0.94,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.009708737864077669,"[Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='as input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMAK:Similar to theLLaMA-Full Context, we pass lastK tokens xsK:s in addition toxs+1:s+o to\ncompute perplexity inxs+1:s+o. The performance ofLLaMAK falls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss')]"
68,dense,True,C,C,0.72,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 8, 'source': './data/paper_refrag.pdf'}, page_content='REFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×\nREFRAG8 + 8 passages 50.2992.27 99.66 94.70 45.23 68.94 71.38 57.701×\nREFRAG16+ 8 passages 49.8489.18 99.66 98.0139.3368.42 70.29 56.672×\nREFRAG32+ 8 passages 49.5191.75 99.50 97.35 42.86 68.17 68.34 56.754×\nLong context\nLLaMAFT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -\nLLaMA-32K+80 passages 22.21 16.49 19.80 16.56 23.76 24.16 34.17 48.86\nREFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,')]"
68,hybrid,True,C,C,1.32,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='as input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMAK:Similar to theLLaMA-Full Context, we pass lastK tokens xsK:s in addition toxs+1:s+o to\ncompute perplexity inxs+1:s+o. The performance ofLLaMAK falls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong\nretriever setting and even outperform LLaMA under the weak retriever setting. This is because our model\nenables larger context and hence enables extract more useful information when the retrieved passages are less\nrelevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings\nas the saved context can be reinvested to include additional information within the same latency budget.\nFigure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal\nnumber of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong\nretriever and a maximum of 10 passages,REFRAGmatches LLaMA’s performance while achieving a5.26×\nspeedup in TTFT. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 15}, page_content='2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer\ncontext, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration\neven without cache is that the encoder is light-weight (e.g., Roberta-large is 355M-sized) and the chunks\nare processed parallel without attending to each other. In terms of TTIT, we achieve3× acceleration in\nlong context scenario in both cached and not cached scenarios. This is expected since they have the same\nnumber of KV caches to attend to. However, CEPE is worse than original LLaMA in TTIT since it require\nthe additional computation of KV cache projection in the inference time. Overall we achieve upto6.78× and\n6.06×acceleration in throughput much higher than CEPE in the long context scenario.\nAcceleration/SaveShortsLongs\nKV cache memory ks+ko\ns+ko 1∼k×k×\nTTFT k2(6ds+s2)\n6dsk+s2 k×k 2×\nTTIT 2dlbsk+nk+2dlbok\n2dlbs+nk+2dlbok 1×k×\nThroughput k∗TTFT+k∗TTIT\nTTFT+kTTIT ∼ k2∗TTFT+k2∗TTIT\nTTFT+k∗TTIT 1∼k×k∼k 2×\nTable 6The acceleration in latency/save in memory ofREFRAGcompared to the original LLaMA model.\nA.1 Modeling REFRAG Selective Compression'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='attention, reducing attention complexity from quadratic to linear; however, this method does not address\nmemory requirements. It is complementary to our approach and can be integrated to further improve latency.\nStreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context\ngeneration, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs\ncross-attention to token embeddings from context tokens, reducing both KV cache memory and attention\ncomputations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of\nthe context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE\ndoes not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with\nour work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 8}, page_content='REFRAG16+80 passages 23.3066.01 12.6538.67 75.4312.08 73.3312.232×\nREFRAG32+80 passages 23.0268.48 12.1438.67 71.749.4068.89 6.424×\nMulti-ChoiceMMLU CommonsenseQA MathQA ECQA HellaSwag SIQA PIQA Winogrande↑\nShort context with the same latency\nLLaMAFT+ 1 context50.2385.05 99.50 84.77 41.80 68.12 67.36 55.641×\nREFRAG8 + 8 passages 50.2992.27 99.66 94.70 45.23 68.94 71.38 57.701×\nREFRAG16+ 8 passages 49.8489.18 99.66 98.0139.3368.42 70.29 56.672×\nREFRAG32+ 8 passages 49.5191.75 99.50 97.35 42.86 68.17 68.34 56.754×\nLong context\nLLaMAFT+ 10 passages 48.66 82.99 68.46 84.11 41.77 67.45 68.01 53.911×\nCEPED+80 passages 26.26 26.29 23.66 24.50 24.95 32.86 48.53 44.51\nREPLUG+80 passages - 78.35 - 76.16 - 65.51 - -\nLLaMA-32K+80 passages 22.21 16.49 19.80 16.56 23.76 24.16 34.17 48.86\nREFRAG8 +80 passages50.42 92.27 99.66 97.35 44.61 68.22 69.37 57.541×\nREFRAG16+80 passages50.88 89.69 99.66 96.6938.5068.47 70.89 56.992×\nREFRAG32+80 passages49.77 90.72 99.50 98.01 43.37 68.47 69.04 56.994×\n- means the corresponding model has out-of-memory error.\neven with a large number of passages, owing to the benefits of our compression approach. Table 5 further'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='passages from retrieval, with only a small subset directly relevant to the query. These passages\noften exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to\nblock-diagonal attention patterns that differ from those in standard LLM generation tasks. Based\non this observation, we argue that most computations over the RAG context during decoding are\nunnecessary and can be eliminated with minimal impact on performance. To this end, we propose\nREFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency\nin RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the\ntime-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In\naddition, our optimization framework for large context enablesREFRAGto extend the context size\nof LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past\ncontext into compact vectors, enabling retrieval of salient information during subsequent processing. Like\nCEPE, PCC is limited to prefix context applications and does not support arbitrary folding or expansion of\ncontexts at any position. Interestingly, Kuratov et al. (2025) investigated the capacity of LLMs to encode\nlong contexts into a single embedding, demonstrating minimal information loss for sequences up to 1500\ntokens. Their work examines the extent to which information can be compressed into a single embedding,\noffering a complementary perspective to REFRAG, which is designed for decoding from multiple compact\nembeddings within the standard decoder architecture.\nCompressive transformer.Rae et al. (2020) first introduced the compressive transformer, which compresses\nthe KV cache to reduce memory requirements for long-context applications. However, this approach only\ndecreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the\nmodel from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,')]"
68,cross_encoder,True,C,C,2.57,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 17}, page_content='architecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.\nREPLUG:A retrieval-augmented language modeling framework that uses different retrieved contexts to perform\nensemble generation. We useREPLUGto refer to applying this framework on the LLaMA pre-trained model,\nREPLUGChat to refer to applying this framework on the LLaMA chat model (i.e., instruction fine-tuned),\nand REPLUGFT to refer to applying it on the LLaMA model fine-tuned on the downstream tasks (see\nsection 5).\nREFRAG:Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,\nfeeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote\nour model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression\nusing our RL policy.\nB.2 Additional Details on Hyperparameters and Experimental Settings for CPT'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 17, 'source': './data/paper_refrag.pdf'}, page_content='as input.\nLLaMA-Full Context:Similar to theLLaMA-No Context, we evaluate the perplexity onxs+1:s+o; however,\nwe also input the whole sequence to the model, including the context tokens, i.e.,x1:T . Therefore, the\nperplexity of this model is expected to be lower thanLLaMA-No Context. The perplexity of this model\nserves as a reference, showing the upper bound of the performance of our model.\nLLaMAK:Similar to theLLaMA-Full Context, we pass lastK tokens xsK:s in addition toxs+1:s+o to\ncompute perplexity inxs+1:s+o. The performance ofLLaMAK falls betweenLLaMA-No Contextand\nLLaMA-Full Context, making it a strong baseline for comparison withREFRAGwhen the number of\ncontext tokens is matched.\nCEPE:A memory-efficient long-context model modified from the LLaMA model (Yen et al., 2024). The model\narchitecture is similar to T5. We feedx1:s into their encoder model and evaluate the perplexity on the output\ntokensx s+1:s+o.CEPEDrefers to its instruction fine-tuned variant.\nLLaMA-32K:A fine-tuned version of the original LLaMA-2 7B model that extends the context length from\nthe original 4K to 32K.'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.\nResult analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.\nOur model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three\ndatasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of\nLLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting\nin the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on\nthe sameLLaMA model without extending its effective positional encoding, maintains robust performance\n3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use\nthe stricter version and hence the reported numbers are lower in general.\n8')]"
69,baseline,True,C,C,0.53,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
69,bm25,False,D,C,0.93,D,❌ FALLO TOTAL,0.02768166089965398,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='BoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric\nsequence for each type of data point, based on the intuition that training should begin with a greater\nproportion of easier examples and gradually introduce more challenging ones as training progresses. The\nright-most column indicates the total number of data points for each type. We allocate more data points to\nlonger-context examples to encourage the model to focus on learning more difficult tasks.\nB.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model\nIn this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model.\nWe denote the following parameters:s as the context length,o as the output length,b as the batch size,d as\n19'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the')]"
69,dense,False,D,C,0.83,D,❌ FALLO TOTAL,0.02768166089965398,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 14}, page_content='Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. In\nAssociation for Computational Linguistics (ACL), 2024.\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel. Adding recurrence to pretrained transformers, 2021.https:\n//openreview.net/forum?id=taQNxF9Sj6.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill\nDolan. DIALOGPT : Large-scale generative pre-training for conversational response generation. In Asli Celikyilmaz\nand Tsung-Hsien Wen, editors,Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278, Online, July 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.aclâĂŚdemos.30.https://aclanthology.org/2020.aclâĂŚdemos.30/.\n15'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'page': 9, 'source': './data/paper_refrag.pdf'}, page_content='scenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression\nand faster inference Li et al. (2023); Liskavets et al. (2024). These approaches are complementary to our work\nand can be integrated to further reduce the latency ofREFRAG.\n10'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'page': 3, 'source': './data/paper_refrag.pdf'}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low')]"
69,hybrid,False,D,C,0.93,D,❌ FALLO TOTAL,0.02768166089965398,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='Evaluation metrics in RAG.Table 7 provides a summarization of the evaluation metrics we use for each\ndataset in RAG experiments.\nExperimental setting for fine-tuning model to take a combination of token and chunk embedding as input.\nWe continue the model training from the continual pre-training checkpoint. To fine-tune the model, we set\np = 0.1(i.e., compression90%of the chunks) and randomly select pL chunks to keep their original token in\nthe decoder. The input arrangement is the same as what we describe in section 2.\nDataset Metric\nOpenAssistant Conversations F1\nCommonsenseQA Accuracy\nMathQA Accuracy\nWeb Questions Exact Match\nWikiQA F1\nYahoo! Answers QA F1\nFreebaseQA Exact Match\nMS MARCO F1\nPubMedQA Exact Match\nQuaRel Accuracy\nGSM8K Exact Match\nStrategyQA Exact Match\nMMLU Accuracy\nBoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric'), Document(metadata={'page': 14, 'source': './data/paper_refrag.pdf'}, page_content='Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. In\nAssociation for Computational Linguistics (ACL), 2024.\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel. Adding recurrence to pretrained transformers, 2021.https:\n//openreview.net/forum?id=taQNxF9Sj6.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill\nDolan. DIALOGPT : Large-scale generative pre-training for conversational response generation. In Asli Celikyilmaz\nand Tsung-Hsien Wen, editors,Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278, Online, July 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.aclâĂŚdemos.30.https://aclanthology.org/2020.aclâĂŚdemos.30/.\n15'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low\nlatency, such as web-scale discovery. Therefore, developing novel model architectures that optimize memory\nusage and inference latency is crucial for enhancing the practicality of contextual learning in these applications.\nOptimizing inference latency for LLMs with extensive context is an active area of research, with approaches\n1\narXiv:2509.01092v2  [cs.CL]  12 Oct 2025'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='Evaluation dataset.We hold out 5% of the data for each dataset in the training dataset for evaluation.\nAdditionally, we use the datasets that are commonly used in RAG literature (Izacard et al., 2023b; Lin\net al., 2024), including MMLU (Hendrycks et al., 2021), BoolQ (Clark et al., 2019), SIQA (Sap et al., 2019),\nPIQA (Bisk et al., 2020), and Knowledge Intensive Language Tasks (KILT) (Petroni et al., 2020) (including\nHellaSwag, Winogrande, TQA, FEVER, NQ). We evaluate our performance on 2 settings: 1)Strong Retriever:\nIn this setting we use a strong retriever and retrieve the K-nearest neighbors to answer the question; 2)Weak\nRetriever: In this setting we retrieve 200 passages and pick random K passages to answer the question. The\nweak retriever setting closely resembles real-world systems, as RAG retrieval systems often suffer from error\naccumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in\ntable 7.\nRetrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl\ndumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We\nuse the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.\n(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.\nResult analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,\nvarying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.\nThis is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is\nthe original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will'), Document(metadata={'page': 0, 'source': './data/paper_refrag.pdf'}, page_content='of LLMs by16×. We provide rigorous validation ofREFRAGacross diverse long-context tasks,\nincluding RAG, multi-turn conversations, and long document summarization, spanning a wide range\nof datasets. Experimental results confirm thatREFRAGdelivers substantial speedup with no loss\nin accuracy compared to LLaMA models and other state-of-the-art baselines across various context\nsizes. Additionally, our experiments establish that the expanded context window ofREFRAGfurther\nenhances accuracy for popular applications.\nDate:October 14, 2025\nCorrespondence:Aritra Ghosh atarighosh@meta.com\nCode:Will be available athttps://github.com/facebookresearch/refrag\n1 Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in contextual learning, leveraging\ninformation from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 9}, page_content='scenarios, demonstrating significant cost and end-to-end speed improvements. Complementary approaches\nrank or prune context by estimated informativeness, e.g.,Selective Contextuses self-information to drop\nlow-value tokens, and sentence-level methods learn context-aware encoders for question-specific compression\nand faster inference Li et al. (2023); Liskavets et al. (2024). These approaches are complementary to our work\nand can be integrated to further reduce the latency ofREFRAG.\n10'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='LLaMA-32K1.037 1.862 1.960 0.898 0.965 1.867 1.947 0.834 0.865 1.840 1.943 0.770\nLLaMA-No Context1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nREPLUG1.253 1.925 2.030 1.126 1.226 1.949 2.032 1.110 1.174 1.939 2.041 1.081\nCEPE1.085 1.856 1.959 0.945 1.032 1.878 1.958 0.904 0.960 1.864 1.966 0.863\nREFRAG8 1.042 1.837 1.922 0.894 0.983 1.839 1.922 0.8580.9771.840 1.9390.891\nREFRAG16 1.058 1.847 1.934 0.910 0.994 1.845 1.932 0.871 0.942 1.840 1.945 0.850\nREFRAG32 1.088 1.8571.946 0.944 1.032 1.860 1.9450.912 0.9691.852 1.9550.880\nperformance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from\ns/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of\nthe reconstruction task.\nReconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.'), Document(metadata={'page': 1, 'source': './data/paper_refrag.pdf'}, page_content='the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight\nreinforcement learning (RL) policy that selectively determines when full chunk token input is necessary\nand when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on\ncomputationally intensive token embeddings, condensing most chunks for the query in RAG settings.\nWe provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training\nand many real word long-context applications including RAG, multi-turn conversation with RAG and long\ndocument summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity\nwhich is3 .75× than previous method. Moreover, with extended context due to our compression,REFRAG\nachieves better performance than LLaMA without incurring higher latency in the downstream applications.\n2 Model Architecture'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 18}, page_content='BoolQ Exact Match\nSIQA Accuracy\nPIQA Accuracy\nHellaSwag Accuracy\nWinogrande Accuracy\nTriviaQA Exact Match\nFEVER Exact Match\nNQ Exact Match\nTable 7Metrics used for each dataset in RAG experiments in table 3\nB.3 Curriculum learning data mixture\nTable 8 presents the number of data points used at each training stage of our model. We employ a geometric\nsequence for each type of data point, based on the intuition that training should begin with a greater\nproportion of easier examples and gradually introduce more challenging ones as training progresses. The\nright-most column indicates the total number of data points for each type. We allocate more data points to\nlonger-context examples to encourage the model to focus on learning more difficult tasks.\nB.4 Detailed Calculation of Acceleration in Latency and Throughput of Our Model\nIn this section, we provide a detailed analysis of the TTFT and generation latency for the LLaMA-2 model.\nWe denote the following parameters:s as the context length,o as the output length,b as the batch size,d as\n19'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='as decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in\nmulti-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 0}, page_content='information from their input to achieve superior performance across a range of downstream applications.\nFor instance, in multi-turn conversations (Roller et al., 2021; Zhang et al., 2020), incorporating historical\ndialogue into the context enables LLMs to respond more effectively to user queries. In retrieval-augmented\ngeneration (RAG) (Guu et al., 2020; Izacard et al., 2022), LLMs generate more accurate answers by utilizing\nrelevant search results retrieved from external sources. These examples highlight the power of LLMs to learn\nfrom context. However, it is well established that increasing prompt length for contextual learning leads\nto higher latency and greater memory consumption during inference (Yen et al., 2024). Specifically, longer\nprompts require additional memory for the key-value (KV) cache, which scales linearly with prompt length.\nMoreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token\n(TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput\ndegrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low')]"
69,cross_encoder,True,C,C,2.68,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.3217993079584775,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 3}, page_content='on empirical evaluation is in section A.\n3 Methodology\nTo align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction\ntasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number\nof tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.\nTo further enhance performance, we introduce selective compression via RL. After aligning the encoder and\ndecoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream\ntasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.\nDuring CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in\npredicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for\nnext-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any\nencoder–decoder combination so that the generations produced withcompressed contextclosely resemble\nthose generated by the original decoder with access to the full context.'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='4 Experimental Results\nTraining datasets.We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-\ntraining. This dataset contains data from Wikipedia, Arxiv, Books, StackExchange, GitHub, Commoncrawl,\nC4. We only use the Book and ArXiv domains from the dataset since these two domains contain long\ntexts (Yen et al., 2024). We sampled from this dataset to construct a20B token training dataset which\ncontains50%data from Arxiv and50%data from Book.\nEvaluation datasets.We report the performance on the Book and ArXiv domain from Slimpajama which\nare hold out for evaluation only. To inspect the generalization of the model, we also report results on the\nPG19 (Rae et al., 2019) and Proof-pile datasets (Azerbayev et al., 2023).\nBaselines.All baseline models are based on LLaMA-2-7B (Touvron et al., 2023), unless otherwise specified, to\nensure fair comparison with prior work (Yen et al., 2024; Shi et al., 2024). Each data point containsT = 4096\ntokens, split intos = 2048context ando = 2048output tokens. We evaluate perplexity onxs+1:s+o. Below, we\nbriefly describe the main baselines; further details are provided in section B.LLaMA-No Context: LLaMA-2-7B'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'page': 14, 'source': './data/paper_refrag.pdf'}, page_content='Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. In\nAssociation for Computational Linguistics (ACL), 2024.\nDavis Yoshida, Allyson Ettinger, and Kevin Gimpel. Adding recurrence to pretrained transformers, 2021.https:\n//openreview.net/forum?id=taQNxF9Sj6.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill\nDolan. DIALOGPT : Large-scale generative pre-training for conversational response generation. In Asli Celikyilmaz\nand Tsung-Hsien Wen, editors,Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278, Online, July 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.aclâĂŚdemos.30.https://aclanthology.org/2020.aclâĂŚdemos.30/.\n15'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='multi-modal models can negatively impact performance when data is scarce. To further validate our training\napproach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14\nreports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the\nArxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting\n(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,\nas evidenced by lower perplexity for a context length of4096compared to2048.\n5 Contextual Learning Applications\nIn this section, we investigate fine-tuning the model obtained from the pre-training stage to address various\ndownstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.\nFor each application, we curate an instruction-tuning dataset to facilitate model fine-tuning.\n5.1 Retrieval Augmented Generation\nTraining dataset.We follow the work of Lin et al. (2024) and use a combination of question answering datasets\nfrom 5 domains to fine-tune our model, which contains 1.1 million data points.Dialogue: OpenAssistant')]"
70,baseline,True,C,C,0.52,C,⚠️ ACIERTO SUERTE (Sin Evidencia),0.0,[]
70,bm25,True,C,C,0.63,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion')]"
70,dense,True,C,C,0.86,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 2}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 6, 'source': './data/paper_refrag.pdf'}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated')]"
70,hybrid,True,C,C,1.1,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG\nunder different compression rates in the continual pre-training task. We observe a performance regression as\nthe compression rate increases; however, even at a compression rate of32, our model remains competitive\n(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in\ndiminished performance. These findings suggest a practical limit to the compression rate beyond which the\nmodel’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 6}, page_content='0 100 101 102\nNumber of Passages\n50\n52\n54\n56\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n50\n52\n54\n56\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\n0 100 101 102\nNumber of Passages\n52\n54\nPerformance\nPerformance vs. Retrieved Passages\n0 100 101 102 103\nInput Length (Latency)\n52\n54\nPerformance\nPerformance vs. Latency\nLlama REFRAG 8 ×  compression\nFigure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a\nstrong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages\n(slightly better in a weaker retriever case) while outperform significantly under the same latency.\nThis finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing\nchunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the\neffectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression\nrate without compromising performance.\nREFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the\ninput (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This\narchitectural design leads to significant reductions in both latency and memory usage, primarily due to the\nshortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve\nthe performance which we will defer the discussion to section 2. Next, we analyze the system performance\ngains achieved with a compression rate ofk.\n103 104\n# Input T okens\n0\n10\n20\n30\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated'), Document(metadata={'page': 5, 'source': './data/paper_refrag.pdf'}, page_content='Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the\nperformance of the continual pre-training task with and without initialization from the reconstruction task.\nThe results indicate that pre-training on the reconstruction task is important for the success of continual\npre-training.\nAdvantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying\nthe number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either\nby configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8\nwith full compression, which is natively trained at a compression rate of8. This raises a natural question:\ndoes the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based\nselective compression consistently outperformsREFRAG8 across different datasets and context lengths.\n4 8 16\nCompression rate\n1.02\n1.03\n1.04\n1.05\n1.06\n1.07\nLog-perplexity\nArxiv\n4 8 16\nCompression rate\n1.82\n1.83\n1.84\n1.85\n1.86\nLog-perplexity\nBook\n4 8 16\nCompression rate\n1.91\n1.92\n1.93\n1.94\n1.95\nLog-perplexity\nPG19\n4 8 16\nCompression rate\n0.87\n0.88'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 1}, page_content='in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout\nrequiring modificationsto the LLM architecture or introducing new decoder parameters.\nREFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved\npassages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-\ntations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It\nshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed\nchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation\ncomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in\nthe context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at\narbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting\nmulti-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='pression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.\nAdditional results in RAG.Table 16 shows the performance of different baselines under the same number of\ncontext. The performance of our model is similar to other methods, in other words no model significantly\n10000 20000 30000 40000 50000 60000 70000\nTraining Steps\n1.46 × 100\n1.47 × 100\n1.48 × 100\n1.49 × 100\n1.5 × 100\n1.51 × 100\n1.52 × 100\nLoss\nx8 Compression\nx16 Compression\nx32 Compression\nx64 Compression\nFigure 10Training trajectory for our model with different compression rate.\n23'), Document(metadata={'source': './data/paper_refrag.pdf', 'page': 7}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 22, 'source': './data/paper_refrag.pdf'}, page_content='103 104\n# Input T okens\n0\n20\n40\n60\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 8Empirical verification of inference acceleration ofREFRAGwithk= 32.\n103 104\n# Input T okens\n0\n5\n10\n15\nAcceleration\nTTFT Acceleration\n103 104\n# Input T okens\n1.0\n1.5\n2.0\n2.5\n3.0\nAcceleration\nTTIT Acceleration\n103 104\n# Input T okens\n2\n4\n6Acceleration\nThroughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 9Empirical verification of inference acceleration ofREFRAGwithk= 8.\nAblation study result for the advantage of RL.Table 13 shows the advantage of using our selective compression\npolicy via RL compared to using a lower compression rate.\nAblation study result of different compression rates.Figure 10 shows the loss trajectory for different com-\npression rate ofREFRAG.\nAblation study result of different combination of encoder and decoder models.Figure 11 shows the performance\nof CPT with different combination of encoder and decoder models. Table 14 shows the performance on\nLLaMA-3.1-8B and LLaMA-3.2-3B model.')]"
70,cross_encoder,True,C,C,1.44,C,✅ ACIERTO PERFECTO (RAG),1.0,"[Document(metadata={'source': './data/paper_refrag.pdf', 'page': 4}, page_content='truncated sequencex s−K:T as input to match the token count ofREFRAG.\nTable 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.\nBolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full\ncontext without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16\nconsistently outperform other baselines across nearly all settings, while also achieving lower latency than\nCEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk\nembeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating\nthe effectiveness of compressed chunk embeddings.\nTable 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is\ntrained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.\nThe original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across'), Document(metadata={'page': 7, 'source': './data/paper_refrag.pdf'}, page_content='the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We\ncompare the performance under both the short context and the long context scenarios. For the short context,\nwe use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will\nhave the same latency as theLLaMAFT model. However, due to the compression, we are able to have more\ncontext information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both\noutperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).\nThe same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice\ntasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests\nthat most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged\nover all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that\nunder the same number of retrieved passages, we are able to match the performance of LLaMA in the strong'), Document(metadata={'page': 23, 'source': './data/paper_refrag.pdf'}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'page': 2, 'source': './data/paper_refrag.pdf'}, page_content='Throughput Acceleration\nREFRAG (Cached) REFRAG (Not Cached) CEPE\nFigure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.\nLatency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first\ntoken; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated\nper unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to\nk× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both\nmetrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG\nwith k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE\n(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared\nto CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly\noutperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared\nto CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion'), Document(metadata={'page': 4, 'source': './data/paper_refrag.pdf'}, page_content='The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via\nchunk embeddings, extending context and supporting broader applications.\nWith a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across\nfour datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE\n(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration\nincreases to30.85×over LLaMA and3.75×over CEPE.\nFigure 3 presents the performance of various methods for selective compression. We expandp fraction of the\nchunks in the original token space using the RL policy. The effective compression ratek\n1−p+kp decreases when\nfewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection\npolicy under differentp. The perplexity-based selection is an heuristic based selection which compresses\nchunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured\nby the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can')]"
