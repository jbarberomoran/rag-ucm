{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e430a911-53d6-47c1-8218-4b1bd439d632",
   "metadata": {},
   "source": [
    "# Two-Stage-Retrieval LLM RAG\n",
    "\n",
    "This project has been developed following the guidelines of the \"Modelización de problemas de la Empresa\", proposed by Management Solutions and the Faculty of Mathematics at UCM. A basic understanding of LLMs and RAG is highly recommended.\n",
    "\n",
    "It implements a question-answering (Q&A) system over a technical research paper from Meta SuperIntelligence Labs. The goal is to compare how different retrieval-augmented generation (RAG) setups help a large language model (LLM) answer multiple-choice questions.\n",
    "\n",
    "The following retrieval pipelines are compared:\n",
    "\n",
    "- **BM25** – Classic keyword search.\n",
    "\n",
    "- **Dense Retrieval** – Embedding-based semantic retrieval.\n",
    "\n",
    "- **LLM Baseline** – LLM without any retrieval.\n",
    "\n",
    "- **(Bonus) Hybrid Retrieval** – Combination of BM25 and Dense Retrieval.\n",
    "\n",
    "- **Hybrid Retrieval + Cross-Encoder** – Reranking of the chunks retrieved by hybrid retrieval.\n",
    "\n",
    "\n",
    "## Features\n",
    "\n",
    "\n",
    "The project includes:\n",
    "\n",
    "- Splitting the paper into sensible chunks and storing them in a **vector database** (ChromaDB).\n",
    "\n",
    "- Comparison between different recursive and semantic **chunk strategies**.\n",
    "\n",
    "- Running multiple executions for statistically relevant results with **high reproducibility**.\n",
    "\n",
    "- Evaluation of **accuracy** and source attribution.\n",
    "\n",
    "- Generating a concise **dashboard** comparing the different pipelines.\n",
    "\n",
    " \n",
    "\n",
    "## Project Structure\n",
    "\n",
    " The project is organized into multiple files under the src/ folder, with a clear separation of responsibilities. Here’s how it works:\n",
    "\n",
    "1. **Input Data**\n",
    "\n",
    "  All of the following are on the `data/` directory:\n",
    "- `**data/questions.json**` - Contains 50 multiple-choice questions extracted from a technical research paper.\n",
    "Each question includes: the correct answer, three distractors, and an optional reference to the source paper.\n",
    "\n",
    "- `**data/chroma_db**` - Embedded vector data base.\n",
    "- `**data/enunciado.pdf**` - The problem statement already described.\n",
    "- `**data/paper_refrag.pdf**` - The technical paper from which questions and answers are extracted.\n",
    "\n",
    "\n",
    "2. **Source Code**\n",
    "- `**main.py**` - Entry point of the project.\n",
    "Supports *Local* mode (`results/local_results/`) and *Persistent* mode (`results/persistent_results/<test_name>/`).  \n",
    "    - Calls the pipeline, saves final results, and generates plots.\n",
    "  \n",
    "\n",
    "  All of the following are on the `src/` directory:\n",
    "- `**launcher.py**` - Sets up the environment for experiments.\n",
    "    - Initializes the vector database (ChromaDB).\n",
    "    - Clears previous results if needed.\n",
    "    - Creates all necessary directories (plots, final CSVs, etc.).\n",
    "\n",
    "- `**src/ingestion.py**` - Prepares and creates the data base. Invoked by the launcher.\n",
    "\n",
    "- `**queries.py**` - Main logic to execute questions.\n",
    "    - For each question and method it:\n",
    "        - Sends the query to the LLM and retrieves documents.\n",
    "        - Computes accuracy and evidence verification.\n",
    "        - Stores partial results in results/resultados_parciales.csv.\n",
    "        - Returns a DataFrame with all results for further evaluation.\n",
    "\n",
    "- `**rag_pipeline.py**` - Implements RAG logic for different retrieval methods\n",
    "    - Contains functions to verify ground truth against retrieved documents.\n",
    "     Computes retrieval scores and status tags for each answer.\n",
    "\n",
    "- `**retrieval.py**` - Implements the retrieval engine.\n",
    "    - Provides a singleton engine to handle different retrieval methods efficiently.\n",
    "\n",
    "- `**evaluation.py**` - Evaluates the results and generates dashboards.\n",
    "    - Accuracy per method  \n",
    "    - RAG quality distribution  \n",
    "    - Response latency  \n",
    "    - Retrieval fidelity\n",
    "\n",
    "3. **Output Data**\n",
    "\n",
    "Found on the `results/` directory:\n",
    "\n",
    "- **Partial results** - Always stored in `results/resultados_parciales.csv`.\n",
    "    - Updated after each question is processed.\n",
    "\n",
    "- **Final results** - Stored in `results/local_results/` or `results/persistent_results/<test_name>`.\n",
    "    - File name: `resultados_finales.csv`.\n",
    "\n",
    "- **Plots/Dashboard** - Stored in `plots/` inside the corresponding results folder.\n",
    "    - Include:\n",
    "        - Bar charts for accuracy and RAG quality\n",
    "        - Boxplots for response latency\n",
    "        - Violin plots for retrieval fidelity\n",
    "\n",
    "> **Tip:** Run `main.py try_n` on the terminal in orden to save the try number n results in that directory<br> in orden not to overwrite other results.\n",
    "\n",
    "\n",
    "\n",
    "##  Workflow Summary\n",
    "\n",
    "1. Load questions dataset (questions.json).\n",
    "\n",
    "2. Initialize vector database (ChromaDB).\n",
    "\n",
    "3. For each question:\n",
    "    - Retrieve relevant documents (BM25 / Dense / Hybrid / Hybrid + Cross-Encoder).\n",
    "    - Query LLM for an answer.\n",
    "    - Verify against ground truth.\n",
    "    - Save partial results.\n",
    "\n",
    "4. After all questions are processed:\n",
    "    - Concatenate all results.\n",
    "    - Save final results CSV.\n",
    "    - Generate plots/dashboard for comparison and analysis.\n",
    "\n",
    "\n",
    "## Data analysis and inference\n",
    "\n",
    "Our **project** is designed for generating four plots:\n",
    "\n",
    "1. **Accuracy** on **answer selected**\n",
    "2. RAG quality\n",
    "3. Latency\n",
    "4. Retrieval fidelity\n",
    "\n",
    "discutir como se recoge cada dato y como esta valorado si se valora\n",
    "\n",
    "\n",
    "Therefore, we will discuss each **pipeline's performance** in each metric described, but before that, we must take into account the **methodology** considered for the evaluation of the RAG quality and the retrieval fidelity judge. This is due to the fact that these metrics are attached to how we evaluate likeness and the threshold of **a** \"lucky answer.\" We will give an insight into this idea:\n",
    "\n",
    "- The plots in `results/persistent_results/27-11-1/2_rag_quality.png` are obtained through a judge that is based on text matching:\n",
    "                ![Text Matching Judge](results/persistent_results/27-11-1/2_rag_quality.png)\n",
    "- Whereas the ones in `results/persistent_results/8-11-1` have an **LLM judge** that searches for the semantic match: \n",
    "                ![LLM Judge](results/persistent_results/28-11-1/2_rag_quality.png)\n",
    "\n",
    "Suddenly, **all** the pipelines have improved. However, when **manually supervised**, neither of **the** judges has **flawless performance** at this task. The first judge **incorrectly** marks as \"lucky answers\" a group that is, in fact, **a** \"correct answer,\" and the second does the opposite.\n",
    "\n",
    "This **false positives** and **false negatives** concept is an extremely important idea in statistics and in other science fields, and the general conclusion for this problem is to be **conservative**.\n",
    "\n",
    "For this reason, we will mention other methodologies we have developed but only show and explore the ones with a risk-averse approach.\n",
    "\n",
    "explicar como de bueno es cada pipeline en cada metrica y decir cual es mejor y en que condiciones\n",
    "\n",
    "## Authors\n",
    "- Jorge Barbero Morán – UCM, Faculty of Mathematics\n",
    "- David Marcos Jimeno – UCM, Faculty of Mathematics\n",
    "\n",
    "\n",
    "## License\n",
    "This project is licensed under the **MIT License**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
