{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48f3a07",
   "metadata": {},
   "source": [
    "# **Two-Stage-Retrieval LLM RAG**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cd76b",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "Formato::::::::: * [5. Data Analysis and Inference](#5-data-analysis-and-inference)\n",
    "    * [5.2. The Evaluation Challenge](#52-the-evaluation-challenge-defining-likeness)\n",
    "\n",
    "\n",
    "* [1. Introduction & Key Features](#1-introduction--key-features)\n",
    "* [2. Project Structure](#2-project-structure)\n",
    "* [3. Workflow Summary](#3-workflow-summary)\n",
    "* [4. Demonstration](~4)\n",
    "    * [4.1 Data Acquisition and Preparation](#41-data-acquisition-and-preparation)\n",
    "    * [4.2 Methodology and Pipeline Design](#32-methodology-and-pipeline-design)\n",
    "    * [4.3 Execution and Results](#43-execution-and-results)\n",
    "* [5. Data Analysis and Inference](#5-data-analysis-and-inference)\n",
    "    * [5.1. Defining Metrics and Data Collection](#51-defining-metrics-and-data-collection)\n",
    "    * [5.2. The Evaluation Challenge: Defining \"Likeness\"](#52-the-evaluation-challenge-defining-likeness)\n",
    "    * [5.3. Pipeline Performance and Inference](#53-pipeline-performance-and-inference)\n",
    "    * [5.4. Conclusion: Optimal Pipeline and Conditions](#54-conclusion-optimal-pipeline-and-conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4aa7d9",
   "metadata": {},
   "source": [
    "# 1. Introduction & Key Features\n",
    "\n",
    "\n",
    "This project implements a question-answering (Q&A) system over a technical research paper from Meta SuperIntelligence Labs. The goal is to compare how different retrieval-augmented generation (RAG) setups help a large language model (LLM) answer multiple-choice questions.\n",
    "\n",
    "The following retrieval pipelines are compared:\n",
    "\n",
    "- **LLM Baseline** ‚Äì Direct generation without retrieval.\n",
    "\n",
    "- **BM25** ‚Äì Classic sparse keyword search (TF-IDF based).\n",
    "\n",
    "- **Dense Retrieval** ‚Äì Embedding-based semantic retrieval.\n",
    "\n",
    "- **(Bonus) Hybrid Retrieval** ‚Äì Combination of BM25 and Dense Retrieval.\n",
    "\n",
    "- **(Bonus) Hybrid Retrieval + Cross-Encoder** ‚Äì A SOTA approach where a high-recall retriever fetches candidates (Hybrid, k=20) and a Cross-Encoder reranks them for high precision (k=5).\n",
    "\n",
    "\n",
    "To ensure robustness, reproducibility, and scientific rigor, this project integrates several software engineering patterns:\n",
    "\n",
    "- **Singleton Pattern for Retrieval Engine:** The `RetrievalEngine` class uses a Singleton pattern with **Lazy Loading**. This prevents memory overhead by loading heavy models only when necessary and ensures consistent database connections across the pipeline.\n",
    "\n",
    "- **Smart Ingestion Strategy:** We compare **Recursive Chunking** (optimized with high overlap to preserve scientific context) against **Semantic Chunking** (experimental), which uses embedding distances to segment text by topic.\n",
    "\n",
    "- **Automated \"Judge\" Evaluation:** Instead of relying solely on the final answer accuracy, we implement a **Ground Truth Verification** system. It compares the retrieved chunks against the source reference in the dataset to detect lucky guesses.\n",
    "\n",
    "- **Two-Stage Architecture:** A funnel approach that prioritizes *Recall* in the first stage (Hybrid Search) and *Precision* in the second stage (Cross-Encoder Re-ranking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34e29e",
   "metadata": {},
   "source": [
    "# 2. Project Structure\n",
    "\n",
    " The project is organized into multiple files under the src/ folder, with a clear separation of responsibilities. Here‚Äôs how it works:\n",
    "\n",
    "1. **Input Data**\n",
    "\n",
    "  All of the following are on the `data/` directory:\n",
    "- `**data/questions.json**` - Contains 50 multiple-choice questions extracted from a technical research paper.\n",
    "Each question includes: the correct answer, three distractors, and an optional reference to the source paper.\n",
    "\n",
    "- `**data/chroma_db**` - Embedded vector data base.\n",
    "- `**data/enunciado.pdf**` - The problem statement already described.\n",
    "- `**data/paper_refrag.pdf**` - The technical paper from which questions and answers are extracted.\n",
    "\n",
    "\n",
    "2. **Source Code**\n",
    "- `**main.py**` - Entry point of the project.\n",
    "Supports *Local* mode (`results/local_results/`) and *Persistent* mode (`results/persistent_results/<test_name>/`).  \n",
    "    - Calls the pipeline, saves final results, and generates plots.\n",
    "  \n",
    "\n",
    "  All of the following are on the `src/` directory:\n",
    "- `**launcher.py**` - Sets up the environment for experiments.\n",
    "    - Initializes the vector database (ChromaDB).\n",
    "    - Clears previous results if needed.\n",
    "    - Creates all necessary directories (plots, final CSVs, etc.).\n",
    "\n",
    "- `**src/ingestion.py**` - Prepares and creates the data base. Invoked by the launcher.\n",
    "\n",
    "- `**queries.py**` - Main logic to execute questions.\n",
    "    - For each question and method it:\n",
    "        - Sends the query to the LLM and retrieves documents.\n",
    "        - Computes accuracy and evidence verification.\n",
    "        - Stores partial results in results/resultados_parciales.csv.\n",
    "        - Returns a DataFrame with all results for further evaluation.\n",
    "\n",
    "- `**rag_pipeline.py**` - Implements RAG logic for different retrieval methods\n",
    "    - Contains functions to verify ground truth against retrieved documents.\n",
    "     Computes retrieval scores and status tags for each answer.\n",
    "\n",
    "- `**retrieval.py**` - Implements the retrieval engine.\n",
    "    - Provides a singleton engine to handle different retrieval methods efficiently.\n",
    "\n",
    "- `**evaluation.py**` - Evaluates the results and generates dashboards.\n",
    "    - Accuracy per method  \n",
    "    - RAG quality distribution  \n",
    "    - Response latency  \n",
    "    - Retrieval fidelity\n",
    "\n",
    "3. **Output Data**\n",
    "\n",
    "    Found on the `results/` directory:\n",
    "\n",
    "- **Partial results** - Always stored in `results/resultados_parciales.csv`.\n",
    "    - Updated after each question is processed.\n",
    "\n",
    "- **Final results** - Stored in `results/local_results/` or `results/persistent_results/<test_name>`.\n",
    "    - File name: `resultados_finales.csv`.\n",
    "\n",
    "- **Plots/Dashboard** - Stored in `plots/` inside the corresponding results folder.\n",
    "    - Include:\n",
    "        - Bar charts for accuracy and RAG quality\n",
    "        - Boxplots for response latency\n",
    "        - Violin plots for retrieval fidelity\n",
    "\n",
    "> **Tip:** Run `main.py try_n` on the terminal in orden to save the try number n results in that directory<br> in orden not to overwrite other results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832372ef",
   "metadata": {},
   "source": [
    "# 3. Workflow Summary\n",
    "\n",
    "1. Load questions dataset (questions.json).\n",
    "\n",
    "2. Initialize vector database (ChromaDB).\n",
    "\n",
    "3. For each question:\n",
    "    - Retrieve relevant documents (BM25 / Dense / Hybrid / Hybrid + Cross-Encoder).\n",
    "    - Query LLM for an answer.\n",
    "    - Verify against ground truth.\n",
    "    - Save partial results.\n",
    "\n",
    "4. After all questions are processed:\n",
    "    - Concatenate all results.\n",
    "    - Save final results CSV.\n",
    "    - Generate plots/dashboard for comparison and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89a3e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jed13\\miniconda3\\envs\\rag_contest\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entorno configurado y librer√≠as cargadas.\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS Y SETUP ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# A√±adimos la ra√≠z del proyecto al path para poder importar 'src'\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "\n",
    "# Importamos nuestros m√≥dulos de ingenier√≠a\n",
    "from src.retrieval import RetrievalEngine\n",
    "from src.rag_pipeline import query_rag\n",
    "from src.evaluation import generate_dashboard\n",
    "from src.queries import run_questions\n",
    "\n",
    "# Cargar API Key\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Configuraci√≥n visual para Pandas\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"‚úÖ Entorno configurado y librer√≠as cargadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47247bf6",
   "metadata": {},
   "source": [
    "## Key Features\n",
    "\n",
    "To ensure robustness, reproducibility, and scientific rigor, this project integrates several software engineering patterns:\n",
    "\n",
    "- **Singleton Pattern for Retrieval Engine:** The `RetrievalEngine` class uses a Singleton pattern with **Lazy Loading**. This prevents memory overhead by loading heavy models only when necessary and ensures consistent database connections across the pipeline.\n",
    "\n",
    "- **Smart Ingestion Strategy:** We compare **Recursive Chunking** (optimized with high overlap to preserve scientific context) against **Semantic Chunking** (experimental), which uses embedding distances to segment text by topic.\n",
    "\n",
    "- **Automated \"Judge\" Evaluation:** Instead of relying solely on the final answer accuracy, we implement a **Ground Truth Verification** system. It compares the retrieved chunks against the source reference in the dataset to detect lucky guesses.\n",
    "\n",
    "- **Two-Stage Architecture:** A funnel approach that prioritizes *Recall* in the first stage (Hybrid Search) and *Precision* in the second stage (Cross-Encoder Re-ranking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6216694",
   "metadata": {},
   "source": [
    "## Data Ingestions and Chunking Strategy\n",
    "\n",
    "The initial stage of the RAG pipeline focuses on **Data Pre-processing**. This involves breaking down the source document (`paper_refrag.pdf`) into discrete units of knowledge (**chunks**) that are stored in **ChromaDB**.\n",
    "\n",
    "### Chunking Methodologies\n",
    "\n",
    "We implemented two primary chunking strategies, selectable via the `CHUNKING_METHOD` variable in `src/ingestion.py`:\n",
    "\n",
    "1.  **Recursive Chunking (Recursive Character Splitter):** This is the baseline method, optimized for technical context. It relies on rules of fixed length (e.g., **1200 characters** with **350 characters of overlap**).\n",
    "\n",
    "2.  **Semantic Chunking:** The advanced method. It utilizes an **embedding model** to compute the similarity between consecutive sentences, identifying natural **topic shifts** to define chunk boundaries.\n",
    "\n",
    "### Why Semantic Chunking?\n",
    "\n",
    "We chose to focus on **Semantic Chunking** because it addresses the primary weakness of RAG in technical domains: **avoiding false negatives** in retrieval.\n",
    "\n",
    "In dense scientific papers, naive splitting often cuts conclusions, cross-references, or complex sentences in half. This leads to:\n",
    "\n",
    "* **Loss of Context:** The embedding model creates an inaccurate vector because the sentence is fragmented.\n",
    "\n",
    "* **Lucky guesses:** The system answers correctly, but the strict evaluation judge fails to find the complete reference phrase because it was broken up.\n",
    "\n",
    "By using **Semantic Chunking**, we ensure that each unit of information passed to the retriever has the **highest possible semantic coherence**, which directly improves the **Retrieval Recall** and **Fidelity** metrics in our evaluation dashboard.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Once the text is chunked, the system proceeds to **initialize the Retriever** in a **Lazy Loading** state, ensuring the database connection is established efficiently right before the first query is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34afb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jed13\\miniconda3\\envs\\rag_contest\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Verificando integridad de los datos...\n",
      "\n",
      "üîå Desconectando motor de b√∫squeda...\n",
      "üìÑ Cargando PDF...\n",
      "   -> PDF cargado: 30 p√°ginas.\n",
      "‚úÇÔ∏è Procesando fragmentos\n",
      "   -> Generados 81 fragmentos.\n",
      "üß† Guardando vectores en disco...\n"
     ]
    }
   ],
   "source": [
    "# --- GESTI√ìN DE DATOS Y VERIFICACI√ìN ---\n",
    "import os\n",
    "from src.ingestion import db_setup\n",
    "from src.retrieval import RetrievalEngine\n",
    "\n",
    "try:\n",
    "    # 1. GARANT√çA DE EXISTENCIA DE BD (Ingesta Manual)\n",
    "    db_setup(rebuild_db=True, chunking_method=\"semantic\") # Opciones: \"recursive\", \"semantic\"\n",
    "    \n",
    "    # 2. CONEXI√ìN AL MOTOR (Singleton)\n",
    "    print(\"\\n‚öôÔ∏è Conectando al motor de recuperaci√≥n...\")\n",
    "    engine = RetrievalEngine.get_instance()\n",
    "    \n",
    "    # 3. PRUEBA DE RECUPERADOR\n",
    "    # Esto asegura que ChromaDB y los Embeddings est√°n cargados en RAM\n",
    "    test_retriever = engine.get_retriever(\"hybrid\", k=1) # As√≠ aseguramos de que se inicien dense y bm25\n",
    "    \n",
    "    # Obtenemos estad√≠sticas reales de la BD\n",
    "    if engine.db:\n",
    "        doc_count = engine.db._collection.count()\n",
    "        print(f\"‚úÖ SISTEMA OPERATIVO\")\n",
    "        print(f\"   - Estado: Base de Datos Conectada\")\n",
    "        print(f\"   - Ruta: ./data/chroma_db\")\n",
    "        print(f\"   - Chunks Indexados: {doc_count}\")\n",
    "        print(f\"   - Modelo Embeddings: all-MiniLM-L6-v2\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR CR√çTICO DE INICIALIZACI√ìN: {e}\")\n",
    "    print(\"   - Verifica que la base de datos est√© construida correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12e07e",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    " The project is organized into multiple files under the src/ folder, with a clear separation of responsibilities. Here‚Äôs how it works:\n",
    "\n",
    "1. **Input Data**\n",
    "\n",
    "  All of the following are on the `data/` directory:\n",
    "- `**data/questions.json**` - Contains 50 multiple-choice questions extracted from a technical research paper.\n",
    "Each question includes: the correct answer, three distractors, and an optional reference to the source paper.\n",
    "\n",
    "- `**data/chroma_db**` - Embedded vector data base.\n",
    "- `**data/enunciado.pdf**` - The problem statement already described.\n",
    "- `**data/paper_refrag.pdf**` - The technical paper from which questions and answers are extracted.\n",
    "\n",
    "\n",
    "2. **Source Code**\n",
    "- `**main.py**` - Entry point of the project.\n",
    "Supports *Local* mode (`results/local_results/`) and *Persistent* mode (`results/persistent_results/<test_name>/`).  \n",
    "    - Calls the pipeline, saves final results, and generates plots.\n",
    "  \n",
    "\n",
    "  All of the following are on the `src/` directory:\n",
    "- `**launcher.py**` - Sets up the environment for experiments.\n",
    "    - Initializes the vector database (ChromaDB).\n",
    "    - Clears previous results if needed.\n",
    "    - Creates all necessary directories (plots, final CSVs, etc.).\n",
    "\n",
    "- `**src/ingestion.py**` - Prepares and creates the data base. Invoked by the launcher.\n",
    "\n",
    "- `**queries.py**` - Main logic to execute questions.\n",
    "    - For each question and method it:\n",
    "        - Sends the query to the LLM and retrieves documents.\n",
    "        - Computes accuracy and evidence verification.\n",
    "        - Stores partial results in results/resultados_parciales.csv.\n",
    "        - Returns a DataFrame with all results for further evaluation.\n",
    "\n",
    "- `**rag_pipeline.py**` - Implements RAG logic for different retrieval methods\n",
    "    - Contains functions to verify ground truth against retrieved documents.\n",
    "     Computes retrieval scores and status tags for each answer.\n",
    "\n",
    "- `**retrieval.py**` - Implements the retrieval engine.\n",
    "    - Provides a singleton engine to handle different retrieval methods efficiently.\n",
    "\n",
    "- `**evaluation.py**` - Evaluates the results and generates dashboards.\n",
    "    - Accuracy per method  \n",
    "    - RAG quality distribution  \n",
    "    - Response latency  \n",
    "    - Retrieval fidelity\n",
    "\n",
    "3. **Output Data**\n",
    "\n",
    "Found on the `results/` directory:\n",
    "\n",
    "- **Partial results** - Always stored in `results/resultados_parciales.csv`.\n",
    "    - Updated after each question is processed.\n",
    "\n",
    "- **Final results** - Stored in `results/local_results/` or `results/persistent_results/<test_name>`.\n",
    "    - File name: `resultados_finales.csv`.\n",
    "\n",
    "- **Plots/Dashboard** - Stored in `plots/` inside the corresponding results folder.\n",
    "    - Include:\n",
    "        - Bar charts for accuracy and RAG quality\n",
    "        - Boxplots for response latency\n",
    "        - Violin plots for retrieval fidelity\n",
    "\n",
    "> **Tip:** Run `main.py try_n` on the terminal in orden to save the try number n results in that directory<br> in orden not to overwrite other results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc067e",
   "metadata": {},
   "source": [
    "# 3. Workflow Summary\n",
    "\n",
    "1. Load questions dataset (questions.json).\n",
    "\n",
    "2. Initialize vector database (ChromaDB).\n",
    "\n",
    "3. For each question:\n",
    "    - Retrieve relevant documents (BM25 / Dense / Hybrid / Hybrid + Cross-Encoder).\n",
    "    - Query LLM for an answer.\n",
    "    - Verify against ground truth.\n",
    "    - Save partial results.\n",
    "\n",
    "4. After all questions are processed:\n",
    "    - Concatenate all results.\n",
    "    - Save final results CSV.\n",
    "    - Generate plots/dashboard for comparison and analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbb2c07",
   "metadata": {},
   "source": [
    "# 5. Data Analysis and Inference\n",
    "\n",
    "\n",
    "Our **project** is designed for generating four plots:\n",
    "\n",
    "1. **Accuracy** on **answer selected**\n",
    "2. RAG quality\n",
    "3. Latency\n",
    "4. Retrieval fidelity\n",
    "\n",
    "## 5.1. Defining Metrics and Data Collection\n",
    "discutir como se recoge cada dato y como esta valorado si se valora\n",
    "\n",
    "## 5.2. The Evaluation Challenge: Defining \"Likeness\"\n",
    "Therefore, we will discuss each **pipeline's performance** in each metric described, but before that, we must take into account the **methodology** considered for the evaluation of the RAG quality and the retrieval fidelity judge. This is due to the fact that these metrics are attached to how we evaluate likeness and the threshold of a \"lucky answer.\" We will give an insight into this idea:\n",
    "\n",
    "- The plots in `results/persistent_results/27-11-1/2_rag_quality.png` are obtained through a judge that is based on text matching:\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"results/persistent_results/27-11-1/plots/2_rag_quality_pct.png\" alt=\"Text Matching Judge (Conservative)\" width=\"45%\"/>\n",
    "</center>\n",
    "\n",
    "- Whereas the ones in `results/persistent_results/8-11-1` have an **LLM judge** that searches for the semantic match:¬†\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"results/persistent_results/28-11-1/plots/2_rag_quality_pct.png\" alt=\"LLM Judge (Semantic)\" width=\"45%\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "Suddenly, all the pipelines have improved. However, when **manually supervised**, neither of the judges has a **flawless performance** at this task. The first judge **incorrectly** marks as \"lucky answers\" a group that is, in fact, a \"correct answer,\" and the second does the opposite.\n",
    "\n",
    "This **false positives** and **false negatives** concept is an extremely important idea in statistics and in other science fields, and the general conclusion for this problem is to be **conservative**.\n",
    "\n",
    "For this reason, we will mention other methodologies we have developed but only show and explore the ones with a risk-averse approach.\n",
    "\n",
    "\n",
    "## 5.3. Pipeline Performance and Inference\n",
    "\n",
    "explicar como de bueno es cada pipeline en cada metrica y decir cual es mejor y en que condiciones\n",
    "\n",
    "## 5.4. Conclusion: Optimal Pipeline and Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c8d7f",
   "metadata": {},
   "source": [
    "## Authors\n",
    "- Jorge Barbero Mor√°n ‚Äì UCM, Faculty of Mathematics\n",
    "- David Marcos Jimeno ‚Äì UCM, Faculty of Mathematics\n",
    "\n",
    "\n",
    "## License\n",
    "This project is licensed under the **MIT License**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_contest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
